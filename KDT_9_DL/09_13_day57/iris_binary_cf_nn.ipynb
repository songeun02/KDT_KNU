{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN 기반 이진분류 모델 구현 맟 학습 모니터링과 저장 \n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐 : 4개 sepal_length, sepal_width, petal_length, petal_width\n",
    "- 타겟/라벨 : 1개 variety - Setosa\n",
    "- 학습 방법 : 지도학습 - 회귀 \n",
    "- 알고리즘 : 인공신경망(ANN) -> MLP(Multi Layer Perceptron), DNN ( ) : 은닉층이 많은 구성 \n",
    "- 프레임 워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 모듈 로딩 및 데이터 준비\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 성능지표 \n",
    "from torchmetrics.classification import F1Score, BinaryF1Score\n",
    "from torchmetrics.classification import BinaryConfusionMatrix \n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 처리 및 시각화  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import * \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch v.2.4.1\n",
      "pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 => 사용자 정의 함수로 구현하기 \n",
    "print(f'torch v.{torch.__version__}')\n",
    "print(f'pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩 \n",
    "DATA_FILE = '../DATA/iris.csv'\n",
    "\n",
    "# csv => DF\n",
    "iris_df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 변경 => 정수화, 클래스 3개 => 2개 \n",
    "iris_df['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1       True\n",
       "2       True\n",
       "3       True\n",
       "4       True\n",
       "       ...  \n",
       "145    False\n",
       "146    False\n",
       "147    False\n",
       "148    False\n",
       "149    False\n",
       "Name: variety, Length: 150, dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['variety'] == 'Setosa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0             5.1          3.5           1.4          0.2     True\n",
       "1             4.9          3.0           1.4          0.2     True\n",
       "2             4.7          3.2           1.3          0.2     True\n",
       "3             4.6          3.1           1.5          0.2     True\n",
       "4             5.0          3.6           1.4          0.2     True\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3    False\n",
       "146           6.3          2.5           5.0          1.9    False\n",
       "147           6.5          3.0           5.2          2.0    False\n",
       "148           6.2          3.4           5.4          2.3    False\n",
       "149           5.9          3.0           5.1          1.8    False\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['variety'] = (iris_df['variety'] == 'Setosa')\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels => {True: 0, False: 1}\n"
     ]
    }
   ],
   "source": [
    "# 타겟 정수화 \n",
    "labels = dict(zip(iris_df['variety'].unique().tolist(), range(3)))\n",
    "print(f'Labels => {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유값 :  [1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        1\n",
       "1           4.9          3.0           1.4          0.2        1\n",
       "2           4.7          3.2           1.3          0.2        1\n",
       "3           4.6          3.1           1.5          0.2        1\n",
       "4           5.0          3.6           1.4          0.2        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['variety'] =iris_df['variety'].astype(int)\n",
    "print(f'고유값 : ',iris_df['variety'].unique())\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 모델 클래스 설계 및 정의 \n",
    "- 클래스 목적 : iris 데이터를 학습 및 추론 목적 \n",
    "- 클래스 이름 : iris_bs_model\n",
    "- 부모 클래스 : nn.Module \n",
    "- 매개 변수 : 층 별 입출력 개수 고정하기 때문에 필요 x \n",
    "- 속성 / 필드 : features_df, target_df, n_rows, n_features (df만들 때 사용)\n",
    "- 기능 / 역할 : __init__() : 모델 구조 생성 , forward : 순방향 학습 (오버라이딩(overriding조건 : 상속관계에서만 가능))\n",
    "- 클래스 구조 \n",
    "    - 입력층 : 입력 4개(피처)        출력 10개(퍼셉트론/뉴런 10개 존재)\n",
    "    - 은닉층 : 입력 10개            출력 5개 (퍼셉트론/뉴런 30개 존재)\n",
    "    - 출력층 : 입력 5개             출력 1개 (퍼셉트론/뉴런 1개 존재 : 이진분류)\n",
    "\n",
    "- 활성화 함수 \n",
    "    - 클래스 형태 ==> nn.MSELoss , nn.ReLU => _ _init_ _() 메서드 \n",
    "    - 함수 형태 => torch.nn.functional 아래에 => forward() 메서드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_bs_model(nn.Module):\n",
    "    \n",
    "    # 모델 구조 구성 및 인스턴스 생성 및 메서드 \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모델 구조 구성 \n",
    "        self.in_layer = nn.Linear(4,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.out_layer = nn.Linear(5,1)\n",
    "\n",
    "    # 순방향 학습 진행 메서드 \n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # 입력층 \n",
    "        y = self.in_layer(input_data) # f11w11 + f12w12 + f13w13 + b, ......., f101w101 + f102w102 + f103w103 + b\n",
    "        y = F.relu(y)             # relu => y 값의 범위 : 0 <= y \n",
    "\n",
    "        # 은닉층 : 10개의 숫자 값(>=0)\n",
    "        y = self.hidden_layer(y)  # f21w11 + f22w12 .... + f210w210 + b, ......., f230w201 + f230w202 ..... f230w210 + b\n",
    "        # 데이터 1개 기준\n",
    "        y = F.relu(y) \n",
    "\n",
    "        # 출력층 : 5개의 숫자 값(>=0)\n",
    "        # self.out_layer(y)         # f31w31 + ...... f330w330 + b\n",
    "        # 회귀라서 활성함수 사용 x -> 바로 return \n",
    "\n",
    "        return F.sigmoid(self.out_layer(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_bs_model(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성 \n",
    "model = iris_bs_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "iris_bs_model                            [17, 1]                   --\n",
       "├─Linear: 1-1                            [17, 10]                  50\n",
       "├─Linear: 1-2                            [17, 5]                   55\n",
       "├─Linear: 1-3                            [17, 1]                   6\n",
       "==========================================================================================\n",
       "Total params: 111\n",
       "Trainable params: 111\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 메모리 정보 확인 \n",
    "\n",
    "summary(model, input_size=(17,4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 데이터셋 클래스 설계 및 정의\n",
    "- 데이터셋 : iris.csv \n",
    "- 피쳐 개수 : 4개 \n",
    "- 타겟 개수 : 1개 \n",
    "- 클래스 이름 : iris_data_set\n",
    "- 부모 클래스 : utils.data.Dataset \n",
    "- 속성 / 필드 : feature_df, target_df \n",
    "- 필수 메서드 \n",
    "    - _ _init_ _(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정 \n",
    "    - _ _len_ _(self) : 데이터의 개수 반환 \n",
    "    - _ _ getItem_ _(self, index) : 특정 인덱스의 피쳐와 타겟 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_data_set(Dataset):\n",
    "    def __init__(self, feature_df, target_df):\n",
    "        self.feature_df = feature_df\n",
    "        self.target_df = target_df\n",
    "        self.n_rows = feature_df.shape[0]\n",
    "        self.n_features = feature_df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        # 텐서화 \n",
    "        feature_ts = torch.FloatTensor(self.feature_df.iloc[index].values) # 시리즈라서 values() 사용해서 numpy -> tensor \n",
    "        target_ts = torch.FloatTensor(self.target_df.iloc[index].values)\n",
    "                \n",
    "        # 피쳐와 타겟 반환 \n",
    "        return feature_ts, target_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 데이터셋 인스턴스 생성 \n",
    "\n",
    "# - DF 에서 피쳐와 타겟 추출 \n",
    "feature_df = iris_df[iris_df.columns[:-1]] # 2D \n",
    "target_df = iris_df[iris_df.columns[-1:]]  # 2D \n",
    "\n",
    "# 커스텀 데이터셋 인스턴스 생성 \n",
    "iris_ds = iris_data_set(feature_df, target_df)\n",
    "\n",
    "# 데이터로더 인스턴스 생성 \n",
    "iris_dl = DataLoader(iris_ds)\n",
    "for feature, label in iris_dl:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] 학습 준비 \n",
    "- 학습 횟수 : EPOCH (처음부터 끝까지 학습할 단위)\n",
    "- 배치 크기 : BATCH_SIZE (한 번에 학습할 데이터셋 양)\n",
    "- 위치 지정 : DEVICE (텐서 저장 및 실행 위치 (GPU, CPU))\n",
    "- 학습률 : 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001 ~ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정 \n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10 \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001 # hyper-parameter : 업데이트 간격  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스 / 객체 : 모델, 데이터셋, 최적화, 손실함수 ,(성능지표)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 \n",
    "model = iris_bs_model()\n",
    "\n",
    "# 데이터셋 인스턴스 \n",
    "iris_ds = iris_data_set(feature_df, target_df)\n",
    "\n",
    "# 데이터로더 인스턴스 \n",
    "iris_dl = DataLoader(iris_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n",
      "variety\n",
      "0          0.702381\n",
      "1          0.297619\n",
      "Name: count, dtype: float64 variety\n",
      "0          0.297619\n",
      "1          0.154762\n",
      "Name: count, dtype: float64 variety\n",
      "0          0.190476\n",
      "1          0.142857\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# DS과 DL 인스턴스 \n",
    "# - 학습용, 검증용, 테스트용 데이터 분리 \n",
    "\n",
    "# 데이터셋 인스턴스 \n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_df, target_df, random_state = 1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state = 1)\n",
    "print(f'{x_train.shape} {x_test.shape} {x_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "print(f'{y_train.value_counts()/y_train.shape[0]} {y_test.value_counts()/y_train.shape[0]} {y_val.value_counts()/y_train.shape[0]}')\n",
    "print(f'{type(x_train)} {type(x_test)} {type(x_val)}')\n",
    "# iris_ds = iris_data_set(x_train, y_train)\n",
    "\n",
    "# 학습용, 검증용 테스트용 데이터셋 \n",
    "train_ds = iris_data_set(x_train, y_train)\n",
    "val_ds = iris_data_set(x_val, y_val)\n",
    "test_ds = iris_data_set(x_test, y_test)\n",
    "\n",
    "# 학습용 데이터로더 인스턴스 \n",
    "# iris_dl = DataLoader(iris_ds, batch_size=BATCH_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => w, b 텐서 즉, model.parameters() 전달 \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 이진분류 BinaryCrossEntropyLoss => BCELoss \n",
    "#                            예측값은 확률값으로 전달 => sigmoid() AF 처리 후 전달 \n",
    "req_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] 학습 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl), train_dl.__len__() # __len__ 쓰면 len() 이 불림 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.n_rows / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 4개 학습이 덜 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.4, 9, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math as m\n",
    "train_ds.n_rows / BATCH_SIZE, m.ceil(train_ds.n_rows/BATCH_SIZE), train_ds.feature_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT : 9\n",
      "[0/1000]\n",
      "- Train Loss : 1.3463615046607122 Score : 0.4377178516652849\n",
      "- Val Loss : 1.1145743131637573 Score : 0.6000000238418579\n",
      "[1/1000]\n",
      "- Train Loss : 1.2223336299260457 Score : 0.4377178516652849\n",
      "- Val Loss : 1.0276764631271362 Score : 0.6000000238418579\n",
      "[2/1000]\n",
      "- Train Loss : 1.1168813440534804 Score : 0.4377178516652849\n",
      "- Val Loss : 0.9563011527061462 Score : 0.6000000238418579\n",
      "[3/1000]\n",
      "- Train Loss : 1.0288457075754802 Score : 0.4377178516652849\n",
      "- Val Loss : 0.8982623219490051 Score : 0.6000000238418579\n",
      "[4/1000]\n",
      "- Train Loss : 0.9561630487442017 Score : 0.4377178516652849\n",
      "- Val Loss : 0.8513818383216858 Score : 0.6000000238418579\n",
      "[5/1000]\n",
      "- Train Loss : 0.8963411185476515 Score : 0.4377178516652849\n",
      "- Val Loss : 0.8137302994728088 Score : 0.6000000238418579\n",
      "[6/1000]\n",
      "- Train Loss : 0.8467613392406039 Score : 0.4377178516652849\n",
      "- Val Loss : 0.7827134728431702 Score : 0.6000000238418579\n",
      "[7/1000]\n",
      "- Train Loss : 0.8034776714113023 Score : 0.4377178516652849\n",
      "- Val Loss : 0.755158543586731 Score : 0.6000000238418579\n",
      "[8/1000]\n",
      "- Train Loss : 0.777704656124115 Score : 0.4377178516652849\n",
      "- Val Loss : 0.7424836158752441 Score : 0.6000000238418579\n",
      "[9/1000]\n",
      "- Train Loss : 0.7586699790424771 Score : 0.4377178516652849\n",
      "- Val Loss : 0.7305299639701843 Score : 0.6000000238418579\n",
      "[10/1000]\n",
      "- Train Loss : 0.7409140268961588 Score : 0.425507836871677\n",
      "- Val Loss : 0.7198085784912109 Score : 0.6000000238418579\n",
      "[11/1000]\n",
      "- Train Loss : 0.7248870001898872 Score : 0.40108780728446114\n",
      "- Val Loss : 0.7102978825569153 Score : 0.5405405163764954\n",
      "[12/1000]\n",
      "- Train Loss : 0.7109397848447164 Score : 0.2172839558786816\n",
      "- Val Loss : 0.702992856502533 Score : 0.2222222238779068\n",
      "[13/1000]\n",
      "- Train Loss : 0.6987729138798184 Score : 0.0634920663303799\n",
      "- Val Loss : 0.6953830122947693 Score : 0.10526315867900848\n",
      "[14/1000]\n",
      "- Train Loss : 0.6880719992849562 Score : 0.05555555555555555\n",
      "- Val Loss : 0.6896877884864807 Score : 0.0\n",
      "[15/1000]\n",
      "- Train Loss : 0.6789262427224053 Score : 0.1111111111111111\n",
      "- Val Loss : 0.6846174597740173 Score : 0.0\n",
      "[16/1000]\n",
      "- Train Loss : 0.670520007610321 Score : 0.0\n",
      "- Val Loss : 0.679781436920166 Score : 0.0\n",
      "[17/1000]\n",
      "- Train Loss : 0.6626459227667915 Score : 0.0\n",
      "- Val Loss : 0.6751859784126282 Score : 0.0\n",
      "[18/1000]\n",
      "- Train Loss : 0.6552179058392843 Score : 0.0\n",
      "- Val Loss : 0.6705603003501892 Score : 0.0\n",
      "[19/1000]\n",
      "- Train Loss : 0.6480967667367723 Score : 0.0\n",
      "- Val Loss : 0.6660251617431641 Score : 0.0\n",
      "[20/1000]\n",
      "- Train Loss : 0.6412300997310214 Score : 0.0\n",
      "- Val Loss : 0.6615645289421082 Score : 0.0\n",
      "[21/1000]\n",
      "- Train Loss : 0.6345588631100125 Score : 0.0\n",
      "- Val Loss : 0.6571427583694458 Score : 0.0\n",
      "[22/1000]\n",
      "- Train Loss : 0.6280320617887709 Score : 0.0\n",
      "- Val Loss : 0.6527234315872192 Score : 0.0\n",
      "[23/1000]\n",
      "- Train Loss : 0.6216265691651238 Score : 0.0\n",
      "- Val Loss : 0.6483426690101624 Score : 0.0\n",
      "[24/1000]\n",
      "- Train Loss : 0.6152881582578024 Score : 0.0\n",
      "- Val Loss : 0.6442674398422241 Score : 0.0\n",
      "[25/1000]\n",
      "- Train Loss : 0.6105682121382819 Score : 0.0\n",
      "- Val Loss : 0.6429550051689148 Score : 0.0\n",
      "[26/1000]\n",
      "- Train Loss : 0.6082370479901632 Score : 0.0\n",
      "- Val Loss : 0.6405423879623413 Score : 0.0\n",
      "[27/1000]\n",
      "- Train Loss : 0.6060196227497525 Score : 0.0\n",
      "- Val Loss : 0.6372815370559692 Score : 0.0\n",
      "[28/1000]\n",
      "- Train Loss : 0.6037626465161642 Score : 0.0\n",
      "- Val Loss : 0.6339077353477478 Score : 0.0\n",
      "[29/1000]\n",
      "- Train Loss : 0.6015183064672682 Score : 0.0\n",
      "- Val Loss : 0.6305986642837524 Score : 0.0\n",
      "[30/1000]\n",
      "- Train Loss : 0.5992660125096639 Score : 0.0\n",
      "- Val Loss : 0.6276347041130066 Score : 0.0\n",
      "[31/1000]\n",
      "- Train Loss : 0.5969093243281046 Score : 0.0\n",
      "- Val Loss : 0.6247431039810181 Score : 0.0\n",
      "[32/1000]\n",
      "- Train Loss : 0.5944304201338027 Score : 0.0\n",
      "- Val Loss : 0.621640682220459 Score : 0.0\n",
      "[33/1000]\n",
      "- Train Loss : 0.5918701423539056 Score : 0.0\n",
      "- Val Loss : 0.6183538436889648 Score : 0.0\n",
      "[34/1000]\n",
      "- Train Loss : 0.5892217887772454 Score : 0.0\n",
      "- Val Loss : 0.6149186491966248 Score : 0.0\n",
      "[35/1000]\n",
      "- Train Loss : 0.5864796770943536 Score : 0.0\n",
      "- Val Loss : 0.6113486886024475 Score : 0.0\n",
      "[36/1000]\n",
      "- Train Loss : 0.5836396018664042 Score : 0.0\n",
      "- Val Loss : 0.6076481938362122 Score : 0.0\n",
      "[37/1000]\n",
      "- Train Loss : 0.580697582827674 Score : 0.1111111111111111\n",
      "- Val Loss : 0.6038175225257874 Score : 0.1538461595773697\n",
      "[38/1000]\n",
      "- Train Loss : 0.5776498516400655 Score : 0.1111111111111111\n",
      "- Val Loss : 0.5998555421829224 Score : 0.1538461595773697\n",
      "[39/1000]\n",
      "- Train Loss : 0.5744922690921359 Score : 0.1111111111111111\n",
      "- Val Loss : 0.5956797003746033 Score : 0.1538461595773697\n",
      "[40/1000]\n",
      "- Train Loss : 0.5712100664774576 Score : 0.2296296324994829\n",
      "- Val Loss : 0.5913203358650208 Score : 0.5\n",
      "[41/1000]\n",
      "- Train Loss : 0.5678550004959106 Score : 0.32962963316175675\n",
      "- Val Loss : 0.5868541598320007 Score : 0.5882353186607361\n",
      "[42/1000]\n",
      "- Train Loss : 0.5643427239523994 Score : 0.551851855383979\n",
      "- Val Loss : 0.5820846557617188 Score : 0.7368420958518982\n",
      "[43/1000]\n",
      "- Train Loss : 0.5607341594166226 Score : 0.5814814865589142\n",
      "- Val Loss : 0.5772129893302917 Score : 0.7368420958518982\n",
      "[44/1000]\n",
      "- Train Loss : 0.5570393800735474 Score : 0.6693121757772234\n",
      "- Val Loss : 0.5723139643669128 Score : 0.9090909361839294\n",
      "[45/1000]\n",
      "- Train Loss : 0.5531946751806471 Score : 0.7328042387962341\n",
      "- Val Loss : 0.5672448873519897 Score : 1.0\n",
      "[46/1000]\n",
      "- Train Loss : 0.5492215951283773 Score : 0.7892416252030267\n",
      "- Val Loss : 0.5618828535079956 Score : 1.0\n",
      "[47/1000]\n",
      "- Train Loss : 0.5451178749402364 Score : 0.9682539701461792\n",
      "- Val Loss : 0.5562418103218079 Score : 1.0\n",
      "[48/1000]\n",
      "- Train Loss : 0.540918747584025 Score : 0.9841269850730896\n",
      "- Val Loss : 0.5505101084709167 Score : 1.0\n",
      "[49/1000]\n",
      "- Train Loss : 0.5366016692585416 Score : 0.9841269850730896\n",
      "- Val Loss : 0.544657289981842 Score : 1.0\n",
      "[50/1000]\n",
      "- Train Loss : 0.5321546196937561 Score : 0.9841269850730896\n",
      "- Val Loss : 0.5386118292808533 Score : 1.0\n",
      "[51/1000]\n",
      "- Train Loss : 0.527582950062222 Score : 0.9841269850730896\n",
      "- Val Loss : 0.532403826713562 Score : 1.0\n",
      "[52/1000]\n",
      "- Train Loss : 0.5229104691081576 Score : 1.0\n",
      "- Val Loss : 0.526213526725769 Score : 1.0\n",
      "[53/1000]\n",
      "- Train Loss : 0.5181320077843137 Score : 1.0\n",
      "- Val Loss : 0.5199922323226929 Score : 1.0\n",
      "[54/1000]\n",
      "- Train Loss : 0.5131703284051683 Score : 1.0\n",
      "- Val Loss : 0.5133911371231079 Score : 1.0\n",
      "[55/1000]\n",
      "- Train Loss : 0.5081256429354349 Score : 1.0\n",
      "- Val Loss : 0.506633996963501 Score : 1.0\n",
      "[56/1000]\n",
      "- Train Loss : 0.5029638840092553 Score : 1.0\n",
      "- Val Loss : 0.49969059228897095 Score : 1.0\n",
      "[57/1000]\n",
      "- Train Loss : 0.4977366560035282 Score : 1.0\n",
      "- Val Loss : 0.4926919937133789 Score : 1.0\n",
      "[58/1000]\n",
      "- Train Loss : 0.4924001097679138 Score : 1.0\n",
      "- Val Loss : 0.4856375753879547 Score : 1.0\n",
      "[59/1000]\n",
      "- Train Loss : 0.48697639505068463 Score : 1.0\n",
      "- Val Loss : 0.4785957932472229 Score : 1.0\n",
      "[60/1000]\n",
      "- Train Loss : 0.48146113091044956 Score : 1.0\n",
      "- Val Loss : 0.47151070833206177 Score : 1.0\n",
      "[61/1000]\n",
      "- Train Loss : 0.47585082054138184 Score : 1.0\n",
      "- Val Loss : 0.46435070037841797 Score : 1.0\n",
      "[62/1000]\n",
      "- Train Loss : 0.4701756404505836 Score : 1.0\n",
      "- Val Loss : 0.4570295512676239 Score : 1.0\n",
      "[63/1000]\n",
      "- Train Loss : 0.4644616014427609 Score : 1.0\n",
      "- Val Loss : 0.44964513182640076 Score : 1.0\n",
      "[64/1000]\n",
      "- Train Loss : 0.4587011867099338 Score : 1.0\n",
      "- Val Loss : 0.44217246770858765 Score : 1.0\n",
      "[65/1000]\n",
      "- Train Loss : 0.452914790974723 Score : 1.0\n",
      "- Val Loss : 0.4345923364162445 Score : 1.0\n",
      "[66/1000]\n",
      "- Train Loss : 0.4471389287047916 Score : 1.0\n",
      "- Val Loss : 0.4271208345890045 Score : 1.0\n",
      "[67/1000]\n",
      "- Train Loss : 0.4413433339860704 Score : 1.0\n",
      "- Val Loss : 0.41971877217292786 Score : 1.0\n",
      "[68/1000]\n",
      "- Train Loss : 0.43558886978361344 Score : 1.0\n",
      "- Val Loss : 0.41238874197006226 Score : 1.0\n",
      "[69/1000]\n",
      "- Train Loss : 0.42977527446217006 Score : 1.0\n",
      "- Val Loss : 0.40479299426078796 Score : 1.0\n",
      "[70/1000]\n",
      "- Train Loss : 0.42406707339816624 Score : 1.0\n",
      "- Val Loss : 0.3973127007484436 Score : 1.0\n",
      "[71/1000]\n",
      "- Train Loss : 0.41850579447216457 Score : 1.0\n",
      "- Val Loss : 0.39034000039100647 Score : 1.0\n",
      "[72/1000]\n",
      "- Train Loss : 0.4128643572330475 Score : 1.0\n",
      "- Val Loss : 0.3831649422645569 Score : 1.0\n",
      "[73/1000]\n",
      "- Train Loss : 0.4073959357208676 Score : 1.0\n",
      "- Val Loss : 0.3763100206851959 Score : 1.0\n",
      "[74/1000]\n",
      "- Train Loss : 0.40197815166579354 Score : 1.0\n",
      "- Val Loss : 0.36959534883499146 Score : 1.0\n",
      "[75/1000]\n",
      "- Train Loss : 0.3966541455851661 Score : 1.0\n",
      "- Val Loss : 0.36309415102005005 Score : 1.0\n",
      "[76/1000]\n",
      "- Train Loss : 0.39141501983006793 Score : 1.0\n",
      "- Val Loss : 0.3568507730960846 Score : 1.0\n",
      "[77/1000]\n",
      "- Train Loss : 0.3862365219328139 Score : 1.0\n",
      "- Val Loss : 0.350949764251709 Score : 1.0\n",
      "[78/1000]\n",
      "- Train Loss : 0.38110098905033535 Score : 1.0\n",
      "- Val Loss : 0.3453533947467804 Score : 1.0\n",
      "[79/1000]\n",
      "- Train Loss : 0.37597411539819503 Score : 1.0\n",
      "- Val Loss : 0.33967718482017517 Score : 1.0\n",
      "[80/1000]\n",
      "- Train Loss : 0.3710051278273265 Score : 1.0\n",
      "- Val Loss : 0.334134042263031 Score : 1.0\n",
      "[81/1000]\n",
      "- Train Loss : 0.36612406704160905 Score : 1.0\n",
      "- Val Loss : 0.32860785722732544 Score : 1.0\n",
      "[82/1000]\n",
      "- Train Loss : 0.36138052410549587 Score : 1.0\n",
      "- Val Loss : 0.3230568468570709 Score : 1.0\n",
      "[83/1000]\n",
      "- Train Loss : 0.35663413670327926 Score : 1.0\n",
      "- Val Loss : 0.3172677159309387 Score : 1.0\n",
      "[84/1000]\n",
      "- Train Loss : 0.352199696832233 Score : 1.0\n",
      "- Val Loss : 0.312217652797699 Score : 1.0\n",
      "[85/1000]\n",
      "- Train Loss : 0.34791133801142377 Score : 1.0\n",
      "- Val Loss : 0.3075288236141205 Score : 1.0\n",
      "[86/1000]\n",
      "- Train Loss : 0.3434951934549544 Score : 1.0\n",
      "- Val Loss : 0.302511066198349 Score : 1.0\n",
      "[87/1000]\n",
      "- Train Loss : 0.33927873770395917 Score : 1.0\n",
      "- Val Loss : 0.29759496450424194 Score : 1.0\n",
      "[88/1000]\n",
      "- Train Loss : 0.33523550298478866 Score : 1.0\n",
      "- Val Loss : 0.2929667532444 Score : 1.0\n",
      "[89/1000]\n",
      "- Train Loss : 0.3313261767228444 Score : 1.0\n",
      "- Val Loss : 0.2886185646057129 Score : 1.0\n",
      "[90/1000]\n",
      "- Train Loss : 0.32751413186391193 Score : 1.0\n",
      "- Val Loss : 0.2845997214317322 Score : 1.0\n",
      "[91/1000]\n",
      "- Train Loss : 0.3237728608979119 Score : 1.0\n",
      "- Val Loss : 0.28083428740501404 Score : 1.0\n",
      "[92/1000]\n",
      "- Train Loss : 0.3200739158524407 Score : 1.0\n",
      "- Val Loss : 0.27707070112228394 Score : 1.0\n",
      "[93/1000]\n",
      "- Train Loss : 0.3164653215143416 Score : 1.0\n",
      "- Val Loss : 0.2733357548713684 Score : 1.0\n",
      "[94/1000]\n",
      "- Train Loss : 0.3129151364167531 Score : 1.0\n",
      "- Val Loss : 0.26953673362731934 Score : 1.0\n",
      "[95/1000]\n",
      "- Train Loss : 0.30955132842063904 Score : 1.0\n",
      "- Val Loss : 0.2660241425037384 Score : 1.0\n",
      "[96/1000]\n",
      "- Train Loss : 0.3062021732330322 Score : 1.0\n",
      "- Val Loss : 0.26245352625846863 Score : 1.0\n",
      "[97/1000]\n",
      "- Train Loss : 0.3029475708802541 Score : 1.0\n",
      "- Val Loss : 0.2589872479438782 Score : 1.0\n",
      "[98/1000]\n",
      "- Train Loss : 0.29991205864482456 Score : 1.0\n",
      "- Val Loss : 0.2559874355792999 Score : 1.0\n",
      "[99/1000]\n",
      "- Train Loss : 0.29675667484601337 Score : 1.0\n",
      "- Val Loss : 0.2527189552783966 Score : 1.0\n",
      "[100/1000]\n",
      "- Train Loss : 0.29374926785628 Score : 1.0\n",
      "- Val Loss : 0.24958741664886475 Score : 1.0\n",
      "[101/1000]\n",
      "- Train Loss : 0.2908874683909946 Score : 1.0\n",
      "- Val Loss : 0.24673517048358917 Score : 1.0\n",
      "[102/1000]\n",
      "- Train Loss : 0.28799886670377517 Score : 1.0\n",
      "- Val Loss : 0.24381399154663086 Score : 1.0\n",
      "[103/1000]\n",
      "- Train Loss : 0.2852151410447227 Score : 1.0\n",
      "- Val Loss : 0.24098478257656097 Score : 1.0\n",
      "[104/1000]\n",
      "- Train Loss : 0.28256046109729344 Score : 1.0\n",
      "- Val Loss : 0.23838698863983154 Score : 1.0\n",
      "[105/1000]\n",
      "- Train Loss : 0.2798575196001265 Score : 1.0\n",
      "- Val Loss : 0.2357402741909027 Score : 1.0\n",
      "[106/1000]\n",
      "- Train Loss : 0.277252487010426 Score : 1.0\n",
      "- Val Loss : 0.23315660655498505 Score : 1.0\n",
      "[107/1000]\n",
      "- Train Loss : 0.27472225328286487 Score : 1.0\n",
      "- Val Loss : 0.23067046701908112 Score : 1.0\n",
      "[108/1000]\n",
      "- Train Loss : 0.2722504619095061 Score : 1.0\n",
      "- Val Loss : 0.22828368842601776 Score : 1.0\n",
      "[109/1000]\n",
      "- Train Loss : 0.2698248094982571 Score : 1.0\n",
      "- Val Loss : 0.225986510515213 Score : 1.0\n",
      "[110/1000]\n",
      "- Train Loss : 0.26743684709072113 Score : 1.0\n",
      "- Val Loss : 0.2237665206193924 Score : 1.0\n",
      "[111/1000]\n",
      "- Train Loss : 0.2650811705324385 Score : 1.0\n",
      "- Val Loss : 0.22161228954792023 Score : 1.0\n",
      "[112/1000]\n",
      "- Train Loss : 0.26275451315773857 Score : 1.0\n",
      "- Val Loss : 0.21951408684253693 Score : 1.0\n",
      "[113/1000]\n",
      "- Train Loss : 0.26042061381869847 Score : 1.0\n",
      "- Val Loss : 0.2173604667186737 Score : 1.0\n",
      "[114/1000]\n",
      "- Train Loss : 0.2581679920355479 Score : 1.0\n",
      "- Val Loss : 0.215226411819458 Score : 1.0\n",
      "[115/1000]\n",
      "- Train Loss : 0.25597206089231705 Score : 1.0\n",
      "- Val Loss : 0.21314777433872223 Score : 1.0\n",
      "[116/1000]\n",
      "- Train Loss : 0.25382277535067665 Score : 1.0\n",
      "- Val Loss : 0.21113218367099762 Score : 1.0\n",
      "[117/1000]\n",
      "- Train Loss : 0.25171180731720394 Score : 1.0\n",
      "- Val Loss : 0.2091769427061081 Score : 1.0\n",
      "[118/1000]\n",
      "- Train Loss : 0.24960662258995903 Score : 1.0\n",
      "- Val Loss : 0.20720896124839783 Score : 1.0\n",
      "[119/1000]\n",
      "- Train Loss : 0.24757955802811515 Score : 1.0\n",
      "- Val Loss : 0.20532388985157013 Score : 1.0\n",
      "[120/1000]\n",
      "- Train Loss : 0.24559028115537432 Score : 1.0\n",
      "- Val Loss : 0.20351581275463104 Score : 1.0\n",
      "[121/1000]\n",
      "- Train Loss : 0.2435995704597897 Score : 1.0\n",
      "- Val Loss : 0.2017010599374771 Score : 1.0\n",
      "[122/1000]\n",
      "- Train Loss : 0.2416286418835322 Score : 1.0\n",
      "- Val Loss : 0.19989919662475586 Score : 1.0\n",
      "[123/1000]\n",
      "- Train Loss : 0.2397065411011378 Score : 1.0\n",
      "- Val Loss : 0.1981363594532013 Score : 1.0\n",
      "[124/1000]\n",
      "- Train Loss : 0.23786727256245083 Score : 1.0\n",
      "- Val Loss : 0.19645825028419495 Score : 1.0\n",
      "[125/1000]\n",
      "- Train Loss : 0.23600496848424277 Score : 1.0\n",
      "- Val Loss : 0.19479875266551971 Score : 1.0\n",
      "[126/1000]\n",
      "- Train Loss : 0.23415995637575784 Score : 1.0\n",
      "- Val Loss : 0.1931539624929428 Score : 1.0\n",
      "[127/1000]\n",
      "- Train Loss : 0.23235190245840284 Score : 1.0\n",
      "- Val Loss : 0.19153575599193573 Score : 1.0\n",
      "[128/1000]\n",
      "- Train Loss : 0.23057314919100869 Score : 1.0\n",
      "- Val Loss : 0.18994645774364471 Score : 1.0\n",
      "[129/1000]\n",
      "- Train Loss : 0.22881885866324106 Score : 1.0\n",
      "- Val Loss : 0.18838533759117126 Score : 1.0\n",
      "[130/1000]\n",
      "- Train Loss : 0.22708662350972494 Score : 1.0\n",
      "- Val Loss : 0.1868506371974945 Score : 1.0\n",
      "[131/1000]\n",
      "- Train Loss : 0.22537402477529314 Score : 1.0\n",
      "- Val Loss : 0.18534013628959656 Score : 1.0\n",
      "[132/1000]\n",
      "- Train Loss : 0.22367978427145216 Score : 1.0\n",
      "- Val Loss : 0.18385182321071625 Score : 1.0\n",
      "[133/1000]\n",
      "- Train Loss : 0.22200315528445774 Score : 1.0\n",
      "- Val Loss : 0.18238404393196106 Score : 1.0\n",
      "[134/1000]\n",
      "- Train Loss : 0.2203435699144999 Score : 1.0\n",
      "- Val Loss : 0.1809355914592743 Score : 1.0\n",
      "[135/1000]\n",
      "- Train Loss : 0.21870068377918667 Score : 1.0\n",
      "- Val Loss : 0.1795053631067276 Score : 1.0\n",
      "[136/1000]\n",
      "- Train Loss : 0.21707421706782448 Score : 1.0\n",
      "- Val Loss : 0.1780925989151001 Score : 1.0\n",
      "[137/1000]\n",
      "- Train Loss : 0.21546397606531778 Score : 1.0\n",
      "- Val Loss : 0.1766965687274933 Score : 1.0\n",
      "[138/1000]\n",
      "- Train Loss : 0.21386974387698704 Score : 1.0\n",
      "- Val Loss : 0.17531678080558777 Score : 1.0\n",
      "[139/1000]\n",
      "- Train Loss : 0.21229136983553568 Score : 1.0\n",
      "- Val Loss : 0.17395269870758057 Score : 1.0\n",
      "[140/1000]\n",
      "- Train Loss : 0.210728676782714 Score : 1.0\n",
      "- Val Loss : 0.17260394990444183 Score : 1.0\n",
      "[141/1000]\n",
      "- Train Loss : 0.20917562974823845 Score : 1.0\n",
      "- Val Loss : 0.17125728726387024 Score : 1.0\n",
      "[142/1000]\n",
      "- Train Loss : 0.20764648914337158 Score : 1.0\n",
      "- Val Loss : 0.16992834210395813 Score : 1.0\n",
      "[143/1000]\n",
      "- Train Loss : 0.2061463346083959 Score : 1.0\n",
      "- Val Loss : 0.16862402856349945 Score : 1.0\n",
      "[144/1000]\n",
      "- Train Loss : 0.20472331013944414 Score : 1.0\n",
      "- Val Loss : 0.16734419763088226 Score : 1.0\n",
      "[145/1000]\n",
      "- Train Loss : 0.20324207180076176 Score : 1.0\n",
      "- Val Loss : 0.16608057916164398 Score : 1.0\n",
      "[146/1000]\n",
      "- Train Loss : 0.20179973873827192 Score : 1.0\n",
      "- Val Loss : 0.16483506560325623 Score : 1.0\n",
      "[147/1000]\n",
      "- Train Loss : 0.20040798518392774 Score : 1.0\n",
      "- Val Loss : 0.1636025607585907 Score : 1.0\n",
      "[148/1000]\n",
      "- Train Loss : 0.19899356034066942 Score : 1.0\n",
      "- Val Loss : 0.16238607466220856 Score : 1.0\n",
      "[149/1000]\n",
      "- Train Loss : 0.19759782983197105 Score : 1.0\n",
      "- Val Loss : 0.1611858457326889 Score : 1.0\n",
      "[150/1000]\n",
      "- Train Loss : 0.19622333513365853 Score : 1.0\n",
      "- Val Loss : 0.16000156104564667 Score : 1.0\n",
      "[151/1000]\n",
      "- Train Loss : 0.1948669602473577 Score : 1.0\n",
      "- Val Loss : 0.15883247554302216 Score : 1.0\n",
      "[152/1000]\n",
      "- Train Loss : 0.19352663887871635 Score : 1.0\n",
      "- Val Loss : 0.1576775759458542 Score : 1.0\n",
      "[153/1000]\n",
      "- Train Loss : 0.19220090409119925 Score : 1.0\n",
      "- Val Loss : 0.1565360575914383 Score : 1.0\n",
      "[154/1000]\n",
      "- Train Loss : 0.19088884194691977 Score : 1.0\n",
      "- Val Loss : 0.15540708601474762 Score : 1.0\n",
      "[155/1000]\n",
      "- Train Loss : 0.18958979182773167 Score : 1.0\n",
      "- Val Loss : 0.1542900949716568 Score : 1.0\n",
      "[156/1000]\n",
      "- Train Loss : 0.1883036775721444 Score : 1.0\n",
      "- Val Loss : 0.15318480134010315 Score : 1.0\n",
      "[157/1000]\n",
      "- Train Loss : 0.1870292392041948 Score : 1.0\n",
      "- Val Loss : 0.15209077298641205 Score : 1.0\n",
      "[158/1000]\n",
      "- Train Loss : 0.18576694362693363 Score : 1.0\n",
      "- Val Loss : 0.15100745856761932 Score : 1.0\n",
      "[159/1000]\n",
      "- Train Loss : 0.18451621797349718 Score : 1.0\n",
      "- Val Loss : 0.149934783577919 Score : 1.0\n",
      "[160/1000]\n",
      "- Train Loss : 0.18327693310048845 Score : 1.0\n",
      "- Val Loss : 0.14887256920337677 Score : 1.0\n",
      "[161/1000]\n",
      "- Train Loss : 0.1820489631758796 Score : 1.0\n",
      "- Val Loss : 0.14782069623470306 Score : 1.0\n",
      "[162/1000]\n",
      "- Train Loss : 0.1808321608437432 Score : 1.0\n",
      "- Val Loss : 0.1467789113521576 Score : 1.0\n",
      "[163/1000]\n",
      "- Train Loss : 0.17962636881404453 Score : 1.0\n",
      "- Val Loss : 0.14577031135559082 Score : 1.0\n",
      "[164/1000]\n",
      "- Train Loss : 0.17843150595823923 Score : 1.0\n",
      "- Val Loss : 0.1448133885860443 Score : 1.0\n",
      "[165/1000]\n",
      "- Train Loss : 0.1772474100192388 Score : 1.0\n",
      "- Val Loss : 0.14386656880378723 Score : 1.0\n",
      "[166/1000]\n",
      "- Train Loss : 0.1760739783445994 Score : 1.0\n",
      "- Val Loss : 0.14292944967746735 Score : 1.0\n",
      "[167/1000]\n",
      "- Train Loss : 0.17491109006934696 Score : 1.0\n",
      "- Val Loss : 0.14200173318386078 Score : 1.0\n",
      "[168/1000]\n",
      "- Train Loss : 0.17375868558883667 Score : 1.0\n",
      "- Val Loss : 0.14108316600322723 Score : 1.0\n",
      "[169/1000]\n",
      "- Train Loss : 0.17261692053741878 Score : 1.0\n",
      "- Val Loss : 0.1401735544204712 Score : 1.0\n",
      "[170/1000]\n",
      "- Train Loss : 0.17148502005471122 Score : 1.0\n",
      "- Val Loss : 0.13927267491817474 Score : 1.0\n",
      "[171/1000]\n",
      "- Train Loss : 0.170371166533894 Score : 1.0\n",
      "- Val Loss : 0.13831253349781036 Score : 1.0\n",
      "[172/1000]\n",
      "- Train Loss : 0.16924533082379234 Score : 1.0\n",
      "- Val Loss : 0.13735708594322205 Score : 1.0\n",
      "[173/1000]\n",
      "- Train Loss : 0.16816717055108812 Score : 1.0\n",
      "- Val Loss : 0.13646641373634338 Score : 1.0\n",
      "[174/1000]\n",
      "- Train Loss : 0.16709042257732815 Score : 1.0\n",
      "- Val Loss : 0.13554200530052185 Score : 1.0\n",
      "[175/1000]\n",
      "- Train Loss : 0.16600206825468275 Score : 1.0\n",
      "- Val Loss : 0.13462969660758972 Score : 1.0\n",
      "[176/1000]\n",
      "- Train Loss : 0.1649469973312484 Score : 1.0\n",
      "- Val Loss : 0.13371819257736206 Score : 1.0\n",
      "[177/1000]\n",
      "- Train Loss : 0.16389924784501395 Score : 1.0\n",
      "- Val Loss : 0.1328313946723938 Score : 1.0\n",
      "[178/1000]\n",
      "- Train Loss : 0.16287142369482252 Score : 1.0\n",
      "- Val Loss : 0.13201013207435608 Score : 1.0\n",
      "[179/1000]\n",
      "- Train Loss : 0.16185031748480266 Score : 1.0\n",
      "- Val Loss : 0.13115720450878143 Score : 1.0\n",
      "[180/1000]\n",
      "- Train Loss : 0.1608003849784533 Score : 1.0\n",
      "- Val Loss : 0.13025398552417755 Score : 1.0\n",
      "[181/1000]\n",
      "- Train Loss : 0.15979770405424965 Score : 1.0\n",
      "- Val Loss : 0.12940242886543274 Score : 1.0\n",
      "[182/1000]\n",
      "- Train Loss : 0.1588008776307106 Score : 1.0\n",
      "- Val Loss : 0.12856575846672058 Score : 1.0\n",
      "[183/1000]\n",
      "- Train Loss : 0.15781201753351423 Score : 1.0\n",
      "- Val Loss : 0.12775442004203796 Score : 1.0\n",
      "[184/1000]\n",
      "- Train Loss : 0.1568378226624595 Score : 1.0\n",
      "- Val Loss : 0.12694670259952545 Score : 1.0\n",
      "[185/1000]\n",
      "- Train Loss : 0.15584329929616716 Score : 1.0\n",
      "- Val Loss : 0.1261042058467865 Score : 1.0\n",
      "[186/1000]\n",
      "- Train Loss : 0.15489409450027677 Score : 1.0\n",
      "- Val Loss : 0.1253117173910141 Score : 1.0\n",
      "[187/1000]\n",
      "- Train Loss : 0.15394528458515802 Score : 1.0\n",
      "- Val Loss : 0.12453227490186691 Score : 1.0\n",
      "[188/1000]\n",
      "- Train Loss : 0.1529804559217559 Score : 1.0\n",
      "- Val Loss : 0.123722605407238 Score : 1.0\n",
      "[189/1000]\n",
      "- Train Loss : 0.1520543379916085 Score : 1.0\n",
      "- Val Loss : 0.12296145409345627 Score : 1.0\n",
      "[190/1000]\n",
      "- Train Loss : 0.15113677663935554 Score : 1.0\n",
      "- Val Loss : 0.1222122386097908 Score : 1.0\n",
      "[191/1000]\n",
      "- Train Loss : 0.15019790497091082 Score : 1.0\n",
      "- Val Loss : 0.12143263965845108 Score : 1.0\n",
      "[192/1000]\n",
      "- Train Loss : 0.14927554378906885 Score : 1.0\n",
      "- Val Loss : 0.12064909934997559 Score : 1.0\n",
      "[193/1000]\n",
      "- Train Loss : 0.1483843500415484 Score : 1.0\n",
      "- Val Loss : 0.11992091685533524 Score : 1.0\n",
      "[194/1000]\n",
      "- Train Loss : 0.14750286605623034 Score : 1.0\n",
      "- Val Loss : 0.11920714378356934 Score : 1.0\n",
      "[195/1000]\n",
      "- Train Loss : 0.1465968911846479 Score : 1.0\n",
      "- Val Loss : 0.11846459656953812 Score : 1.0\n",
      "[196/1000]\n",
      "- Train Loss : 0.14570710642470253 Score : 1.0\n",
      "- Val Loss : 0.11771742254495621 Score : 1.0\n",
      "[197/1000]\n",
      "- Train Loss : 0.1448295381334093 Score : 1.0\n",
      "- Val Loss : 0.11697573959827423 Score : 1.0\n",
      "[198/1000]\n",
      "- Train Loss : 0.14396216058068806 Score : 1.0\n",
      "- Val Loss : 0.11624326556921005 Score : 1.0\n",
      "[199/1000]\n",
      "- Train Loss : 0.14311882191234165 Score : 1.0\n",
      "- Val Loss : 0.11556585878133774 Score : 1.0\n",
      "[200/1000]\n",
      "- Train Loss : 0.14228310767147276 Score : 1.0\n",
      "- Val Loss : 0.1149023100733757 Score : 1.0\n",
      "[201/1000]\n",
      "- Train Loss : 0.14142178826861912 Score : 1.0\n",
      "- Val Loss : 0.11421101540327072 Score : 1.0\n",
      "[202/1000]\n",
      "- Train Loss : 0.14057660102844238 Score : 1.0\n",
      "- Val Loss : 0.1135137528181076 Score : 1.0\n",
      "[203/1000]\n",
      "- Train Loss : 0.13974253171020085 Score : 1.0\n",
      "- Val Loss : 0.11281956732273102 Score : 1.0\n",
      "[204/1000]\n",
      "- Train Loss : 0.1389173542459806 Score : 1.0\n",
      "- Val Loss : 0.11213207989931107 Score : 1.0\n",
      "[205/1000]\n",
      "- Train Loss : 0.1380999493930075 Score : 1.0\n",
      "- Val Loss : 0.11145239323377609 Score : 1.0\n",
      "[206/1000]\n",
      "- Train Loss : 0.1372897658083174 Score : 1.0\n",
      "- Val Loss : 0.1107807606458664 Score : 1.0\n",
      "[207/1000]\n",
      "- Train Loss : 0.13648639288213518 Score : 1.0\n",
      "- Val Loss : 0.11011699587106705 Score : 1.0\n",
      "[208/1000]\n",
      "- Train Loss : 0.13568955991003248 Score : 1.0\n",
      "- Val Loss : 0.10946077853441238 Score : 1.0\n",
      "[209/1000]\n",
      "- Train Loss : 0.13489912864234713 Score : 1.0\n",
      "- Val Loss : 0.10881178826093674 Score : 1.0\n",
      "[210/1000]\n",
      "- Train Loss : 0.13411505354775322 Score : 1.0\n",
      "- Val Loss : 0.10816993564367294 Score : 1.0\n",
      "[211/1000]\n",
      "- Train Loss : 0.13333713014920553 Score : 1.0\n",
      "- Val Loss : 0.1075347289443016 Score : 1.0\n",
      "[212/1000]\n",
      "- Train Loss : 0.13256514734692043 Score : 1.0\n",
      "- Val Loss : 0.10690565407276154 Score : 1.0\n",
      "[213/1000]\n",
      "- Train Loss : 0.1317990521589915 Score : 1.0\n",
      "- Val Loss : 0.10628264397382736 Score : 1.0\n",
      "[214/1000]\n",
      "- Train Loss : 0.13103878498077393 Score : 1.0\n",
      "- Val Loss : 0.10566548258066177 Score : 1.0\n",
      "[215/1000]\n",
      "- Train Loss : 0.1302842613723543 Score : 1.0\n",
      "- Val Loss : 0.10505395382642746 Score : 1.0\n",
      "[216/1000]\n",
      "- Train Loss : 0.12953544739219877 Score : 1.0\n",
      "- Val Loss : 0.10444794595241547 Score : 1.0\n",
      "[217/1000]\n",
      "- Train Loss : 0.12879227184587055 Score : 1.0\n",
      "- Val Loss : 0.10384726524353027 Score : 1.0\n",
      "[218/1000]\n",
      "- Train Loss : 0.12805464615424475 Score : 1.0\n",
      "- Val Loss : 0.10325177013874054 Score : 1.0\n",
      "[219/1000]\n",
      "- Train Loss : 0.12732253389226067 Score : 1.0\n",
      "- Val Loss : 0.1026613712310791 Score : 1.0\n",
      "[220/1000]\n",
      "- Train Loss : 0.1265959060854382 Score : 1.0\n",
      "- Val Loss : 0.10207589715719223 Score : 1.0\n",
      "[221/1000]\n",
      "- Train Loss : 0.12587468326091766 Score : 1.0\n",
      "- Val Loss : 0.10149527341127396 Score : 1.0\n",
      "[222/1000]\n",
      "- Train Loss : 0.12515881409247717 Score : 1.0\n",
      "- Val Loss : 0.10091938078403473 Score : 1.0\n",
      "[223/1000]\n",
      "- Train Loss : 0.12444820172256893 Score : 1.0\n",
      "- Val Loss : 0.10034789144992828 Score : 1.0\n",
      "[224/1000]\n",
      "- Train Loss : 0.1237430398662885 Score : 1.0\n",
      "- Val Loss : 0.09978096932172775 Score : 1.0\n",
      "[225/1000]\n",
      "- Train Loss : 0.12304281029436323 Score : 1.0\n",
      "- Val Loss : 0.09921853989362717 Score : 1.0\n",
      "[226/1000]\n",
      "- Train Loss : 0.12234802130195829 Score : 1.0\n",
      "- Val Loss : 0.09866060316562653 Score : 1.0\n",
      "[227/1000]\n",
      "- Train Loss : 0.12165823082129161 Score : 1.0\n",
      "- Val Loss : 0.09810697287321091 Score : 1.0\n",
      "[228/1000]\n",
      "- Train Loss : 0.12097339828809102 Score : 1.0\n",
      "- Val Loss : 0.09755748510360718 Score : 1.0\n",
      "[229/1000]\n",
      "- Train Loss : 0.1202935270137257 Score : 1.0\n",
      "- Val Loss : 0.097011998295784 Score : 1.0\n",
      "[230/1000]\n",
      "- Train Loss : 0.11961867246362898 Score : 1.0\n",
      "- Val Loss : 0.09647063910961151 Score : 1.0\n",
      "[231/1000]\n",
      "- Train Loss : 0.11894875268141429 Score : 1.0\n",
      "- Val Loss : 0.09593330323696136 Score : 1.0\n",
      "[232/1000]\n",
      "- Train Loss : 0.11828372793065177 Score : 1.0\n",
      "- Val Loss : 0.09540007263422012 Score : 1.0\n",
      "[233/1000]\n",
      "- Train Loss : 0.11762352370553547 Score : 1.0\n",
      "- Val Loss : 0.09487079828977585 Score : 1.0\n",
      "[234/1000]\n",
      "- Train Loss : 0.11696808205710517 Score : 1.0\n",
      "- Val Loss : 0.09434548765420914 Score : 1.0\n",
      "[235/1000]\n",
      "- Train Loss : 0.1163173731830385 Score : 1.0\n",
      "- Val Loss : 0.09382406622171402 Score : 1.0\n",
      "[236/1000]\n",
      "- Train Loss : 0.1156713358230061 Score : 1.0\n",
      "- Val Loss : 0.09330646693706512 Score : 1.0\n",
      "[237/1000]\n",
      "- Train Loss : 0.11502995838721593 Score : 1.0\n",
      "- Val Loss : 0.09279267489910126 Score : 1.0\n",
      "[238/1000]\n",
      "- Train Loss : 0.11439320196708043 Score : 1.0\n",
      "- Val Loss : 0.09228263795375824 Score : 1.0\n",
      "[239/1000]\n",
      "- Train Loss : 0.11361114929119746 Score : 1.0\n",
      "- Val Loss : 0.08919496834278107 Score : 1.0\n",
      "[240/1000]\n",
      "- Train Loss : 0.1024665708343188 Score : 1.0\n",
      "- Val Loss : 0.07121296972036362 Score : 1.0\n",
      "[241/1000]\n",
      "- Train Loss : 0.0786217028896014 Score : 1.0\n",
      "- Val Loss : 0.050833381712436676 Score : 1.0\n",
      "[242/1000]\n",
      "- Train Loss : 0.055262485726012125 Score : 1.0\n",
      "- Val Loss : 0.03433308005332947 Score : 1.0\n",
      "[243/1000]\n",
      "- Train Loss : 0.03758843450082673 Score : 1.0\n",
      "- Val Loss : 0.02337295189499855 Score : 1.0\n",
      "[244/1000]\n",
      "- Train Loss : 0.026143054374390177 Score : 1.0\n",
      "- Val Loss : 0.01678283140063286 Score : 1.0\n",
      "[245/1000]\n",
      "- Train Loss : 0.01921353613336881 Score : 1.0\n",
      "- Val Loss : 0.01283236127346754 Score : 1.0\n",
      "[246/1000]\n",
      "- Train Loss : 0.014967038192682795 Score : 1.0\n",
      "- Val Loss : 0.010311287827789783 Score : 1.0\n",
      "[247/1000]\n",
      "- Train Loss : 0.012206403745545281 Score : 1.0\n",
      "- Val Loss : 0.008571957238018513 Score : 1.0\n",
      "[248/1000]\n",
      "- Train Loss : 0.01028735599377089 Score : 1.0\n",
      "- Val Loss : 0.007298001088202 Score : 1.0\n",
      "[249/1000]\n",
      "- Train Loss : 0.008878511842340231 Score : 1.0\n",
      "- Val Loss : 0.006326253525912762 Score : 1.0\n",
      "[250/1000]\n",
      "- Train Loss : 0.007807076236026155 Score : 1.0\n",
      "- Val Loss : 0.005566380452364683 Score : 1.0\n",
      "[251/1000]\n",
      "- Train Loss : 0.006965459997041358 Score : 1.0\n",
      "- Val Loss : 0.0049750166945159435 Score : 1.0\n",
      "[252/1000]\n",
      "- Train Loss : 0.006284019909799099 Score : 1.0\n",
      "- Val Loss : 0.004499667789787054 Score : 1.0\n",
      "[253/1000]\n",
      "- Train Loss : 0.005720112032981383 Score : 1.0\n",
      "- Val Loss : 0.004103190265595913 Score : 1.0\n",
      "[254/1000]\n",
      "- Train Loss : 0.005245188690928949 Score : 1.0\n",
      "- Val Loss : 0.0037677758373320103 Score : 1.0\n",
      "[255/1000]\n",
      "- Train Loss : 0.004842858761548996 Score : 1.0\n",
      "- Val Loss : 0.003482655854895711 Score : 1.0\n",
      "[256/1000]\n",
      "- Train Loss : 0.00449986109096143 Score : 1.0\n",
      "- Val Loss : 0.0032366078812628984 Score : 1.0\n",
      "[257/1000]\n",
      "- Train Loss : 0.004201020010643535 Score : 1.0\n",
      "- Val Loss : 0.003021599492058158 Score : 1.0\n",
      "[258/1000]\n",
      "- Train Loss : 0.003938443441357877 Score : 1.0\n",
      "- Val Loss : 0.0028321039862930775 Score : 1.0\n",
      "[259/1000]\n",
      "- Train Loss : 0.0037059938121173116 Score : 1.0\n",
      "- Val Loss : 0.002663910388946533 Score : 1.0\n",
      "[260/1000]\n",
      "- Train Loss : 0.0034980567482610545 Score : 1.0\n",
      "- Val Loss : 0.0025145874824374914 Score : 1.0\n",
      "[261/1000]\n",
      "- Train Loss : 0.0033115257716013324 Score : 1.0\n",
      "- Val Loss : 0.0023808726109564304 Score : 1.0\n",
      "[262/1000]\n",
      "- Train Loss : 0.003143202810962167 Score : 1.0\n",
      "- Val Loss : 0.002260857727378607 Score : 1.0\n",
      "[263/1000]\n",
      "- Train Loss : 0.0029906298473684322 Score : 1.0\n",
      "- Val Loss : 0.0021530583035200834 Score : 1.0\n",
      "[264/1000]\n",
      "- Train Loss : 0.0028530793885389962 Score : 1.0\n",
      "- Val Loss : 0.002056573750451207 Score : 1.0\n",
      "[265/1000]\n",
      "- Train Loss : 0.0027291494447530974 Score : 1.0\n",
      "- Val Loss : 0.001969763543456793 Score : 1.0\n",
      "[266/1000]\n",
      "- Train Loss : 0.0026163201675646836 Score : 1.0\n",
      "- Val Loss : 0.0018905490869656205 Score : 1.0\n",
      "[267/1000]\n",
      "- Train Loss : 0.0025122128815079727 Score : 1.0\n",
      "- Val Loss : 0.0018174605211243033 Score : 1.0\n",
      "[268/1000]\n",
      "- Train Loss : 0.0024157997945116628 Score : 1.0\n",
      "- Val Loss : 0.0017503913259133697 Score : 1.0\n",
      "[269/1000]\n",
      "- Train Loss : 0.0023263918313508234 Score : 1.0\n",
      "- Val Loss : 0.0016882781637832522 Score : 1.0\n",
      "[270/1000]\n",
      "- Train Loss : 0.0022436538452489507 Score : 1.0\n",
      "- Val Loss : 0.001630512299016118 Score : 1.0\n",
      "[271/1000]\n",
      "- Train Loss : 0.00216661776519484 Score : 1.0\n",
      "- Val Loss : 0.0015766978031024337 Score : 1.0\n",
      "[272/1000]\n",
      "- Train Loss : 0.002094830179380046 Score : 1.0\n",
      "- Val Loss : 0.0015274712350219488 Score : 1.0\n",
      "[273/1000]\n",
      "- Train Loss : 0.002027952668464018 Score : 1.0\n",
      "- Val Loss : 0.0014812982408329844 Score : 1.0\n",
      "[274/1000]\n",
      "- Train Loss : 0.001965197592249347 Score : 1.0\n",
      "- Val Loss : 0.001437885919585824 Score : 1.0\n",
      "[275/1000]\n",
      "- Train Loss : 0.0019061427641039093 Score : 1.0\n",
      "- Val Loss : 0.001396939973346889 Score : 1.0\n",
      "[276/1000]\n",
      "- Train Loss : 0.001850678042198221 Score : 1.0\n",
      "- Val Loss : 0.0013582586543634534 Score : 1.0\n",
      "[277/1000]\n",
      "- Train Loss : 0.001798987647311555 Score : 1.0\n",
      "- Val Loss : 0.0013217211235314608 Score : 1.0\n",
      "[278/1000]\n",
      "- Train Loss : 0.0017504169566867251 Score : 1.0\n",
      "- Val Loss : 0.001287162653170526 Score : 1.0\n",
      "[279/1000]\n",
      "- Train Loss : 0.001704429741948843 Score : 1.0\n",
      "- Val Loss : 0.0012543712509796023 Score : 1.0\n",
      "[280/1000]\n",
      "- Train Loss : 0.0016607182721296947 Score : 1.0\n",
      "- Val Loss : 0.0012232258450239897 Score : 1.0\n",
      "[281/1000]\n",
      "- Train Loss : 0.0016191392076305216 Score : 1.0\n",
      "- Val Loss : 0.001193585223518312 Score : 1.0\n",
      "[282/1000]\n",
      "- Train Loss : 0.0015795173239894211 Score : 1.0\n",
      "- Val Loss : 0.0011653073597699404 Score : 1.0\n",
      "[283/1000]\n",
      "- Train Loss : 0.001541728224967503 Score : 1.0\n",
      "- Val Loss : 0.001138250227086246 Score : 1.0\n",
      "[284/1000]\n",
      "- Train Loss : 0.001505624483494709 Score : 1.0\n",
      "- Val Loss : 0.001112348516471684 Score : 1.0\n",
      "[285/1000]\n",
      "- Train Loss : 0.0014710707861619692 Score : 1.0\n",
      "- Val Loss : 0.001087518292479217 Score : 1.0\n",
      "[286/1000]\n",
      "- Train Loss : 0.001438052794482145 Score : 1.0\n",
      "- Val Loss : 0.0010637438390403986 Score : 1.0\n",
      "[287/1000]\n",
      "- Train Loss : 0.0014065617837736176 Score : 1.0\n",
      "- Val Loss : 0.00104098382871598 Score : 1.0\n",
      "[288/1000]\n",
      "- Train Loss : 0.0013767183538422817 Score : 1.0\n",
      "- Val Loss : 0.0010191890178248286 Score : 1.0\n",
      "[289/1000]\n",
      "- Train Loss : 0.001348000077996403 Score : 1.0\n",
      "- Val Loss : 0.0009982687188312411 Score : 1.0\n",
      "[290/1000]\n",
      "- Train Loss : 0.0013203737908042967 Score : 1.0\n",
      "- Val Loss : 0.0009781422559171915 Score : 1.0\n",
      "[291/1000]\n",
      "- Train Loss : 0.0012938821455463767 Score : 1.0\n",
      "- Val Loss : 0.0009587761014699936 Score : 1.0\n",
      "[292/1000]\n",
      "- Train Loss : 0.0012683483267513414 Score : 1.0\n",
      "- Val Loss : 0.0009400809067301452 Score : 1.0\n",
      "[293/1000]\n",
      "- Train Loss : 0.0012438044650480151 Score : 1.0\n",
      "- Val Loss : 0.0009221130167134106 Score : 1.0\n",
      "[294/1000]\n",
      "- Train Loss : 0.0012202007411461738 Score : 1.0\n",
      "- Val Loss : 0.0009047450148500502 Score : 1.0\n",
      "[295/1000]\n",
      "- Train Loss : 0.0011974917142651975 Score : 1.0\n",
      "- Val Loss : 0.0008880184032022953 Score : 1.0\n",
      "[296/1000]\n",
      "- Train Loss : 0.0011755244405422774 Score : 1.0\n",
      "- Val Loss : 0.0008718547178432345 Score : 1.0\n",
      "[297/1000]\n",
      "- Train Loss : 0.0011542719536616157 Score : 1.0\n",
      "- Val Loss : 0.0008561942959204316 Score : 1.0\n",
      "[298/1000]\n",
      "- Train Loss : 0.0011337855927801381 Score : 1.0\n",
      "- Val Loss : 0.0008410583832301199 Score : 1.0\n",
      "[299/1000]\n",
      "- Train Loss : 0.0011139888536288505 Score : 1.0\n",
      "- Val Loss : 0.0008264160715043545 Score : 1.0\n",
      "[300/1000]\n",
      "- Train Loss : 0.0010947982534869677 Score : 1.0\n",
      "- Val Loss : 0.0008122142753563821 Score : 1.0\n",
      "[301/1000]\n",
      "- Train Loss : 0.0010761832430337865 Score : 1.0\n",
      "- Val Loss : 0.0007984357071109116 Score : 1.0\n",
      "[302/1000]\n",
      "- Train Loss : 0.0010581174930040208 Score : 1.0\n",
      "- Val Loss : 0.0007850488764233887 Score : 1.0\n",
      "[303/1000]\n",
      "- Train Loss : 0.0010405715082823816 Score : 1.0\n",
      "- Val Loss : 0.0007720389403402805 Score : 1.0\n",
      "[304/1000]\n",
      "- Train Loss : 0.0010236284902526273 Score : 1.0\n",
      "- Val Loss : 0.0007594031631015241 Score : 1.0\n",
      "[305/1000]\n",
      "- Train Loss : 0.001007322603577955 Score : 1.0\n",
      "- Val Loss : 0.0007471806020475924 Score : 1.0\n",
      "[306/1000]\n",
      "- Train Loss : 0.0009914360254899496 Score : 1.0\n",
      "- Val Loss : 0.0007353080436587334 Score : 1.0\n",
      "[307/1000]\n",
      "- Train Loss : 0.0009759678570036259 Score : 1.0\n",
      "- Val Loss : 0.0007237645913846791 Score : 1.0\n",
      "[308/1000]\n",
      "- Train Loss : 0.0009609087719582021 Score : 1.0\n",
      "- Val Loss : 0.000712507520802319 Score : 1.0\n",
      "[309/1000]\n",
      "- Train Loss : 0.0009462470285749683 Score : 1.0\n",
      "- Val Loss : 0.0007015550509095192 Score : 1.0\n",
      "[310/1000]\n",
      "- Train Loss : 0.0009319685923401266 Score : 1.0\n",
      "- Val Loss : 0.0006908631767146289 Score : 1.0\n",
      "[311/1000]\n",
      "- Train Loss : 0.0009180573494328806 Score : 1.0\n",
      "- Val Loss : 0.0006804453441873193 Score : 1.0\n",
      "[312/1000]\n",
      "- Train Loss : 0.000904493987844843 Score : 1.0\n",
      "- Val Loss : 0.0006702880491502583 Score : 1.0\n",
      "[313/1000]\n",
      "- Train Loss : 0.0008912725121869395 Score : 1.0\n",
      "- Val Loss : 0.0006603579968214035 Score : 1.0\n",
      "[314/1000]\n",
      "- Train Loss : 0.0008783740437744806 Score : 1.0\n",
      "- Val Loss : 0.0006506684003397822 Score : 1.0\n",
      "[315/1000]\n",
      "- Train Loss : 0.0008657921797647658 Score : 1.0\n",
      "- Val Loss : 0.000641220307443291 Score : 1.0\n",
      "[316/1000]\n",
      "- Train Loss : 0.0008535066218529311 Score : 1.0\n",
      "- Val Loss : 0.000631977804005146 Score : 1.0\n",
      "[317/1000]\n",
      "- Train Loss : 0.0008415139260857055 Score : 1.0\n",
      "- Val Loss : 0.0006229557911865413 Score : 1.0\n",
      "[318/1000]\n",
      "- Train Loss : 0.0008298001355595059 Score : 1.0\n",
      "- Val Loss : 0.000614140008110553 Score : 1.0\n",
      "[319/1000]\n",
      "- Train Loss : 0.0008183391700084838 Score : 1.0\n",
      "- Val Loss : 0.0006055353442206979 Score : 1.0\n",
      "[320/1000]\n",
      "- Train Loss : 0.000807147874082956 Score : 1.0\n",
      "- Val Loss : 0.000597104022745043 Score : 1.0\n",
      "[321/1000]\n",
      "- Train Loss : 0.0007962106707661102 Score : 1.0\n",
      "- Val Loss : 0.0005888944724574685 Score : 1.0\n",
      "[322/1000]\n",
      "- Train Loss : 0.0007855042511235095 Score : 1.0\n",
      "- Val Loss : 0.0005810441798530519 Score : 1.0\n",
      "[323/1000]\n",
      "- Train Loss : 0.0007750355968407044 Score : 1.0\n",
      "- Val Loss : 0.0005733604193665087 Score : 1.0\n",
      "[324/1000]\n",
      "- Train Loss : 0.0007649413795055201 Score : 1.0\n",
      "- Val Loss : 0.0005658566951751709 Score : 1.0\n",
      "[325/1000]\n",
      "- Train Loss : 0.0007550854643341154 Score : 1.0\n",
      "- Val Loss : 0.0005584889440797269 Score : 1.0\n",
      "[326/1000]\n",
      "- Train Loss : 0.0007454041721454511 Score : 1.0\n",
      "- Val Loss : 0.0005512724746949971 Score : 1.0\n",
      "[327/1000]\n",
      "- Train Loss : 0.000735911983712059 Score : 1.0\n",
      "- Val Loss : 0.0005442235851660371 Score : 1.0\n",
      "[328/1000]\n",
      "- Train Loss : 0.0007266111594314376 Score : 1.0\n",
      "- Val Loss : 0.0005373109597712755 Score : 1.0\n",
      "[329/1000]\n",
      "- Train Loss : 0.0007175022102374998 Score : 1.0\n",
      "- Val Loss : 0.0005305319209583104 Score : 1.0\n",
      "[330/1000]\n",
      "- Train Loss : 0.0007085817762547069 Score : 1.0\n",
      "- Val Loss : 0.0005239016027189791 Score : 1.0\n",
      "[331/1000]\n",
      "- Train Loss : 0.0006998349563218653 Score : 1.0\n",
      "- Val Loss : 0.0005174087127670646 Score : 1.0\n",
      "[332/1000]\n",
      "- Train Loss : 0.0006912668096548361 Score : 1.0\n",
      "- Val Loss : 0.0005110184429213405 Score : 1.0\n",
      "[333/1000]\n",
      "- Train Loss : 0.0006828791309898305 Score : 1.0\n",
      "- Val Loss : 0.0005047786398790777 Score : 1.0\n",
      "[334/1000]\n",
      "- Train Loss : 0.0006746882443419761 Score : 1.0\n",
      "- Val Loss : 0.0004986705607734621 Score : 1.0\n",
      "[335/1000]\n",
      "- Train Loss : 0.0006666447427253135 Score : 1.0\n",
      "- Val Loss : 0.0004927054396830499 Score : 1.0\n",
      "[336/1000]\n",
      "- Train Loss : 0.0006587596985304521 Score : 1.0\n",
      "- Val Loss : 0.00048684864304959774 Score : 1.0\n",
      "[337/1000]\n",
      "- Train Loss : 0.0006510386835240448 Score : 1.0\n",
      "- Val Loss : 0.00048111233627423644 Score : 1.0\n",
      "[338/1000]\n",
      "- Train Loss : 0.0006435119446703336 Score : 1.0\n",
      "- Val Loss : 0.0004755018453579396 Score : 1.0\n",
      "[339/1000]\n",
      "- Train Loss : 0.0006361101113725454 Score : 1.0\n",
      "- Val Loss : 0.00047000855556689203 Score : 1.0\n",
      "[340/1000]\n",
      "- Train Loss : 0.0006288362168965654 Score : 1.0\n",
      "- Val Loss : 0.00046460796147584915 Score : 1.0\n",
      "[341/1000]\n",
      "- Train Loss : 0.0006216883711102936 Score : 1.0\n",
      "- Val Loss : 0.0004593042249325663 Score : 1.0\n",
      "[342/1000]\n",
      "- Train Loss : 0.0006146711465488705 Score : 1.0\n",
      "- Val Loss : 0.0004540947556961328 Score : 1.0\n",
      "[343/1000]\n",
      "- Train Loss : 0.0006077730277967122 Score : 1.0\n",
      "- Val Loss : 0.00044896890176460147 Score : 1.0\n",
      "[344/1000]\n",
      "- Train Loss : 0.0006009951482863269 Score : 1.0\n",
      "- Val Loss : 0.00044394523138180375 Score : 1.0\n",
      "[345/1000]\n",
      "- Train Loss : 0.0005943347512382186 Score : 1.0\n",
      "- Val Loss : 0.000438994902651757 Score : 1.0\n",
      "[346/1000]\n",
      "- Train Loss : 0.0005877829454321829 Score : 1.0\n",
      "- Val Loss : 0.0004341447202023119 Score : 1.0\n",
      "[347/1000]\n",
      "- Train Loss : 0.0005813498897219284 Score : 1.0\n",
      "- Val Loss : 0.0004293456731829792 Score : 1.0\n",
      "[348/1000]\n",
      "- Train Loss : 0.0005750195204099226 Score : 1.0\n",
      "- Val Loss : 0.00042464808211661875 Score : 1.0\n",
      "[349/1000]\n",
      "- Train Loss : 0.0005687908398815327 Score : 1.0\n",
      "- Val Loss : 0.0004200309340376407 Score : 1.0\n",
      "[350/1000]\n",
      "- Train Loss : 0.0005626661020667396 Score : 1.0\n",
      "- Val Loss : 0.0004154865746386349 Score : 1.0\n",
      "[351/1000]\n",
      "- Train Loss : 0.0005566494186900349 Score : 1.0\n",
      "- Val Loss : 0.00041101474198512733 Score : 1.0\n",
      "[352/1000]\n",
      "- Train Loss : 0.0005507246694631047 Score : 1.0\n",
      "- Val Loss : 0.00040661898674443364 Score : 1.0\n",
      "[353/1000]\n",
      "- Train Loss : 0.0005448979742747421 Score : 1.0\n",
      "- Val Loss : 0.00040229185833595693 Score : 1.0\n",
      "[354/1000]\n",
      "- Train Loss : 0.0005391602899180725 Score : 1.0\n",
      "- Val Loss : 0.0003980275650974363 Score : 1.0\n",
      "[355/1000]\n",
      "- Train Loss : 0.0005335135889860491 Score : 1.0\n",
      "- Val Loss : 0.00039384138653986156 Score : 1.0\n",
      "[356/1000]\n",
      "- Train Loss : 0.0005279568625458827 Score : 1.0\n",
      "- Val Loss : 0.0003897087590303272 Score : 1.0\n",
      "[357/1000]\n",
      "- Train Loss : 0.0005224886149840637 Score : 1.0\n",
      "- Val Loss : 0.00038565401337109506 Score : 1.0\n",
      "[358/1000]\n",
      "- Train Loss : 0.0005171010561753064 Score : 1.0\n",
      "- Val Loss : 0.00038165462319739163 Score : 1.0\n",
      "[359/1000]\n",
      "- Train Loss : 0.0005117963511212212 Score : 1.0\n",
      "- Val Loss : 0.00037771990173496306 Score : 1.0\n",
      "[360/1000]\n",
      "- Train Loss : 0.000506574276692441 Score : 1.0\n",
      "- Val Loss : 0.0003738383820746094 Score : 1.0\n",
      "[361/1000]\n",
      "- Train Loss : 0.0005014294874854386 Score : 1.0\n",
      "- Val Loss : 0.0003700238012243062 Score : 1.0\n",
      "[362/1000]\n",
      "- Train Loss : 0.0004963631541209503 Score : 1.0\n",
      "- Val Loss : 0.000366264401236549 Score : 1.0\n",
      "[363/1000]\n",
      "- Train Loss : 0.0004913694914042329 Score : 1.0\n",
      "- Val Loss : 0.00036256154999136925 Score : 1.0\n",
      "[364/1000]\n",
      "- Train Loss : 0.0004864458929255812 Score : 1.0\n",
      "- Val Loss : 0.00035891434527002275 Score : 1.0\n",
      "[365/1000]\n",
      "- Train Loss : 0.0004815978350557594 Score : 1.0\n",
      "- Val Loss : 0.00035531562753021717 Score : 1.0\n",
      "[366/1000]\n",
      "- Train Loss : 0.00047682272432009794 Score : 1.0\n",
      "- Val Loss : 0.0003517709264997393 Score : 1.0\n",
      "[367/1000]\n",
      "- Train Loss : 0.0004721149097248498 Score : 1.0\n",
      "- Val Loss : 0.0003482817846816033 Score : 1.0\n",
      "[368/1000]\n",
      "- Train Loss : 0.00046748063889228634 Score : 1.0\n",
      "- Val Loss : 0.0003448388888500631 Score : 1.0\n",
      "[369/1000]\n",
      "- Train Loss : 0.00046290377536529884 Score : 1.0\n",
      "- Val Loss : 0.000341456790920347 Score : 1.0\n",
      "[370/1000]\n",
      "- Train Loss : 0.0004583957165272699 Score : 1.0\n",
      "- Val Loss : 0.0003381066781003028 Score : 1.0\n",
      "[371/1000]\n",
      "- Train Loss : 0.00045395616163861833 Score : 1.0\n",
      "- Val Loss : 0.0003348181489855051 Score : 1.0\n",
      "[372/1000]\n",
      "- Train Loss : 0.00044957198002116964 Score : 1.0\n",
      "- Val Loss : 0.0003315671347081661 Score : 1.0\n",
      "[373/1000]\n",
      "- Train Loss : 0.0004452443972695619 Score : 1.0\n",
      "- Val Loss : 0.0003283741243649274 Score : 1.0\n",
      "[374/1000]\n",
      "- Train Loss : 0.000440982020033213 Score : 1.0\n",
      "- Val Loss : 0.00032520273816771805 Score : 1.0\n",
      "[375/1000]\n",
      "- Train Loss : 0.0004367911719277294 Score : 1.0\n",
      "- Val Loss : 0.0003221058286726475 Score : 1.0\n",
      "[376/1000]\n",
      "- Train Loss : 0.00043264264402548887 Score : 1.0\n",
      "- Val Loss : 0.0003190334828104824 Score : 1.0\n",
      "[377/1000]\n",
      "- Train Loss : 0.000428549578322822 Score : 1.0\n",
      "- Val Loss : 0.0003160024934913963 Score : 1.0\n",
      "[378/1000]\n",
      "- Train Loss : 0.00042452034055410576 Score : 1.0\n",
      "- Val Loss : 0.00031301673152484 Score : 1.0\n",
      "[379/1000]\n",
      "- Train Loss : 0.00042054006189573556 Score : 1.0\n",
      "- Val Loss : 0.0003100779722444713 Score : 1.0\n",
      "[380/1000]\n",
      "- Train Loss : 0.00041661546371566754 Score : 1.0\n",
      "- Val Loss : 0.00030717067420482635 Score : 1.0\n",
      "[381/1000]\n",
      "- Train Loss : 0.0004127390630957153 Score : 1.0\n",
      "- Val Loss : 0.0003043075848836452 Score : 1.0\n",
      "[382/1000]\n",
      "- Train Loss : 0.000408923820941709 Score : 1.0\n",
      "- Val Loss : 0.00030147956567816436 Score : 1.0\n",
      "[383/1000]\n",
      "- Train Loss : 0.00040514916569615406 Score : 1.0\n",
      "- Val Loss : 0.00029868222190998495 Score : 1.0\n",
      "[384/1000]\n",
      "- Train Loss : 0.0004014717730266663 Score : 1.0\n",
      "- Val Loss : 0.00029592812643386424 Score : 1.0\n",
      "[385/1000]\n",
      "- Train Loss : 0.00039786298616996244 Score : 1.0\n",
      "- Val Loss : 0.0002931917260866612 Score : 1.0\n",
      "[386/1000]\n",
      "- Train Loss : 0.00039428905276710994 Score : 1.0\n",
      "- Val Loss : 0.00029049694421701133 Score : 1.0\n",
      "[387/1000]\n",
      "- Train Loss : 0.00039074822173764306 Score : 1.0\n",
      "- Val Loss : 0.0002878519590012729 Score : 1.0\n",
      "[388/1000]\n",
      "- Train Loss : 0.00038724924767545116 Score : 1.0\n",
      "- Val Loss : 0.0002852175384759903 Score : 1.0\n",
      "[389/1000]\n",
      "- Train Loss : 0.00038380340912327584 Score : 1.0\n",
      "- Val Loss : 0.0002826396084856242 Score : 1.0\n",
      "[390/1000]\n",
      "- Train Loss : 0.0003803971315695283 Score : 1.0\n",
      "- Val Loss : 0.0002800778311211616 Score : 1.0\n",
      "[391/1000]\n",
      "- Train Loss : 0.00037702787570500124 Score : 1.0\n",
      "- Val Loss : 0.0002775499306153506 Score : 1.0\n",
      "[392/1000]\n",
      "- Train Loss : 0.00037370549398474395 Score : 1.0\n",
      "- Val Loss : 0.00027505980688147247 Score : 1.0\n",
      "[393/1000]\n",
      "- Train Loss : 0.00037042289396696206 Score : 1.0\n",
      "- Val Loss : 0.00027260591741651297 Score : 1.0\n",
      "[394/1000]\n",
      "- Train Loss : 0.0003671906557024664 Score : 1.0\n",
      "- Val Loss : 0.00027017906541004777 Score : 1.0\n",
      "[395/1000]\n",
      "- Train Loss : 0.00036398697799692553 Score : 1.0\n",
      "- Val Loss : 0.00026777174207381904 Score : 1.0\n",
      "[396/1000]\n",
      "- Train Loss : 0.000360829663501742 Score : 1.0\n",
      "- Val Loss : 0.0002653980045579374 Score : 1.0\n",
      "[397/1000]\n",
      "- Train Loss : 0.00035771022925877734 Score : 1.0\n",
      "- Val Loss : 0.000263067486230284 Score : 1.0\n",
      "[398/1000]\n",
      "- Train Loss : 0.00035463515733782615 Score : 1.0\n",
      "- Val Loss : 0.0002607527712825686 Score : 1.0\n",
      "[399/1000]\n",
      "- Train Loss : 0.00035159270838549774 Score : 1.0\n",
      "- Val Loss : 0.0002584689646027982 Score : 1.0\n",
      "[400/1000]\n",
      "- Train Loss : 0.0003485909667991412 Score : 1.0\n",
      "- Val Loss : 0.0002562203153502196 Score : 1.0\n",
      "[401/1000]\n",
      "- Train Loss : 0.0003456225111019901 Score : 1.0\n",
      "- Val Loss : 0.000253990845521912 Score : 1.0\n",
      "[402/1000]\n",
      "- Train Loss : 0.0003426960570828265 Score : 1.0\n",
      "- Val Loss : 0.00025179897784255445 Score : 1.0\n",
      "[403/1000]\n",
      "- Train Loss : 0.00033980117020999186 Score : 1.0\n",
      "- Val Loss : 0.000249618839006871 Score : 1.0\n",
      "[404/1000]\n",
      "- Train Loss : 0.0003369393525645137 Score : 1.0\n",
      "- Val Loss : 0.00024748078431002796 Score : 1.0\n",
      "[405/1000]\n",
      "- Train Loss : 0.0003341100018587895 Score : 1.0\n",
      "- Val Loss : 0.00024535073316656053 Score : 1.0\n",
      "[406/1000]\n",
      "- Train Loss : 0.00033131726377177984 Score : 1.0\n",
      "- Val Loss : 0.00024325796402990818 Score : 1.0\n",
      "[407/1000]\n",
      "- Train Loss : 0.0003285534686357197 Score : 1.0\n",
      "- Val Loss : 0.00024118089640978724 Score : 1.0\n",
      "[408/1000]\n",
      "- Train Loss : 0.00032582245896466903 Score : 1.0\n",
      "- Val Loss : 0.00023913593031466007 Score : 1.0\n",
      "[409/1000]\n",
      "- Train Loss : 0.00032311751500755135 Score : 1.0\n",
      "- Val Loss : 0.00023711417452432215 Score : 1.0\n",
      "[410/1000]\n",
      "- Train Loss : 0.00032044864443984505 Score : 1.0\n",
      "- Val Loss : 0.0002351236471440643 Score : 1.0\n",
      "[411/1000]\n",
      "- Train Loss : 0.00031781120762591146 Score : 1.0\n",
      "- Val Loss : 0.00023313412384595722 Score : 1.0\n",
      "[412/1000]\n",
      "- Train Loss : 0.00031519860203843564 Score : 1.0\n",
      "- Val Loss : 0.00023118565150070935 Score : 1.0\n",
      "[413/1000]\n",
      "- Train Loss : 0.0003126153913197211 Score : 1.0\n",
      "- Val Loss : 0.00022924579388927668 Score : 1.0\n",
      "[414/1000]\n",
      "- Train Loss : 0.0003100665708188899 Score : 1.0\n",
      "- Val Loss : 0.00022733576770406216 Score : 1.0\n",
      "[415/1000]\n",
      "- Train Loss : 0.0003075444619753398 Score : 1.0\n",
      "- Val Loss : 0.00022545325919054449 Score : 1.0\n",
      "[416/1000]\n",
      "- Train Loss : 0.00030504597089020535 Score : 1.0\n",
      "- Val Loss : 0.0002235679276054725 Score : 1.0\n",
      "[417/1000]\n",
      "- Train Loss : 0.00030257578085487086 Score : 1.0\n",
      "- Val Loss : 0.00022172402532305568 Score : 1.0\n",
      "[418/1000]\n",
      "- Train Loss : 0.00030013018113095313 Score : 1.0\n",
      "- Val Loss : 0.00021990019013173878 Score : 1.0\n",
      "[419/1000]\n",
      "- Train Loss : 0.00029771736040452705 Score : 1.0\n",
      "- Val Loss : 0.00021809397730976343 Score : 1.0\n",
      "[420/1000]\n",
      "- Train Loss : 0.00029533019551308826 Score : 1.0\n",
      "- Val Loss : 0.00021630513947457075 Score : 1.0\n",
      "[421/1000]\n",
      "- Train Loss : 0.00029296987243772793 Score : 1.0\n",
      "- Val Loss : 0.00021454192756209522 Score : 1.0\n",
      "[422/1000]\n",
      "- Train Loss : 0.00029062546107323014 Score : 1.0\n",
      "- Val Loss : 0.00021279156499076635 Score : 1.0\n",
      "[423/1000]\n",
      "- Train Loss : 0.00028831319003883336 Score : 1.0\n",
      "- Val Loss : 0.00021105784981045872 Score : 1.0\n",
      "[424/1000]\n",
      "- Train Loss : 0.00028602066757028096 Score : 1.0\n",
      "- Val Loss : 0.00020934906206093729 Score : 1.0\n",
      "[425/1000]\n",
      "- Train Loss : 0.00028375559971512604 Score : 1.0\n",
      "- Val Loss : 0.0002076545060845092 Score : 1.0\n",
      "[426/1000]\n",
      "- Train Loss : 0.0002815157810497719 Score : 1.0\n",
      "- Val Loss : 0.00020597824186552316 Score : 1.0\n",
      "[427/1000]\n",
      "- Train Loss : 0.0002792995275942505 Score : 1.0\n",
      "- Val Loss : 0.00020432642486412078 Score : 1.0\n",
      "[428/1000]\n",
      "- Train Loss : 0.0002771020897651195 Score : 1.0\n",
      "- Val Loss : 0.00020268195657990873 Score : 1.0\n",
      "[429/1000]\n",
      "- Train Loss : 0.00027492986717132025 Score : 1.0\n",
      "- Val Loss : 0.0002010656608035788 Score : 1.0\n",
      "[430/1000]\n",
      "- Train Loss : 0.0002727816034975048 Score : 1.0\n",
      "- Val Loss : 0.00019945848907809705 Score : 1.0\n",
      "[431/1000]\n",
      "- Train Loss : 0.00027065088781657524 Score : 1.0\n",
      "- Val Loss : 0.00019787442579399794 Score : 1.0\n",
      "[432/1000]\n",
      "- Train Loss : 0.0002685461044570224 Score : 1.0\n",
      "- Val Loss : 0.00019631267059594393 Score : 1.0\n",
      "[433/1000]\n",
      "- Train Loss : 0.0002664623122351865 Score : 1.0\n",
      "- Val Loss : 0.0001947721466422081 Score : 1.0\n",
      "[434/1000]\n",
      "- Train Loss : 0.00026440317823370505 Score : 1.0\n",
      "- Val Loss : 0.0001932461600517854 Score : 1.0\n",
      "[435/1000]\n",
      "- Train Loss : 0.00026235999798195227 Score : 1.0\n",
      "- Val Loss : 0.00019173265900462866 Score : 1.0\n",
      "[436/1000]\n",
      "- Train Loss : 0.00026034125686338585 Score : 1.0\n",
      "- Val Loss : 0.0001902484509628266 Score : 1.0\n",
      "[437/1000]\n",
      "- Train Loss : 0.0002583380426383681 Score : 1.0\n",
      "- Val Loss : 0.00018876358808483928 Score : 1.0\n",
      "[438/1000]\n",
      "- Train Loss : 0.0002563607356730952 Score : 1.0\n",
      "- Val Loss : 0.00018729282601270825 Score : 1.0\n",
      "[439/1000]\n",
      "- Train Loss : 0.0002543967420933768 Score : 1.0\n",
      "- Val Loss : 0.00018584911595098674 Score : 1.0\n",
      "[440/1000]\n",
      "- Train Loss : 0.00025245240815113194 Score : 1.0\n",
      "- Val Loss : 0.00018440655549056828 Score : 1.0\n",
      "[441/1000]\n",
      "- Train Loss : 0.0002505357697373256 Score : 1.0\n",
      "- Val Loss : 0.00018299084331374615 Score : 1.0\n",
      "[442/1000]\n",
      "- Train Loss : 0.00024862852620167867 Score : 1.0\n",
      "- Val Loss : 0.0001815866125980392 Score : 1.0\n",
      "[443/1000]\n",
      "- Train Loss : 0.00024674579536723386 Score : 1.0\n",
      "- Val Loss : 0.0001801959879230708 Score : 1.0\n",
      "[444/1000]\n",
      "- Train Loss : 0.00024488207418471575 Score : 1.0\n",
      "- Val Loss : 0.00017881263920571655 Score : 1.0\n",
      "[445/1000]\n",
      "- Train Loss : 0.00024304020431979248 Score : 1.0\n",
      "- Val Loss : 0.0001774472329998389 Score : 1.0\n",
      "[446/1000]\n",
      "- Train Loss : 0.00024121001155840026 Score : 1.0\n",
      "- Val Loss : 0.00017609524365980178 Score : 1.0\n",
      "[447/1000]\n",
      "- Train Loss : 0.00023939182897770984 Score : 1.0\n",
      "- Val Loss : 0.0001747778878780082 Score : 1.0\n",
      "[448/1000]\n",
      "- Train Loss : 0.00023760561291257746 Score : 1.0\n",
      "- Val Loss : 0.0001735036785248667 Score : 1.0\n",
      "[449/1000]\n",
      "- Train Loss : 0.00023582590640419058 Score : 1.0\n",
      "- Val Loss : 0.00017222159658558667 Score : 1.0\n",
      "[450/1000]\n",
      "- Train Loss : 0.0002340667492212055 Score : 1.0\n",
      "- Val Loss : 0.00017096110968850553 Score : 1.0\n",
      "[451/1000]\n",
      "- Train Loss : 0.00023232312701616643 Score : 1.0\n",
      "- Val Loss : 0.00016972202865872532 Score : 1.0\n",
      "[452/1000]\n",
      "- Train Loss : 0.00023060098222534484 Score : 1.0\n",
      "- Val Loss : 0.00016848118684720248 Score : 1.0\n",
      "[453/1000]\n",
      "- Train Loss : 0.00022889030353528343 Score : 1.0\n",
      "- Val Loss : 0.00016725968453101814 Score : 1.0\n",
      "[454/1000]\n",
      "- Train Loss : 0.00022719962766536305 Score : 1.0\n",
      "- Val Loss : 0.00016605119162704796 Score : 1.0\n",
      "[455/1000]\n",
      "- Train Loss : 0.00022552458621147607 Score : 1.0\n",
      "- Val Loss : 0.0001648426114115864 Score : 1.0\n",
      "[456/1000]\n",
      "- Train Loss : 0.00022386167741691074 Score : 1.0\n",
      "- Val Loss : 0.000163655451615341 Score : 1.0\n",
      "[457/1000]\n",
      "- Train Loss : 0.00022221259536713155 Score : 1.0\n",
      "- Val Loss : 0.000162468189955689 Score : 1.0\n",
      "[458/1000]\n",
      "- Train Loss : 0.00022058829603742601 Score : 1.0\n",
      "- Val Loss : 0.00016129770665429533 Score : 1.0\n",
      "[459/1000]\n",
      "- Train Loss : 0.00021897209525276493 Score : 1.0\n",
      "- Val Loss : 0.00016014416178222746 Score : 1.0\n",
      "[460/1000]\n",
      "- Train Loss : 0.00021736999284864092 Score : 1.0\n",
      "- Val Loss : 0.0001589944731676951 Score : 1.0\n",
      "[461/1000]\n",
      "- Train Loss : 0.00021579057970989702 Score : 1.0\n",
      "- Val Loss : 0.00015786374569870532 Score : 1.0\n",
      "[462/1000]\n",
      "- Train Loss : 0.00021421308297754472 Score : 1.0\n",
      "- Val Loss : 0.00015673486632294953 Score : 1.0\n",
      "[463/1000]\n",
      "- Train Loss : 0.00021266415084634596 Score : 1.0\n",
      "- Val Loss : 0.0001556182833155617 Score : 1.0\n",
      "[464/1000]\n",
      "- Train Loss : 0.00021112161226079075 Score : 1.0\n",
      "- Val Loss : 0.00015451619401574135 Score : 1.0\n",
      "[465/1000]\n",
      "- Train Loss : 0.00020959808292294232 Score : 1.0\n",
      "- Val Loss : 0.00015341571997851133 Score : 1.0\n",
      "[466/1000]\n",
      "- Train Loss : 0.000208085556449886 Score : 1.0\n",
      "- Val Loss : 0.00015232746955007315 Score : 1.0\n",
      "[467/1000]\n",
      "- Train Loss : 0.0002065825594602049 Score : 1.0\n",
      "- Val Loss : 0.00015125353820621967 Score : 1.0\n",
      "[468/1000]\n",
      "- Train Loss : 0.000205096329106406 Score : 1.0\n",
      "- Val Loss : 0.00015019594866316766 Score : 1.0\n",
      "[469/1000]\n",
      "- Train Loss : 0.00020362415509427793 Score : 1.0\n",
      "- Val Loss : 0.00014913557970430702 Score : 1.0\n",
      "[470/1000]\n",
      "- Train Loss : 0.00020216992844426486 Score : 1.0\n",
      "- Val Loss : 0.00014809156709816307 Score : 1.0\n",
      "[471/1000]\n",
      "- Train Loss : 0.00020072096433270618 Score : 1.0\n",
      "- Val Loss : 0.00014705504872836173 Score : 1.0\n",
      "[472/1000]\n",
      "- Train Loss : 0.0001992844178554757 Score : 1.0\n",
      "- Val Loss : 0.00014602215378545225 Score : 1.0\n",
      "[473/1000]\n",
      "- Train Loss : 0.0001978683527947093 Score : 1.0\n",
      "- Val Loss : 0.00014499899407383054 Score : 1.0\n",
      "[474/1000]\n",
      "- Train Loss : 0.00019646201285973398 Score : 1.0\n",
      "- Val Loss : 0.00014398552593775094 Score : 1.0\n",
      "[475/1000]\n",
      "- Train Loss : 0.00019507326861558895 Score : 1.0\n",
      "- Val Loss : 0.0001429987751180306 Score : 1.0\n",
      "[476/1000]\n",
      "- Train Loss : 0.00019368350735425742 Score : 1.0\n",
      "- Val Loss : 0.00014199831639416516 Score : 1.0\n",
      "[477/1000]\n",
      "- Train Loss : 0.00019231387098746686 Score : 1.0\n",
      "- Val Loss : 0.00014101168198976666 Score : 1.0\n",
      "[478/1000]\n",
      "- Train Loss : 0.0001909590553421165 Score : 1.0\n",
      "- Val Loss : 0.00014003893011249602 Score : 1.0\n",
      "[479/1000]\n",
      "- Train Loss : 0.0001896123418797894 Score : 1.0\n",
      "- Val Loss : 0.0001390757242916152 Score : 1.0\n",
      "[480/1000]\n",
      "- Train Loss : 0.00018828421202164868 Score : 1.0\n",
      "- Val Loss : 0.00013811553071718663 Score : 1.0\n",
      "[481/1000]\n",
      "- Train Loss : 0.00018695924437553104 Score : 1.0\n",
      "- Val Loss : 0.00013716267130803317 Score : 1.0\n",
      "[482/1000]\n",
      "- Train Loss : 0.00018564569594066901 Score : 1.0\n",
      "- Val Loss : 0.00013621711696032435 Score : 1.0\n",
      "[483/1000]\n",
      "- Train Loss : 0.00018434986001617895 Score : 1.0\n",
      "- Val Loss : 0.00013528738054446876 Score : 1.0\n",
      "[484/1000]\n",
      "- Train Loss : 0.00018306047665343309 Score : 1.0\n",
      "- Val Loss : 0.0001343563199043274 Score : 1.0\n",
      "[485/1000]\n",
      "- Train Loss : 0.0001817829252104275 Score : 1.0\n",
      "- Val Loss : 0.0001334410917479545 Score : 1.0\n",
      "[486/1000]\n",
      "- Train Loss : 0.0001805236574404666 Score : 1.0\n",
      "- Val Loss : 0.00013252439384814352 Score : 1.0\n",
      "[487/1000]\n",
      "- Train Loss : 0.00017929190572532307 Score : 1.0\n",
      "- Val Loss : 0.00013157223293092102 Score : 1.0\n",
      "[488/1000]\n",
      "- Train Loss : 0.00017818195313642113 Score : 1.0\n",
      "- Val Loss : 0.00013055159070063382 Score : 1.0\n",
      "[489/1000]\n",
      "- Train Loss : 0.00017687530716001574 Score : 1.0\n",
      "- Val Loss : 0.00012961945321876556 Score : 1.0\n",
      "[490/1000]\n",
      "- Train Loss : 0.00017562701921431452 Score : 1.0\n",
      "- Val Loss : 0.0001287205668631941 Score : 1.0\n",
      "[491/1000]\n",
      "- Train Loss : 0.00017440233892153224 Score : 1.0\n",
      "- Val Loss : 0.0001278549898415804 Score : 1.0\n",
      "[492/1000]\n",
      "- Train Loss : 0.00017319650861382898 Score : 1.0\n",
      "- Val Loss : 0.00012700453225988895 Score : 1.0\n",
      "[493/1000]\n",
      "- Train Loss : 0.00017204251263238903 Score : 1.0\n",
      "- Val Loss : 0.00012609818077180535 Score : 1.0\n",
      "[494/1000]\n",
      "- Train Loss : 0.00017097943135821779 Score : 1.0\n",
      "- Val Loss : 0.00012514267291408032 Score : 1.0\n",
      "[495/1000]\n",
      "- Train Loss : 0.00016973697913474299 Score : 1.0\n",
      "- Val Loss : 0.00012425849854480475 Score : 1.0\n",
      "[496/1000]\n",
      "- Train Loss : 0.00016855200253000172 Score : 1.0\n",
      "- Val Loss : 0.0001234170631505549 Score : 1.0\n",
      "[497/1000]\n",
      "- Train Loss : 0.0001673875360413351 Score : 1.0\n",
      "- Val Loss : 0.00012258962669875473 Score : 1.0\n",
      "[498/1000]\n",
      "- Train Loss : 0.00016627189607889805 Score : 1.0\n",
      "- Val Loss : 0.0001217383542098105 Score : 1.0\n",
      "[499/1000]\n",
      "- Train Loss : 0.00016526199720424807 Score : 1.0\n",
      "- Val Loss : 0.00012080808664904907 Score : 1.0\n",
      "[500/1000]\n",
      "- Train Loss : 0.00016406658485518873 Score : 1.0\n",
      "- Val Loss : 0.00011997052206424996 Score : 1.0\n",
      "[501/1000]\n",
      "- Train Loss : 0.00016292908953295814 Score : 1.0\n",
      "- Val Loss : 0.00011918593372683972 Score : 1.0\n",
      "[502/1000]\n",
      "- Train Loss : 0.00016181095715081837 Score : 1.0\n",
      "- Val Loss : 0.00011839245416922495 Score : 1.0\n",
      "[503/1000]\n",
      "- Train Loss : 0.000160768098592396 Score : 1.0\n",
      "- Val Loss : 0.00011757617176044732 Score : 1.0\n",
      "[504/1000]\n",
      "- Train Loss : 0.00015977323144195706 Score : 1.0\n",
      "- Val Loss : 0.00011670047388179228 Score : 1.0\n",
      "[505/1000]\n",
      "- Train Loss : 0.0001586316991759102 Score : 1.0\n",
      "- Val Loss : 0.00011590807844186202 Score : 1.0\n",
      "[506/1000]\n",
      "- Train Loss : 0.0001575392804321988 Score : 1.0\n",
      "- Val Loss : 0.00011514506331877783 Score : 1.0\n",
      "[507/1000]\n",
      "- Train Loss : 0.00015649292132972428 Score : 1.0\n",
      "- Val Loss : 0.0001143557601608336 Score : 1.0\n",
      "[508/1000]\n",
      "- Train Loss : 0.0001555581127629719 Score : 1.0\n",
      "- Val Loss : 0.00011349457054166123 Score : 1.0\n",
      "[509/1000]\n",
      "- Train Loss : 0.0001544523163627471 Score : 1.0\n",
      "- Val Loss : 0.0001127259893110022 Score : 1.0\n",
      "[510/1000]\n",
      "- Train Loss : 0.0001533949576292394 Score : 1.0\n",
      "- Val Loss : 0.00011198566062375903 Score : 1.0\n",
      "[511/1000]\n",
      "- Train Loss : 0.00015237368400751924 Score : 1.0\n",
      "- Val Loss : 0.00011123263539047912 Score : 1.0\n",
      "[512/1000]\n",
      "- Train Loss : 0.00015147879538643692 Score : 1.0\n",
      "- Val Loss : 0.00011040749086532742 Score : 1.0\n",
      "[513/1000]\n",
      "- Train Loss : 0.00015041150375812626 Score : 1.0\n",
      "- Val Loss : 0.00010966507397824898 Score : 1.0\n",
      "[514/1000]\n",
      "- Train Loss : 0.00014938040516830774 Score : 1.0\n",
      "- Val Loss : 0.00010896620369749144 Score : 1.0\n",
      "[515/1000]\n",
      "- Train Loss : 0.00014839971320018068 Score : 1.0\n",
      "- Val Loss : 0.00010822957119671628 Score : 1.0\n",
      "[516/1000]\n",
      "- Train Loss : 0.00014752741319373145 Score : 1.0\n",
      "- Val Loss : 0.00010743097664089873 Score : 1.0\n",
      "[517/1000]\n",
      "- Train Loss : 0.00014648980181340853 Score : 1.0\n",
      "- Val Loss : 0.00010671614290913567 Score : 1.0\n",
      "[518/1000]\n",
      "- Train Loss : 0.00014549791699715165 Score : 1.0\n",
      "- Val Loss : 0.00010602852125884965 Score : 1.0\n",
      "[519/1000]\n",
      "- Train Loss : 0.00014456403854031427 Score : 1.0\n",
      "- Val Loss : 0.00010531973384786397 Score : 1.0\n",
      "[520/1000]\n",
      "- Train Loss : 0.00014370799100207578 Score : 1.0\n",
      "- Val Loss : 0.00010455925075802952 Score : 1.0\n",
      "[521/1000]\n",
      "- Train Loss : 0.00014270388176858736 Score : 1.0\n",
      "- Val Loss : 0.0001038709087879397 Score : 1.0\n",
      "[522/1000]\n",
      "- Train Loss : 0.00014173569798003882 Score : 1.0\n",
      "- Val Loss : 0.00010320635192329064 Score : 1.0\n",
      "[523/1000]\n",
      "- Train Loss : 0.00014084920500560352 Score : 1.0\n",
      "- Val Loss : 0.00010252265929011628 Score : 1.0\n",
      "[524/1000]\n",
      "- Train Loss : 0.00013999892043001536 Score : 1.0\n",
      "- Val Loss : 0.0001017810427583754 Score : 1.0\n",
      "[525/1000]\n",
      "- Train Loss : 0.00013902571441172363 Score : 1.0\n",
      "- Val Loss : 0.0001011130734696053 Score : 1.0\n",
      "[526/1000]\n",
      "- Train Loss : 0.0001381096628594807 Score : 1.0\n",
      "- Val Loss : 0.00010043739166576415 Score : 1.0\n",
      "[527/1000]\n",
      "- Train Loss : 0.00013731771751837287 Score : 1.0\n",
      "- Val Loss : 9.970775863621384e-05 Score : 1.0\n",
      "[528/1000]\n",
      "- Train Loss : 0.0001363636114951482 Score : 1.0\n",
      "- Val Loss : 9.906269406201318e-05 Score : 1.0\n",
      "[529/1000]\n",
      "- Train Loss : 0.00013544472737218408 Score : 1.0\n",
      "- Val Loss : 9.843659063335508e-05 Score : 1.0\n",
      "[530/1000]\n",
      "- Train Loss : 0.00013459214137078056 Score : 1.0\n",
      "- Val Loss : 9.779339598026127e-05 Score : 1.0\n",
      "[531/1000]\n",
      "- Train Loss : 0.00013380613664695475 Score : 1.0\n",
      "- Val Loss : 9.709967707749456e-05 Score : 1.0\n",
      "[532/1000]\n",
      "- Train Loss : 0.00013288181900053233 Score : 1.0\n",
      "- Val Loss : 9.647045226301998e-05 Score : 1.0\n",
      "[533/1000]\n",
      "- Train Loss : 0.00013201411760140522 Score : 1.0\n",
      "- Val Loss : 9.583805513102561e-05 Score : 1.0\n",
      "[534/1000]\n",
      "- Train Loss : 0.00013126369453352204 Score : 1.0\n",
      "- Val Loss : 9.514770499663427e-05 Score : 1.0\n",
      "[535/1000]\n",
      "- Train Loss : 0.0001303540925113743 Score : 1.0\n",
      "- Val Loss : 9.453068196307868e-05 Score : 1.0\n",
      "[536/1000]\n",
      "- Train Loss : 0.0001294836802117061 Score : 1.0\n",
      "- Val Loss : 9.394739754498005e-05 Score : 1.0\n",
      "[537/1000]\n",
      "- Train Loss : 0.0001286908310349746 Score : 1.0\n",
      "- Val Loss : 9.33397896005772e-05 Score : 1.0\n",
      "[538/1000]\n",
      "- Train Loss : 0.00012793378987731153 Score : 1.0\n",
      "- Val Loss : 9.268883877666667e-05 Score : 1.0\n",
      "[539/1000]\n",
      "- Train Loss : 0.00012705075945026087 Score : 1.0\n",
      "- Val Loss : 9.209371637552977e-05 Score : 1.0\n",
      "[540/1000]\n",
      "- Train Loss : 0.00012624678998286577 Score : 1.0\n",
      "- Val Loss : 9.148695244221017e-05 Score : 1.0\n",
      "[541/1000]\n",
      "- Train Loss : 0.00012551324612609783 Score : 1.0\n",
      "- Val Loss : 9.084836347028613e-05 Score : 1.0\n",
      "[542/1000]\n",
      "- Train Loss : 0.00012465321762849472 Score : 1.0\n",
      "- Val Loss : 9.026371117215604e-05 Score : 1.0\n",
      "[543/1000]\n",
      "- Train Loss : 0.00012384835483519256 Score : 1.0\n",
      "- Val Loss : 8.967355825006962e-05 Score : 1.0\n",
      "[544/1000]\n",
      "- Train Loss : 0.00012315293214568455 Score : 1.0\n",
      "- Val Loss : 8.904466812964529e-05 Score : 1.0\n",
      "[545/1000]\n",
      "- Train Loss : 0.00012231316597737733 Score : 1.0\n",
      "- Val Loss : 8.847662684274837e-05 Score : 1.0\n",
      "[546/1000]\n",
      "- Train Loss : 0.00012151059187696471 Score : 1.0\n",
      "- Val Loss : 8.79095314303413e-05 Score : 1.0\n",
      "[547/1000]\n",
      "- Train Loss : 0.00012084460932884313 Score : 1.0\n",
      "- Val Loss : 8.728981629246846e-05 Score : 1.0\n",
      "[548/1000]\n",
      "- Train Loss : 0.00012002019138890319 Score : 1.0\n",
      "- Val Loss : 8.674176933709532e-05 Score : 1.0\n",
      "[549/1000]\n",
      "- Train Loss : 0.00011922202368926567 Score : 1.0\n",
      "- Val Loss : 8.621130109531805e-05 Score : 1.0\n",
      "[550/1000]\n",
      "- Train Loss : 0.00011851584834706348 Score : 1.0\n",
      "- Val Loss : 8.568003249820322e-05 Score : 1.0\n",
      "[551/1000]\n",
      "- Train Loss : 0.00011781119004202385 Score : 1.0\n",
      "- Val Loss : 8.508509927196428e-05 Score : 1.0\n",
      "[552/1000]\n",
      "- Train Loss : 0.00011702682624874999 Score : 1.0\n",
      "- Val Loss : 8.452068868791685e-05 Score : 1.0\n",
      "[553/1000]\n",
      "- Train Loss : 0.00011638157027644209 Score : 1.0\n",
      "- Val Loss : 8.392286690650508e-05 Score : 1.0\n",
      "[554/1000]\n",
      "- Train Loss : 0.00011558727884322353 Score : 1.0\n",
      "- Val Loss : 8.339015766978264e-05 Score : 1.0\n",
      "[555/1000]\n",
      "- Train Loss : 0.00011482142690510955 Score : 1.0\n",
      "- Val Loss : 8.28900738270022e-05 Score : 1.0\n",
      "[556/1000]\n",
      "- Train Loss : 0.00011412836465751752 Score : 1.0\n",
      "- Val Loss : 8.237055590143427e-05 Score : 1.0\n",
      "[557/1000]\n",
      "- Train Loss : 0.0001134756065665796 Score : 1.0\n",
      "- Val Loss : 8.181567682186142e-05 Score : 1.0\n",
      "[558/1000]\n",
      "- Train Loss : 0.00011271391465723152 Score : 1.0\n",
      "- Val Loss : 8.126469037961215e-05 Score : 1.0\n",
      "[559/1000]\n",
      "- Train Loss : 0.00011210311155688639 Score : 1.0\n",
      "- Val Loss : 8.069415343925357e-05 Score : 1.0\n",
      "[560/1000]\n",
      "- Train Loss : 0.0001113418701505806 Score : 1.0\n",
      "- Val Loss : 8.019631786737591e-05 Score : 1.0\n",
      "[561/1000]\n",
      "- Train Loss : 0.0001106142321481861 Score : 1.0\n",
      "- Val Loss : 7.971590821398422e-05 Score : 1.0\n",
      "[562/1000]\n",
      "- Train Loss : 0.00010995409461934792 Score : 1.0\n",
      "- Val Loss : 7.9235942394007e-05 Score : 1.0\n",
      "[563/1000]\n",
      "- Train Loss : 0.00010932372707530804 Score : 1.0\n",
      "- Val Loss : 7.868678221711889e-05 Score : 1.0\n",
      "[564/1000]\n",
      "- Train Loss : 0.00010859292281869178 Score : 1.0\n",
      "- Val Loss : 7.816996367182583e-05 Score : 1.0\n",
      "[565/1000]\n",
      "- Train Loss : 0.00010801411190186627 Score : 1.0\n",
      "- Val Loss : 7.762660970911384e-05 Score : 1.0\n",
      "[566/1000]\n",
      "- Train Loss : 0.00010728582775401365 Score : 1.0\n",
      "- Val Loss : 7.714604726061225e-05 Score : 1.0\n",
      "[567/1000]\n",
      "- Train Loss : 0.00010658158842286664 Score : 1.0\n",
      "- Val Loss : 7.669538172194734e-05 Score : 1.0\n",
      "[568/1000]\n",
      "- Train Loss : 0.00010594893582391605 Score : 1.0\n",
      "- Val Loss : 7.622308476129547e-05 Score : 1.0\n",
      "[569/1000]\n",
      "- Train Loss : 0.00010534293849357507 Score : 1.0\n",
      "- Val Loss : 7.570299931103364e-05 Score : 1.0\n",
      "[570/1000]\n",
      "- Train Loss : 0.00010465050076567827 Score : 1.0\n",
      "- Val Loss : 7.520544750150293e-05 Score : 1.0\n",
      "[571/1000]\n",
      "- Train Loss : 0.00010409034964848413 Score : 1.0\n",
      "- Val Loss : 7.468689000234008e-05 Score : 1.0\n",
      "[572/1000]\n",
      "- Train Loss : 0.00010339156607288815 Score : 1.0\n",
      "- Val Loss : 7.423186616506428e-05 Score : 1.0\n",
      "[573/1000]\n",
      "- Train Loss : 0.00010272000953490433 Score : 1.0\n",
      "- Val Loss : 7.380524039035663e-05 Score : 1.0\n",
      "[574/1000]\n",
      "- Train Loss : 0.00010212302711604732 Score : 1.0\n",
      "- Val Loss : 7.335149712162092e-05 Score : 1.0\n",
      "[575/1000]\n",
      "- Train Loss : 0.0001015339857985964 Score : 1.0\n",
      "- Val Loss : 7.282581645995378e-05 Score : 1.0\n",
      "[576/1000]\n",
      "- Train Loss : 0.00010098437491655609 Score : 1.0\n",
      "- Val Loss : 7.231100607896224e-05 Score : 1.0\n",
      "[577/1000]\n",
      "- Train Loss : 0.00010030261324168855 Score : 1.0\n",
      "- Val Loss : 7.185300637502223e-05 Score : 1.0\n",
      "[578/1000]\n",
      "- Train Loss : 9.964758586041474e-05 Score : 1.0\n",
      "- Val Loss : 7.144378469092771e-05 Score : 1.0\n",
      "[579/1000]\n",
      "- Train Loss : 9.902214474600946e-05 Score : 1.0\n",
      "- Val Loss : 7.101374649209902e-05 Score : 1.0\n",
      "[580/1000]\n",
      "- Train Loss : 9.850381300364259e-05 Score : 1.0\n",
      "- Val Loss : 7.054095476632938e-05 Score : 1.0\n",
      "[581/1000]\n",
      "- Train Loss : 9.785015471505985e-05 Score : 1.0\n",
      "- Val Loss : 7.011606066953391e-05 Score : 1.0\n",
      "[582/1000]\n",
      "- Train Loss : 9.724579932582047e-05 Score : 1.0\n",
      "- Val Loss : 6.968605157453567e-05 Score : 1.0\n",
      "[583/1000]\n",
      "- Train Loss : 9.672249547697397e-05 Score : 1.0\n",
      "- Val Loss : 6.921641761437058e-05 Score : 1.0\n",
      "[584/1000]\n",
      "- Train Loss : 9.608272961890584e-05 Score : 1.0\n",
      "- Val Loss : 6.880888395244256e-05 Score : 1.0\n",
      "[585/1000]\n",
      "- Train Loss : 9.550689214342533e-05 Score : 1.0\n",
      "- Val Loss : 6.83809194015339e-05 Score : 1.0\n",
      "[586/1000]\n",
      "- Train Loss : 9.49753569633079e-05 Score : 1.0\n",
      "- Val Loss : 6.791880878154188e-05 Score : 1.0\n",
      "[587/1000]\n",
      "- Train Loss : 9.434978488166558e-05 Score : 1.0\n",
      "- Val Loss : 6.748778105247766e-05 Score : 1.0\n",
      "[588/1000]\n",
      "- Train Loss : 9.386152790587705e-05 Score : 1.0\n",
      "- Val Loss : 6.702743849018589e-05 Score : 1.0\n",
      "[589/1000]\n",
      "- Train Loss : 9.323895028501283e-05 Score : 1.0\n",
      "- Val Loss : 6.662739178864285e-05 Score : 1.0\n",
      "[590/1000]\n",
      "- Train Loss : 9.263784623827733e-05 Score : 1.0\n",
      "- Val Loss : 6.622778164455667e-05 Score : 1.0\n",
      "[591/1000]\n",
      "- Train Loss : 9.216781856796135e-05 Score : 1.0\n",
      "- Val Loss : 6.578434113180265e-05 Score : 1.0\n",
      "[592/1000]\n",
      "- Train Loss : 9.15563119229369e-05 Score : 1.0\n",
      "- Val Loss : 6.539179594255984e-05 Score : 1.0\n",
      "[593/1000]\n",
      "- Train Loss : 9.099137140664324e-05 Score : 1.0\n",
      "- Val Loss : 6.500097515527159e-05 Score : 1.0\n",
      "[594/1000]\n",
      "- Train Loss : 9.051507671150223e-05 Score : 1.0\n",
      "- Val Loss : 6.455305265262723e-05 Score : 1.0\n",
      "[595/1000]\n",
      "- Train Loss : 8.991652309002045e-05 Score : 1.0\n",
      "- Val Loss : 6.418406701413915e-05 Score : 1.0\n",
      "[596/1000]\n",
      "- Train Loss : 8.938175718261239e-05 Score : 1.0\n",
      "- Val Loss : 6.379131082212552e-05 Score : 1.0\n",
      "[597/1000]\n",
      "- Train Loss : 8.889661330613308e-05 Score : 1.0\n",
      "- Val Loss : 6.33636154816486e-05 Score : 1.0\n",
      "[598/1000]\n",
      "- Train Loss : 8.8310637390047e-05 Score : 1.0\n",
      "- Val Loss : 6.29734349786304e-05 Score : 1.0\n",
      "[599/1000]\n",
      "- Train Loss : 8.785827433004872e-05 Score : 1.0\n",
      "- Val Loss : 6.255167681956664e-05 Score : 1.0\n",
      "[600/1000]\n",
      "- Train Loss : 8.728026558755018e-05 Score : 1.0\n",
      "- Val Loss : 6.218527414603159e-05 Score : 1.0\n",
      "[601/1000]\n",
      "- Train Loss : 8.6730492613343e-05 Score : 1.0\n",
      "- Val Loss : 6.180272612255067e-05 Score : 1.0\n",
      "[602/1000]\n",
      "- Train Loss : 8.628786518885236e-05 Score : 1.0\n",
      "- Val Loss : 6.139197648735717e-05 Score : 1.0\n",
      "[603/1000]\n",
      "- Train Loss : 8.571951447164692e-05 Score : 1.0\n",
      "- Val Loss : 6.104046769905835e-05 Score : 1.0\n",
      "[604/1000]\n",
      "- Train Loss : 8.520578436825114e-05 Score : 1.0\n",
      "- Val Loss : 6.068276707082987e-05 Score : 1.0\n",
      "[605/1000]\n",
      "- Train Loss : 8.474754758935887e-05 Score : 1.0\n",
      "- Val Loss : 6.027513518347405e-05 Score : 1.0\n",
      "[606/1000]\n",
      "- Train Loss : 8.419493335370337e-05 Score : 1.0\n",
      "- Val Loss : 5.9925125242443755e-05 Score : 1.0\n",
      "[607/1000]\n",
      "- Train Loss : 8.371636087960926e-05 Score : 1.0\n",
      "- Val Loss : 5.9564779803622514e-05 Score : 1.0\n",
      "[608/1000]\n",
      "- Train Loss : 8.324506193781013e-05 Score : 1.0\n",
      "- Val Loss : 5.9180925745749846e-05 Score : 1.0\n",
      "[609/1000]\n",
      "- Train Loss : 8.27226837524601e-05 Score : 1.0\n",
      "- Val Loss : 5.8807221648748964e-05 Score : 1.0\n",
      "[610/1000]\n",
      "- Train Loss : 8.228654910959044e-05 Score : 1.0\n",
      "- Val Loss : 5.8416550018591806e-05 Score : 1.0\n",
      "[611/1000]\n",
      "- Train Loss : 8.174352600083996e-05 Score : 1.0\n",
      "- Val Loss : 5.80766900384333e-05 Score : 1.0\n",
      "[612/1000]\n",
      "- Train Loss : 8.125260167920108e-05 Score : 1.0\n",
      "- Val Loss : 5.772963777417317e-05 Score : 1.0\n",
      "[613/1000]\n",
      "- Train Loss : 8.082606816767818e-05 Score : 1.0\n",
      "- Val Loss : 5.735303057008423e-05 Score : 1.0\n",
      "[614/1000]\n",
      "- Train Loss : 8.029188261894483e-05 Score : 1.0\n",
      "- Val Loss : 5.7027351431315765e-05 Score : 1.0\n",
      "[615/1000]\n",
      "- Train Loss : 7.983800454288333e-05 Score : 1.0\n",
      "- Val Loss : 5.667899313266389e-05 Score : 1.0\n",
      "[616/1000]\n",
      "- Train Loss : 7.938716448229065e-05 Score : 1.0\n",
      "- Val Loss : 5.628545113722794e-05 Score : 1.0\n",
      "[617/1000]\n",
      "- Train Loss : 7.898482181190047e-05 Score : 1.0\n",
      "- Val Loss : 5.589508145931177e-05 Score : 1.0\n",
      "[618/1000]\n",
      "- Train Loss : 7.845965471157494e-05 Score : 1.0\n",
      "- Val Loss : 5.5562220950378105e-05 Score : 1.0\n",
      "[619/1000]\n",
      "- Train Loss : 7.795673829807331e-05 Score : 1.0\n",
      "- Val Loss : 5.52699675608892e-05 Score : 1.0\n",
      "[620/1000]\n",
      "- Train Loss : 7.749717527379592e-05 Score : 1.0\n",
      "- Val Loss : 5.4945350711932406e-05 Score : 1.0\n",
      "[621/1000]\n",
      "- Train Loss : 7.709401021808542e-05 Score : 1.0\n",
      "- Val Loss : 5.45945658814162e-05 Score : 1.0\n",
      "[622/1000]\n",
      "- Train Loss : 7.66035193211994e-05 Score : 1.0\n",
      "- Val Loss : 5.425798735814169e-05 Score : 1.0\n",
      "[623/1000]\n",
      "- Train Loss : 7.62114050303353e-05 Score : 1.0\n",
      "- Val Loss : 5.38970016350504e-05 Score : 1.0\n",
      "[624/1000]\n",
      "- Train Loss : 7.571330247123519e-05 Score : 1.0\n",
      "- Val Loss : 5.358133421395905e-05 Score : 1.0\n",
      "[625/1000]\n",
      "- Train Loss : 7.525899198097047e-05 Score : 1.0\n",
      "- Val Loss : 5.327158942236565e-05 Score : 1.0\n",
      "[626/1000]\n",
      "- Train Loss : 7.486882309927346e-05 Score : 1.0\n",
      "- Val Loss : 5.293903450365178e-05 Score : 1.0\n",
      "[627/1000]\n",
      "- Train Loss : 7.438245261760635e-05 Score : 1.0\n",
      "- Val Loss : 5.2601164497900754e-05 Score : 1.0\n",
      "[628/1000]\n",
      "- Train Loss : 7.401868960086076e-05 Score : 1.0\n",
      "- Val Loss : 5.2249208238208666e-05 Score : 1.0\n",
      "[629/1000]\n",
      "- Train Loss : 7.353343506919272e-05 Score : 1.0\n",
      "- Val Loss : 5.194980258238502e-05 Score : 1.0\n",
      "[630/1000]\n",
      "- Train Loss : 7.308385536614676e-05 Score : 1.0\n",
      "- Val Loss : 5.165669790585525e-05 Score : 1.0\n",
      "[631/1000]\n",
      "- Train Loss : 7.271908129445769e-05 Score : 1.0\n",
      "- Val Loss : 5.1325368985999376e-05 Score : 1.0\n",
      "[632/1000]\n",
      "- Train Loss : 7.224425027945674e-05 Score : 1.0\n",
      "- Val Loss : 5.103472722112201e-05 Score : 1.0\n",
      "[633/1000]\n",
      "- Train Loss : 7.183355066647184e-05 Score : 1.0\n",
      "- Val Loss : 5.073929423815571e-05 Score : 1.0\n",
      "[634/1000]\n",
      "- Train Loss : 7.144503767146186e-05 Score : 1.0\n",
      "- Val Loss : 5.040963878855109e-05 Score : 1.0\n",
      "[635/1000]\n",
      "- Train Loss : 7.10053251774904e-05 Score : 1.0\n",
      "- Val Loss : 5.009891901863739e-05 Score : 1.0\n",
      "[636/1000]\n",
      "- Train Loss : 7.063787739348804e-05 Score : 1.0\n",
      "- Val Loss : 4.9771162593970075e-05 Score : 1.0\n",
      "[637/1000]\n",
      "- Train Loss : 7.01726906603047e-05 Score : 1.0\n",
      "- Val Loss : 4.94844643981196e-05 Score : 1.0\n",
      "[638/1000]\n",
      "- Train Loss : 6.977426397093546e-05 Score : 1.0\n",
      "- Val Loss : 4.9204303650185466e-05 Score : 1.0\n",
      "[639/1000]\n",
      "- Train Loss : 6.940410306318275e-05 Score : 1.0\n",
      "- Val Loss : 4.8892143240664154e-05 Score : 1.0\n",
      "[640/1000]\n",
      "- Train Loss : 6.89697811443087e-05 Score : 1.0\n",
      "- Val Loss : 4.859440741711296e-05 Score : 1.0\n",
      "[641/1000]\n",
      "- Train Loss : 6.861889879574947e-05 Score : 1.0\n",
      "- Val Loss : 4.827612792723812e-05 Score : 1.0\n",
      "[642/1000]\n",
      "- Train Loss : 6.816954555688426e-05 Score : 1.0\n",
      "- Val Loss : 4.80059752590023e-05 Score : 1.0\n",
      "[643/1000]\n",
      "- Train Loss : 6.778054446638432e-05 Score : 1.0\n",
      "- Val Loss : 4.772130705532618e-05 Score : 1.0\n",
      "[644/1000]\n",
      "- Train Loss : 6.742302300861209e-05 Score : 1.0\n",
      "- Val Loss : 4.742268720292486e-05 Score : 1.0\n",
      "[645/1000]\n",
      "- Train Loss : 6.699958329489973e-05 Score : 1.0\n",
      "- Val Loss : 4.71250168629922e-05 Score : 1.0\n",
      "[646/1000]\n",
      "- Train Loss : 6.666359584212639e-05 Score : 1.0\n",
      "- Val Loss : 4.682873623096384e-05 Score : 1.0\n",
      "[647/1000]\n",
      "- Train Loss : 6.623431757664851e-05 Score : 1.0\n",
      "- Val Loss : 4.657051613321528e-05 Score : 1.0\n",
      "[648/1000]\n",
      "- Train Loss : 6.58489711390252e-05 Score : 1.0\n",
      "- Val Loss : 4.630222247214988e-05 Score : 1.0\n",
      "[649/1000]\n",
      "- Train Loss : 6.550612569198065e-05 Score : 1.0\n",
      "- Val Loss : 4.599962994689122e-05 Score : 1.0\n",
      "[650/1000]\n",
      "- Train Loss : 6.509301672647578e-05 Score : 1.0\n",
      "- Val Loss : 4.573144906316884e-05 Score : 1.0\n",
      "[651/1000]\n",
      "- Train Loss : 6.476876842498314e-05 Score : 1.0\n",
      "- Val Loss : 4.543127215583809e-05 Score : 1.0\n",
      "[652/1000]\n",
      "- Train Loss : 6.435176843903416e-05 Score : 1.0\n",
      "- Val Loss : 4.517602064879611e-05 Score : 1.0\n",
      "[653/1000]\n",
      "- Train Loss : 6.397816165796637e-05 Score : 1.0\n",
      "- Val Loss : 4.4915101170772687e-05 Score : 1.0\n",
      "[654/1000]\n",
      "- Train Loss : 6.364608361764112e-05 Score : 1.0\n",
      "- Val Loss : 4.4638029066845775e-05 Score : 1.0\n",
      "[655/1000]\n",
      "- Train Loss : 6.325335087037981e-05 Score : 1.0\n",
      "- Val Loss : 4.436055678525008e-05 Score : 1.0\n",
      "[656/1000]\n",
      "- Train Loss : 6.293497901626526e-05 Score : 1.0\n",
      "- Val Loss : 4.4073145545553416e-05 Score : 1.0\n",
      "[657/1000]\n",
      "- Train Loss : 6.252907506778784e-05 Score : 1.0\n",
      "- Val Loss : 4.383758641779423e-05 Score : 1.0\n",
      "[658/1000]\n",
      "- Train Loss : 6.21688281575593e-05 Score : 1.0\n",
      "- Val Loss : 4.358386286185123e-05 Score : 1.0\n",
      "[659/1000]\n",
      "- Train Loss : 6.184517981940492e-05 Score : 1.0\n",
      "- Val Loss : 4.3310596083756536e-05 Score : 1.0\n",
      "[660/1000]\n",
      "- Train Loss : 6.146511259430554e-05 Score : 1.0\n",
      "- Val Loss : 4.305344191379845e-05 Score : 1.0\n",
      "[661/1000]\n",
      "- Train Loss : 6.115425912867067e-05 Score : 1.0\n",
      "- Val Loss : 4.276996332919225e-05 Score : 1.0\n",
      "[662/1000]\n",
      "- Train Loss : 6.0761093057711456e-05 Score : 1.0\n",
      "- Val Loss : 4.25410908064805e-05 Score : 1.0\n",
      "[663/1000]\n",
      "- Train Loss : 6.0416461565182544e-05 Score : 1.0\n",
      "- Val Loss : 4.2298492189729586e-05 Score : 1.0\n",
      "[664/1000]\n",
      "- Train Loss : 6.0097550784929706e-05 Score : 1.0\n",
      "- Val Loss : 4.202672062092461e-05 Score : 1.0\n",
      "[665/1000]\n",
      "- Train Loss : 5.973184468328125e-05 Score : 1.0\n",
      "- Val Loss : 4.178533345111646e-05 Score : 1.0\n",
      "[666/1000]\n",
      "- Train Loss : 5.9428791650538594e-05 Score : 1.0\n",
      "- Val Loss : 4.1509774746373296e-05 Score : 1.0\n",
      "[667/1000]\n",
      "- Train Loss : 5.9044694858635514e-05 Score : 1.0\n",
      "- Val Loss : 4.127891224925406e-05 Score : 1.0\n",
      "[668/1000]\n",
      "- Train Loss : 5.871878521348764e-05 Score : 1.0\n",
      "- Val Loss : 4.105148764210753e-05 Score : 1.0\n",
      "[669/1000]\n",
      "- Train Loss : 5.84118288315949e-05 Score : 1.0\n",
      "- Val Loss : 4.0765989979263395e-05 Score : 1.0\n",
      "[670/1000]\n",
      "- Train Loss : 5.811836975529635e-05 Score : 1.0\n",
      "- Val Loss : 4.0498911403119564e-05 Score : 1.0\n",
      "[671/1000]\n",
      "- Train Loss : 5.773978571394562e-05 Score : 1.0\n",
      "- Val Loss : 4.026835449622013e-05 Score : 1.0\n",
      "[672/1000]\n",
      "- Train Loss : 5.737547447400478e-05 Score : 1.0\n",
      "- Val Loss : 4.006588278571144e-05 Score : 1.0\n",
      "[673/1000]\n",
      "- Train Loss : 5.706973843189189e-05 Score : 1.0\n",
      "- Val Loss : 3.984529394074343e-05 Score : 1.0\n",
      "[674/1000]\n",
      "- Train Loss : 5.677226322910024e-05 Score : 1.0\n",
      "- Val Loss : 3.9567014027852565e-05 Score : 1.0\n",
      "[675/1000]\n",
      "- Train Loss : 5.6483900152670685e-05 Score : 1.0\n",
      "- Val Loss : 3.930473030777648e-05 Score : 1.0\n",
      "[676/1000]\n",
      "- Train Loss : 5.6118179499107e-05 Score : 1.0\n",
      "- Val Loss : 3.908919097739272e-05 Score : 1.0\n",
      "[677/1000]\n",
      "- Train Loss : 5.576607610338316e-05 Score : 1.0\n",
      "- Val Loss : 3.886982085532509e-05 Score : 1.0\n",
      "[678/1000]\n",
      "- Train Loss : 5.5507152359900326e-05 Score : 1.0\n",
      "- Val Loss : 3.8627964386250824e-05 Score : 1.0\n",
      "[679/1000]\n",
      "- Train Loss : 5.5149938614502185e-05 Score : 1.0\n",
      "- Val Loss : 3.842297155642882e-05 Score : 1.0\n",
      "[680/1000]\n",
      "- Train Loss : 5.48466481025874e-05 Score : 1.0\n",
      "- Val Loss : 3.821577047347091e-05 Score : 1.0\n",
      "[681/1000]\n",
      "- Train Loss : 5.455981550363099e-05 Score : 1.0\n",
      "- Val Loss : 3.795349766733125e-05 Score : 1.0\n",
      "[682/1000]\n",
      "- Train Loss : 5.428860640677158e-05 Score : 1.0\n",
      "- Val Loss : 3.769193426705897e-05 Score : 1.0\n",
      "[683/1000]\n",
      "- Train Loss : 5.393893560621008e-05 Score : 1.0\n",
      "- Val Loss : 3.748836388695054e-05 Score : 1.0\n",
      "[684/1000]\n",
      "- Train Loss : 5.360013801691821e-05 Score : 1.0\n",
      "- Val Loss : 3.7298428651411086e-05 Score : 1.0\n",
      "[685/1000]\n",
      "- Train Loss : 5.3317419567772755e-05 Score : 1.0\n",
      "- Val Loss : 3.709152224473655e-05 Score : 1.0\n",
      "[686/1000]\n",
      "- Train Loss : 5.3045294609748656e-05 Score : 1.0\n",
      "- Val Loss : 3.6843739508185536e-05 Score : 1.0\n",
      "[687/1000]\n",
      "- Train Loss : 5.277961089531244e-05 Score : 1.0\n",
      "- Val Loss : 3.659041612991132e-05 Score : 1.0\n",
      "[688/1000]\n",
      "- Train Loss : 5.2432542563312585e-05 Score : 1.0\n",
      "- Val Loss : 3.639702845248394e-05 Score : 1.0\n",
      "[689/1000]\n",
      "- Train Loss : 5.211061781867304e-05 Score : 1.0\n",
      "- Val Loss : 3.6195684515405446e-05 Score : 1.0\n",
      "[690/1000]\n",
      "- Train Loss : 5.186638231356887e-05 Score : 1.0\n",
      "- Val Loss : 3.597382237785496e-05 Score : 1.0\n",
      "[691/1000]\n",
      "- Train Loss : 5.153642036424976e-05 Score : 1.0\n",
      "- Val Loss : 3.576906965463422e-05 Score : 1.0\n",
      "[692/1000]\n",
      "- Train Loss : 5.125982887774525e-05 Score : 1.0\n",
      "- Val Loss : 3.55838674295228e-05 Score : 1.0\n",
      "[693/1000]\n",
      "- Train Loss : 5.099007466924377e-05 Score : 1.0\n",
      "- Val Loss : 3.533875133143738e-05 Score : 1.0\n",
      "[694/1000]\n",
      "- Train Loss : 5.073807703107983e-05 Score : 1.0\n",
      "- Val Loss : 3.510251553962007e-05 Score : 1.0\n",
      "[695/1000]\n",
      "- Train Loss : 5.040752158594033e-05 Score : 1.0\n",
      "- Val Loss : 3.4907781810034066e-05 Score : 1.0\n",
      "[696/1000]\n",
      "- Train Loss : 5.0099145887038e-05 Score : 1.0\n",
      "- Val Loss : 3.471823220024817e-05 Score : 1.0\n",
      "[697/1000]\n",
      "- Train Loss : 4.9862519820938665e-05 Score : 1.0\n",
      "- Val Loss : 3.4496599255362526e-05 Score : 1.0\n",
      "[698/1000]\n",
      "- Train Loss : 4.954124122175724e-05 Score : 1.0\n",
      "- Val Loss : 3.4320197300985456e-05 Score : 1.0\n",
      "[699/1000]\n",
      "- Train Loss : 4.927677036903333e-05 Score : 1.0\n",
      "- Val Loss : 3.413380909478292e-05 Score : 1.0\n",
      "[700/1000]\n",
      "- Train Loss : 4.902294666761817e-05 Score : 1.0\n",
      "- Val Loss : 3.390662095625885e-05 Score : 1.0\n",
      "[701/1000]\n",
      "- Train Loss : 4.8783265204595715e-05 Score : 1.0\n",
      "- Val Loss : 3.3683889341773465e-05 Score : 1.0\n",
      "[702/1000]\n",
      "- Train Loss : 4.846791135706654e-05 Score : 1.0\n",
      "- Val Loss : 3.349630060256459e-05 Score : 1.0\n",
      "[703/1000]\n",
      "- Train Loss : 4.8165072864018534e-05 Score : 1.0\n",
      "- Val Loss : 3.331397965666838e-05 Score : 1.0\n",
      "[704/1000]\n",
      "- Train Loss : 4.794616102622563e-05 Score : 1.0\n",
      "- Val Loss : 3.310485044494271e-05 Score : 1.0\n",
      "[705/1000]\n",
      "- Train Loss : 4.7636267633303454e-05 Score : 1.0\n",
      "- Val Loss : 3.293516783742234e-05 Score : 1.0\n",
      "[706/1000]\n",
      "- Train Loss : 4.738080057601716e-05 Score : 1.0\n",
      "- Val Loss : 3.275579001638107e-05 Score : 1.0\n",
      "[707/1000]\n",
      "- Train Loss : 4.7141154936980456e-05 Score : 1.0\n",
      "- Val Loss : 3.252908936701715e-05 Score : 1.0\n",
      "[708/1000]\n",
      "- Train Loss : 4.690792709677933e-05 Score : 1.0\n",
      "- Val Loss : 3.230659422115423e-05 Score : 1.0\n",
      "[709/1000]\n",
      "- Train Loss : 4.660582842714373e-05 Score : 1.0\n",
      "- Val Loss : 3.2147083402378485e-05 Score : 1.0\n",
      "[710/1000]\n",
      "- Train Loss : 4.6313987998776916e-05 Score : 1.0\n",
      "- Val Loss : 3.196308534825221e-05 Score : 1.0\n",
      "[711/1000]\n",
      "- Train Loss : 4.610434305302786e-05 Score : 1.0\n",
      "- Val Loss : 3.176611426169984e-05 Score : 1.0\n",
      "[712/1000]\n",
      "- Train Loss : 4.581071415385749e-05 Score : 1.0\n",
      "- Val Loss : 3.16028599627316e-05 Score : 1.0\n",
      "[713/1000]\n",
      "- Train Loss : 4.556714687472171e-05 Score : 1.0\n",
      "- Val Loss : 3.142591594951227e-05 Score : 1.0\n",
      "[714/1000]\n",
      "- Train Loss : 4.5333240147253186e-05 Score : 1.0\n",
      "- Val Loss : 3.122479029116221e-05 Score : 1.0\n",
      "[715/1000]\n",
      "- Train Loss : 4.511386663984417e-05 Score : 1.0\n",
      "- Val Loss : 3.1002247851574793e-05 Score : 1.0\n",
      "[716/1000]\n",
      "- Train Loss : 4.482068551523197e-05 Score : 1.0\n",
      "- Val Loss : 3.0840743420412764e-05 Score : 1.0\n",
      "[717/1000]\n",
      "- Train Loss : 4.454546807311191e-05 Score : 1.0\n",
      "- Val Loss : 3.068034493480809e-05 Score : 1.0\n",
      "[718/1000]\n",
      "- Train Loss : 4.434139752144498e-05 Score : 1.0\n",
      "- Val Loss : 3.049097904295195e-05 Score : 1.0\n",
      "[719/1000]\n",
      "- Train Loss : 4.4059624239404606e-05 Score : 1.0\n",
      "- Val Loss : 3.033391294593457e-05 Score : 1.0\n",
      "[720/1000]\n",
      "- Train Loss : 4.382305698729599e-05 Score : 1.0\n",
      "- Val Loss : 3.0171939215506427e-05 Score : 1.0\n",
      "[721/1000]\n",
      "- Train Loss : 4.360118737167795e-05 Score : 1.0\n",
      "- Val Loss : 2.9962064218125306e-05 Score : 1.0\n",
      "[722/1000]\n",
      "- Train Loss : 4.3385988546005036e-05 Score : 1.0\n",
      "- Val Loss : 2.9768896638415754e-05 Score : 1.0\n",
      "[723/1000]\n",
      "- Train Loss : 4.310589287746957e-05 Score : 1.0\n",
      "- Val Loss : 2.960513302241452e-05 Score : 1.0\n",
      "[724/1000]\n",
      "- Train Loss : 4.285028494955946e-05 Score : 1.0\n",
      "- Val Loss : 2.944261723314412e-05 Score : 1.0\n",
      "[725/1000]\n",
      "- Train Loss : 4.264886395806874e-05 Score : 1.0\n",
      "- Val Loss : 2.9264841941767372e-05 Score : 1.0\n",
      "[726/1000]\n",
      "- Train Loss : 4.2379046640740446e-05 Score : 1.0\n",
      "- Val Loss : 2.9105873181833886e-05 Score : 1.0\n",
      "[727/1000]\n",
      "- Train Loss : 4.218609410600038e-05 Score : 1.0\n",
      "- Val Loss : 2.8919661417603493e-05 Score : 1.0\n",
      "[728/1000]\n",
      "- Train Loss : 4.191607048495724e-05 Score : 1.0\n",
      "- Val Loss : 2.8767648473149166e-05 Score : 1.0\n",
      "[729/1000]\n",
      "- Train Loss : 4.168402129936213e-05 Score : 1.0\n",
      "- Val Loss : 2.8616788767976686e-05 Score : 1.0\n",
      "[730/1000]\n",
      "- Train Loss : 4.147896601757061e-05 Score : 1.0\n",
      "- Val Loss : 2.8420870876288973e-05 Score : 1.0\n",
      "[731/1000]\n",
      "- Train Loss : 4.1283107040928575e-05 Score : 1.0\n",
      "- Val Loss : 2.822898022714071e-05 Score : 1.0\n",
      "[732/1000]\n",
      "- Train Loss : 4.1011532680891425e-05 Score : 1.0\n",
      "- Val Loss : 2.8085525627830066e-05 Score : 1.0\n",
      "[733/1000]\n",
      "- Train Loss : 4.0759885400297615e-05 Score : 1.0\n",
      "- Val Loss : 2.7943397071794607e-05 Score : 1.0\n",
      "[734/1000]\n",
      "- Train Loss : 4.0578636900237245e-05 Score : 1.0\n",
      "- Val Loss : 2.7770107408287004e-05 Score : 1.0\n",
      "[735/1000]\n",
      "- Train Loss : 4.0319726192845134e-05 Score : 1.0\n",
      "- Val Loss : 2.7621788831311278e-05 Score : 1.0\n",
      "[736/1000]\n",
      "- Train Loss : 4.0110502090505586e-05 Score : 1.0\n",
      "- Val Loss : 2.747242797340732e-05 Score : 1.0\n",
      "[737/1000]\n",
      "- Train Loss : 3.99160424725172e-05 Score : 1.0\n",
      "- Val Loss : 2.7291633159620687e-05 Score : 1.0\n",
      "[738/1000]\n",
      "- Train Loss : 3.971233379868661e-05 Score : 1.0\n",
      "- Val Loss : 2.711438173719216e-05 Score : 1.0\n",
      "[739/1000]\n",
      "- Train Loss : 3.9458140135037764e-05 Score : 1.0\n",
      "- Val Loss : 2.696676892810501e-05 Score : 1.0\n",
      "[740/1000]\n",
      "- Train Loss : 3.921873182359074e-05 Score : 1.0\n",
      "- Val Loss : 2.6824811357073486e-05 Score : 1.0\n",
      "[741/1000]\n",
      "- Train Loss : 3.9043286455530004e-05 Score : 1.0\n",
      "- Val Loss : 2.6652758606360294e-05 Score : 1.0\n",
      "[742/1000]\n",
      "- Train Loss : 3.8798234274711125e-05 Score : 1.0\n",
      "- Val Loss : 2.651529575814493e-05 Score : 1.0\n",
      "[743/1000]\n",
      "- Train Loss : 3.862567235854739e-05 Score : 1.0\n",
      "- Val Loss : 2.633941221574787e-05 Score : 1.0\n",
      "[744/1000]\n",
      "- Train Loss : 3.837541761539695e-05 Score : 1.0\n",
      "- Val Loss : 2.6203124434687197e-05 Score : 1.0\n",
      "[745/1000]\n",
      "- Train Loss : 3.816903239971402e-05 Score : 1.0\n",
      "- Val Loss : 2.6064220946864225e-05 Score : 1.0\n",
      "[746/1000]\n",
      "- Train Loss : 3.797925299093347e-05 Score : 1.0\n",
      "- Val Loss : 2.589694304333534e-05 Score : 1.0\n",
      "[747/1000]\n",
      "- Train Loss : 3.78002088028653e-05 Score : 1.0\n",
      "- Val Loss : 2.572490120655857e-05 Score : 1.0\n",
      "[748/1000]\n",
      "- Train Loss : 3.7561399292624425e-05 Score : 1.0\n",
      "- Val Loss : 2.55847680818988e-05 Score : 1.0\n",
      "[749/1000]\n",
      "- Train Loss : 3.732585901161656e-05 Score : 1.0\n",
      "- Val Loss : 2.5454650312894955e-05 Score : 1.0\n",
      "[750/1000]\n",
      "- Train Loss : 3.71598237810152e-05 Score : 1.0\n",
      "- Val Loss : 2.530401798139792e-05 Score : 1.0\n",
      "[751/1000]\n",
      "- Train Loss : 3.692454396918442e-05 Score : 1.0\n",
      "- Val Loss : 2.5161685698549263e-05 Score : 1.0\n",
      "[752/1000]\n",
      "- Train Loss : 3.676373464461196e-05 Score : 1.0\n",
      "- Val Loss : 2.4998693334055133e-05 Score : 1.0\n",
      "[753/1000]\n",
      "- Train Loss : 3.652918429805949e-05 Score : 1.0\n",
      "- Val Loss : 2.487358869984746e-05 Score : 1.0\n",
      "[754/1000]\n",
      "- Train Loss : 3.6329919213231835e-05 Score : 1.0\n",
      "- Val Loss : 2.4741822926444e-05 Score : 1.0\n",
      "[755/1000]\n",
      "- Train Loss : 3.615367217611897e-05 Score : 1.0\n",
      "- Val Loss : 2.4583690901636146e-05 Score : 1.0\n",
      "[756/1000]\n",
      "- Train Loss : 3.598379180402844e-05 Score : 1.0\n",
      "- Val Loss : 2.4416311134700663e-05 Score : 1.0\n",
      "[757/1000]\n",
      "- Train Loss : 3.575168875613599e-05 Score : 1.0\n",
      "- Val Loss : 2.4295773982885294e-05 Score : 1.0\n",
      "[758/1000]\n",
      "- Train Loss : 3.552972601836599e-05 Score : 1.0\n",
      "- Val Loss : 2.4159797249012627e-05 Score : 1.0\n",
      "[759/1000]\n",
      "- Train Loss : 3.5374221473224075e-05 Score : 1.0\n",
      "- Val Loss : 2.4017137548071332e-05 Score : 1.0\n",
      "[760/1000]\n",
      "- Train Loss : 3.515253183650202e-05 Score : 1.0\n",
      "- Val Loss : 2.3890605007181875e-05 Score : 1.0\n",
      "[761/1000]\n",
      "- Train Loss : 3.49986738304627e-05 Score : 1.0\n",
      "- Val Loss : 2.3731408873572946e-05 Score : 1.0\n",
      "[762/1000]\n",
      "- Train Loss : 3.4775789673504834e-05 Score : 1.0\n",
      "- Val Loss : 2.3612688892171718e-05 Score : 1.0\n",
      "[763/1000]\n",
      "- Train Loss : 3.458697594598764e-05 Score : 1.0\n",
      "- Val Loss : 2.3491826141253114e-05 Score : 1.0\n",
      "[764/1000]\n",
      "- Train Loss : 3.44195710163654e-05 Score : 1.0\n",
      "- Val Loss : 2.3333695935434662e-05 Score : 1.0\n",
      "[765/1000]\n",
      "- Train Loss : 3.425801060075173e-05 Score : 1.0\n",
      "- Val Loss : 2.318748738616705e-05 Score : 1.0\n",
      "[766/1000]\n",
      "- Train Loss : 3.403860263359901e-05 Score : 1.0\n",
      "- Val Loss : 2.3056343707139604e-05 Score : 1.0\n",
      "[767/1000]\n",
      "- Train Loss : 3.383504220740482e-05 Score : 1.0\n",
      "- Val Loss : 2.2943897420191206e-05 Score : 1.0\n",
      "[768/1000]\n",
      "- Train Loss : 3.3680263135870016e-05 Score : 1.0\n",
      "- Val Loss : 2.281302113260608e-05 Score : 1.0\n",
      "[769/1000]\n",
      "- Train Loss : 3.347158801463795e-05 Score : 1.0\n",
      "- Val Loss : 2.2680618712911382e-05 Score : 1.0\n",
      "[770/1000]\n",
      "- Train Loss : 3.332284318781199e-05 Score : 1.0\n",
      "- Val Loss : 2.252900776511524e-05 Score : 1.0\n",
      "[771/1000]\n",
      "- Train Loss : 3.310506862261617e-05 Score : 1.0\n",
      "- Val Loss : 2.2424848793889396e-05 Score : 1.0\n",
      "[772/1000]\n",
      "- Train Loss : 3.293220434999158e-05 Score : 1.0\n",
      "- Val Loss : 2.2305946913547814e-05 Score : 1.0\n",
      "[773/1000]\n",
      "- Train Loss : 3.27718272375124e-05 Score : 1.0\n",
      "- Val Loss : 2.2155969418236054e-05 Score : 1.0\n",
      "[774/1000]\n",
      "- Train Loss : 3.2619408557366114e-05 Score : 1.0\n",
      "- Val Loss : 2.200922062911559e-05 Score : 1.0\n",
      "[775/1000]\n",
      "- Train Loss : 3.240550692377534e-05 Score : 1.0\n",
      "- Val Loss : 2.1901219952269457e-05 Score : 1.0\n",
      "[776/1000]\n",
      "- Train Loss : 3.2217985941921747e-05 Score : 1.0\n",
      "- Val Loss : 2.179067269025836e-05 Score : 1.0\n",
      "[777/1000]\n",
      "- Train Loss : 3.207007633439692e-05 Score : 1.0\n",
      "- Val Loss : 2.1654235752066597e-05 Score : 1.0\n",
      "[778/1000]\n",
      "- Train Loss : 3.187935079848911e-05 Score : 1.0\n",
      "- Val Loss : 2.1532696337089874e-05 Score : 1.0\n",
      "[779/1000]\n",
      "- Train Loss : 3.1728712454221546e-05 Score : 1.0\n",
      "- Val Loss : 2.140544711437542e-05 Score : 1.0\n",
      "[780/1000]\n",
      "- Train Loss : 3.152611871984037e-05 Score : 1.0\n",
      "- Val Loss : 2.1283081878209487e-05 Score : 1.0\n",
      "[781/1000]\n",
      "- Train Loss : 3.139069273553711e-05 Score : 1.0\n",
      "- Val Loss : 2.1147785446373746e-05 Score : 1.0\n",
      "[782/1000]\n",
      "- Train Loss : 3.1188936191028915e-05 Score : 1.0\n",
      "- Val Loss : 2.1045823814347386e-05 Score : 1.0\n",
      "[783/1000]\n",
      "- Train Loss : 3.1027273154096394e-05 Score : 1.0\n",
      "- Val Loss : 2.0942385162925348e-05 Score : 1.0\n",
      "[784/1000]\n",
      "- Train Loss : 3.0880157232281315e-05 Score : 1.0\n",
      "- Val Loss : 2.0793062503798865e-05 Score : 1.0\n",
      "[785/1000]\n",
      "- Train Loss : 3.072234418747636e-05 Score : 1.0\n",
      "- Val Loss : 2.066379238385707e-05 Score : 1.0\n",
      "[786/1000]\n",
      "- Train Loss : 3.052835760778786e-05 Score : 1.0\n",
      "- Val Loss : 2.0553972717607394e-05 Score : 1.0\n",
      "[787/1000]\n",
      "- Train Loss : 3.0354251369620517e-05 Score : 1.0\n",
      "- Val Loss : 2.0450277588679455e-05 Score : 1.0\n",
      "[788/1000]\n",
      "- Train Loss : 3.021015330280837e-05 Score : 1.0\n",
      "- Val Loss : 2.0321813281043433e-05 Score : 1.0\n",
      "[789/1000]\n",
      "- Train Loss : 3.003161721911359e-05 Score : 1.0\n",
      "- Val Loss : 2.0220288206473924e-05 Score : 1.0\n",
      "[790/1000]\n",
      "- Train Loss : 2.9890492239954052e-05 Score : 1.0\n",
      "- Val Loss : 2.01010570890503e-05 Score : 1.0\n",
      "[791/1000]\n",
      "- Train Loss : 2.9704153348575346e-05 Score : 1.0\n",
      "- Val Loss : 1.998168954742141e-05 Score : 1.0\n",
      "[792/1000]\n",
      "- Train Loss : 2.9571658867805833e-05 Score : 1.0\n",
      "- Val Loss : 1.9858662199112587e-05 Score : 1.0\n",
      "[793/1000]\n",
      "- Train Loss : 2.9380757041508332e-05 Score : 1.0\n",
      "- Val Loss : 1.9758683265536092e-05 Score : 1.0\n",
      "[794/1000]\n",
      "- Train Loss : 2.9234551372307276e-05 Score : 1.0\n",
      "- Val Loss : 1.965311821550131e-05 Score : 1.0\n",
      "[795/1000]\n",
      "- Train Loss : 2.9096035935557058e-05 Score : 1.0\n",
      "- Val Loss : 1.9529337805579416e-05 Score : 1.0\n",
      "[796/1000]\n",
      "- Train Loss : 2.8951111946500736e-05 Score : 1.0\n",
      "- Val Loss : 1.940843867487274e-05 Score : 1.0\n",
      "[797/1000]\n",
      "- Train Loss : 2.87631003933105e-05 Score : 1.0\n",
      "- Val Loss : 1.930916914716363e-05 Score : 1.0\n",
      "[798/1000]\n",
      "- Train Loss : 2.8603180958776244e-05 Score : 1.0\n",
      "- Val Loss : 1.9199054804630578e-05 Score : 1.0\n",
      "[799/1000]\n",
      "- Train Loss : 2.8468884718475034e-05 Score : 1.0\n",
      "- Val Loss : 1.9090828573098406e-05 Score : 1.0\n",
      "[800/1000]\n",
      "- Train Loss : 2.830324345066199e-05 Score : 1.0\n",
      "- Val Loss : 1.8983351765200496e-05 Score : 1.0\n",
      "[801/1000]\n",
      "- Train Loss : 2.8167149417236538e-05 Score : 1.0\n",
      "- Val Loss : 1.887175312731415e-05 Score : 1.0\n",
      "[802/1000]\n",
      "- Train Loss : 2.7993847854506588e-05 Score : 1.0\n",
      "- Val Loss : 1.876343958429061e-05 Score : 1.0\n",
      "[803/1000]\n",
      "- Train Loss : 2.7864973566869998e-05 Score : 1.0\n",
      "- Val Loss : 1.865644117060583e-05 Score : 1.0\n",
      "[804/1000]\n",
      "- Train Loss : 2.7690812784284288e-05 Score : 1.0\n",
      "- Val Loss : 1.8544496924732812e-05 Score : 1.0\n",
      "[805/1000]\n",
      "- Train Loss : 2.7568794596138308e-05 Score : 1.0\n",
      "- Val Loss : 1.8434164303471334e-05 Score : 1.0\n",
      "[806/1000]\n",
      "- Train Loss : 2.739454240933507e-05 Score : 1.0\n",
      "- Val Loss : 1.8340759197599255e-05 Score : 1.0\n",
      "[807/1000]\n",
      "- Train Loss : 2.725496804285083e-05 Score : 1.0\n",
      "- Val Loss : 1.8254868336953223e-05 Score : 1.0\n",
      "[808/1000]\n",
      "- Train Loss : 2.712168338803167e-05 Score : 1.0\n",
      "- Val Loss : 1.8140251995646395e-05 Score : 1.0\n",
      "[809/1000]\n",
      "- Train Loss : 2.69905199527582e-05 Score : 1.0\n",
      "- Val Loss : 1.8011300198850222e-05 Score : 1.0\n",
      "[810/1000]\n",
      "- Train Loss : 2.6818398838359928e-05 Score : 1.0\n",
      "- Val Loss : 1.7918708181241527e-05 Score : 1.0\n",
      "[811/1000]\n",
      "- Train Loss : 2.6670481904350003e-05 Score : 1.0\n",
      "- Val Loss : 1.7828117051976733e-05 Score : 1.0\n",
      "[812/1000]\n",
      "- Train Loss : 2.653906815592361e-05 Score : 1.0\n",
      "- Val Loss : 1.770637209119741e-05 Score : 1.0\n",
      "[813/1000]\n",
      "- Train Loss : 2.64243919900764e-05 Score : 1.0\n",
      "- Val Loss : 1.759623955877032e-05 Score : 1.0\n",
      "[814/1000]\n",
      "- Train Loss : 2.6250416744409853e-05 Score : 1.0\n",
      "- Val Loss : 1.750916453602258e-05 Score : 1.0\n",
      "[815/1000]\n",
      "- Train Loss : 2.6094035623221618e-05 Score : 1.0\n",
      "- Val Loss : 1.7415643014828674e-05 Score : 1.0\n",
      "[816/1000]\n",
      "- Train Loss : 2.5983123830681303e-05 Score : 1.0\n",
      "- Val Loss : 1.7308568203588948e-05 Score : 1.0\n",
      "[817/1000]\n",
      "- Train Loss : 2.5823642924175754e-05 Score : 1.0\n",
      "- Val Loss : 1.7226784621016122e-05 Score : 1.0\n",
      "[818/1000]\n",
      "- Train Loss : 2.5708245857458678e-05 Score : 1.0\n",
      "- Val Loss : 1.7120684788096696e-05 Score : 1.0\n",
      "[819/1000]\n",
      "- Train Loss : 2.5547802781349877e-05 Score : 1.0\n",
      "- Val Loss : 1.7029435184667818e-05 Score : 1.0\n",
      "[820/1000]\n",
      "- Train Loss : 2.5428888395456146e-05 Score : 1.0\n",
      "- Val Loss : 1.6919218978728168e-05 Score : 1.0\n",
      "[821/1000]\n",
      "- Train Loss : 2.52720992648392e-05 Score : 1.0\n",
      "- Val Loss : 1.6836518625495955e-05 Score : 1.0\n",
      "[822/1000]\n",
      "- Train Loss : 2.514290253606709e-05 Score : 1.0\n",
      "- Val Loss : 1.6757318007876165e-05 Score : 1.0\n",
      "[823/1000]\n",
      "- Train Loss : 2.502523524425568e-05 Score : 1.0\n",
      "- Val Loss : 1.66434001585003e-05 Score : 1.0\n",
      "[824/1000]\n",
      "- Train Loss : 2.490056085156943e-05 Score : 1.0\n",
      "- Val Loss : 1.6540492651984096e-05 Score : 1.0\n",
      "[825/1000]\n",
      "- Train Loss : 2.4741045889540044e-05 Score : 1.0\n",
      "- Val Loss : 1.6454378055641428e-05 Score : 1.0\n",
      "[826/1000]\n",
      "- Train Loss : 2.4606813541241638e-05 Score : 1.0\n",
      "- Val Loss : 1.6378869986510836e-05 Score : 1.0\n",
      "[827/1000]\n",
      "- Train Loss : 2.448817488382499e-05 Score : 1.0\n",
      "- Val Loss : 1.6261876226053573e-05 Score : 1.0\n",
      "[828/1000]\n",
      "- Train Loss : 2.437707525536502e-05 Score : 1.0\n",
      "- Val Loss : 1.6160516679519787e-05 Score : 1.0\n",
      "[829/1000]\n",
      "- Train Loss : 2.4223312392576998e-05 Score : 1.0\n",
      "- Val Loss : 1.607976810191758e-05 Score : 1.0\n",
      "[830/1000]\n",
      "- Train Loss : 2.4078458358821485e-05 Score : 1.0\n",
      "- Val Loss : 1.5996913134586066e-05 Score : 1.0\n",
      "[831/1000]\n",
      "- Train Loss : 2.3970175738617803e-05 Score : 1.0\n",
      "- Val Loss : 1.5910361980786547e-05 Score : 1.0\n",
      "[832/1000]\n",
      "- Train Loss : 2.3830449713892045e-05 Score : 1.0\n",
      "- Val Loss : 1.582275035616476e-05 Score : 1.0\n",
      "[833/1000]\n",
      "- Train Loss : 2.372091140993386e-05 Score : 1.0\n",
      "- Val Loss : 1.5724566765129566e-05 Score : 1.0\n",
      "[834/1000]\n",
      "- Train Loss : 2.357246055200651e-05 Score : 1.0\n",
      "- Val Loss : 1.563604746479541e-05 Score : 1.0\n",
      "[835/1000]\n",
      "- Train Loss : 2.3468036336756388e-05 Score : 1.0\n",
      "- Val Loss : 1.5542240362265147e-05 Score : 1.0\n",
      "[836/1000]\n",
      "- Train Loss : 2.33257259575718e-05 Score : 1.0\n",
      "- Val Loss : 1.5462645023944788e-05 Score : 1.0\n",
      "[837/1000]\n",
      "- Train Loss : 2.3223207866370405e-05 Score : 1.0\n",
      "- Val Loss : 1.5361010810011066e-05 Score : 1.0\n",
      "[838/1000]\n",
      "- Train Loss : 2.3070444108776024e-05 Score : 1.0\n",
      "- Val Loss : 1.5282024833140895e-05 Score : 1.0\n",
      "[839/1000]\n",
      "- Train Loss : 2.2981325426169657e-05 Score : 1.0\n",
      "- Val Loss : 1.5189589248620905e-05 Score : 1.0\n",
      "[840/1000]\n",
      "- Train Loss : 2.2828140597792095e-05 Score : 1.0\n",
      "- Val Loss : 1.5118020201043691e-05 Score : 1.0\n",
      "[841/1000]\n",
      "- Train Loss : 2.2713766737878257e-05 Score : 1.0\n",
      "- Val Loss : 1.5041780898172874e-05 Score : 1.0\n",
      "[842/1000]\n",
      "- Train Loss : 2.2605146063546676e-05 Score : 1.0\n",
      "- Val Loss : 1.4938035747036338e-05 Score : 1.0\n",
      "[843/1000]\n",
      "- Train Loss : 2.2497471238441198e-05 Score : 1.0\n",
      "- Val Loss : 1.4849307262920775e-05 Score : 1.0\n",
      "[844/1000]\n",
      "- Train Loss : 2.2346210749270136e-05 Score : 1.0\n",
      "- Val Loss : 1.4770136658626143e-05 Score : 1.0\n",
      "[845/1000]\n",
      "- Train Loss : 2.2227515248434633e-05 Score : 1.0\n",
      "- Val Loss : 1.4705966350447852e-05 Score : 1.0\n",
      "[846/1000]\n",
      "- Train Loss : 2.2128980440862102e-05 Score : 1.0\n",
      "- Val Loss : 1.4607374396291561e-05 Score : 1.0\n",
      "[847/1000]\n",
      "- Train Loss : 2.2022562411115763e-05 Score : 1.0\n",
      "- Val Loss : 1.4511390872939955e-05 Score : 1.0\n",
      "[848/1000]\n",
      "- Train Loss : 2.1881373489425943e-05 Score : 1.0\n",
      "- Val Loss : 1.4433083379117306e-05 Score : 1.0\n",
      "[849/1000]\n",
      "- Train Loss : 2.175770540209972e-05 Score : 1.0\n",
      "- Val Loss : 1.4369846212503035e-05 Score : 1.0\n",
      "[850/1000]\n",
      "- Train Loss : 2.1657281826669027e-05 Score : 1.0\n",
      "- Val Loss : 1.427468396286713e-05 Score : 1.0\n",
      "[851/1000]\n",
      "- Train Loss : 2.1529421019295114e-05 Score : 1.0\n",
      "- Val Loss : 1.4207304047886282e-05 Score : 1.0\n",
      "[852/1000]\n",
      "- Train Loss : 2.143173580609275e-05 Score : 1.0\n",
      "- Val Loss : 1.4121949789114296e-05 Score : 1.0\n",
      "[853/1000]\n",
      "- Train Loss : 2.1302612190589167e-05 Score : 1.0\n",
      "- Val Loss : 1.4036665561434347e-05 Score : 1.0\n",
      "[854/1000]\n",
      "- Train Loss : 2.1202077682068597e-05 Score : 1.0\n",
      "- Val Loss : 1.3959857824374922e-05 Score : 1.0\n",
      "[855/1000]\n",
      "- Train Loss : 2.107566026805014e-05 Score : 1.0\n",
      "- Val Loss : 1.3883400242775679e-05 Score : 1.0\n",
      "[856/1000]\n",
      "- Train Loss : 2.098223394063502e-05 Score : 1.0\n",
      "- Val Loss : 1.3794394362776075e-05 Score : 1.0\n",
      "[857/1000]\n",
      "- Train Loss : 2.085385196753325e-05 Score : 1.0\n",
      "- Val Loss : 1.3735504580836277e-05 Score : 1.0\n",
      "[858/1000]\n",
      "- Train Loss : 2.076046853795156e-05 Score : 1.0\n",
      "- Val Loss : 1.3642887097375933e-05 Score : 1.0\n",
      "[859/1000]\n",
      "- Train Loss : 2.062683600140897e-05 Score : 1.0\n",
      "- Val Loss : 1.357169276161585e-05 Score : 1.0\n",
      "[860/1000]\n",
      "- Train Loss : 2.0543363436647472e-05 Score : 1.0\n",
      "- Val Loss : 1.3483897419064306e-05 Score : 1.0\n",
      "[861/1000]\n",
      "- Train Loss : 2.0410498311523246e-05 Score : 1.0\n",
      "- Val Loss : 1.3422971278487239e-05 Score : 1.0\n",
      "[862/1000]\n",
      "- Train Loss : 2.031031370582544e-05 Score : 1.0\n",
      "- Val Loss : 1.33620078486274e-05 Score : 1.0\n",
      "[863/1000]\n",
      "- Train Loss : 2.0212805768298698e-05 Score : 1.0\n",
      "- Val Loss : 1.3276537174533587e-05 Score : 1.0\n",
      "[864/1000]\n",
      "- Train Loss : 2.0118237797659174e-05 Score : 1.0\n",
      "- Val Loss : 1.3180341738916468e-05 Score : 1.0\n",
      "[865/1000]\n",
      "- Train Loss : 1.9984720261668877e-05 Score : 1.0\n",
      "- Val Loss : 1.312904350925237e-05 Score : 1.0\n",
      "[866/1000]\n",
      "- Train Loss : 1.9882417745975545e-05 Score : 1.0\n",
      "- Val Loss : 1.3067358850094024e-05 Score : 1.0\n",
      "[867/1000]\n",
      "- Train Loss : 1.979095749346824e-05 Score : 1.0\n",
      "- Val Loss : 1.2969861018063966e-05 Score : 1.0\n",
      "[868/1000]\n",
      "- Train Loss : 1.969745348004734e-05 Score : 1.0\n",
      "- Val Loss : 1.289178089791676e-05 Score : 1.0\n",
      "[869/1000]\n",
      "- Train Loss : 1.9569143634928172e-05 Score : 1.0\n",
      "- Val Loss : 1.2824157238355838e-05 Score : 1.0\n",
      "[870/1000]\n",
      "- Train Loss : 1.9458382565870932e-05 Score : 1.0\n",
      "- Val Loss : 1.2758995580952615e-05 Score : 1.0\n",
      "[871/1000]\n",
      "- Train Loss : 1.9368651161736732e-05 Score : 1.0\n",
      "- Val Loss : 1.2684037756116595e-05 Score : 1.0\n",
      "[872/1000]\n",
      "- Train Loss : 1.928139618535675e-05 Score : 1.0\n",
      "- Val Loss : 1.2594375220942311e-05 Score : 1.0\n",
      "[873/1000]\n",
      "- Train Loss : 1.9161698042403765e-05 Score : 1.0\n",
      "- Val Loss : 1.2540194802568294e-05 Score : 1.0\n",
      "[874/1000]\n",
      "- Train Loss : 1.90463791669673e-05 Score : 1.0\n",
      "- Val Loss : 1.2475692528823856e-05 Score : 1.0\n",
      "[875/1000]\n",
      "- Train Loss : 1.8965634404190092e-05 Score : 1.0\n",
      "- Val Loss : 1.2402369065966923e-05 Score : 1.0\n",
      "[876/1000]\n",
      "- Train Loss : 1.885166496625364e-05 Score : 1.0\n",
      "- Val Loss : 1.2334472557995468e-05 Score : 1.0\n",
      "[877/1000]\n",
      "- Train Loss : 1.876387836495673e-05 Score : 1.0\n",
      "- Val Loss : 1.2262666132301092e-05 Score : 1.0\n",
      "[878/1000]\n",
      "- Train Loss : 1.8657472209168998e-05 Score : 1.0\n",
      "- Val Loss : 1.220241392729804e-05 Score : 1.0\n",
      "[879/1000]\n",
      "- Train Loss : 1.856685202660123e-05 Score : 1.0\n",
      "- Val Loss : 1.213064206240233e-05 Score : 1.0\n",
      "[880/1000]\n",
      "- Train Loss : 1.845851023871445e-05 Score : 1.0\n",
      "- Val Loss : 1.2062097084708512e-05 Score : 1.0\n",
      "[881/1000]\n",
      "- Train Loss : 1.83709003320271e-05 Score : 1.0\n",
      "- Val Loss : 1.1986516256001778e-05 Score : 1.0\n",
      "[882/1000]\n",
      "- Train Loss : 1.8264372455531782e-05 Score : 1.0\n",
      "- Val Loss : 1.1922621524718124e-05 Score : 1.0\n",
      "[883/1000]\n",
      "- Train Loss : 1.8182592409882796e-05 Score : 1.0\n",
      "- Val Loss : 1.1856046512548346e-05 Score : 1.0\n",
      "[884/1000]\n",
      "- Train Loss : 1.8073997479304024e-05 Score : 1.0\n",
      "- Val Loss : 1.1796832950494718e-05 Score : 1.0\n",
      "[885/1000]\n",
      "- Train Loss : 1.7990117410388644e-05 Score : 1.0\n",
      "- Val Loss : 1.172224529000232e-05 Score : 1.0\n",
      "[886/1000]\n",
      "- Train Loss : 1.78794333704799e-05 Score : 1.0\n",
      "- Val Loss : 1.165914272860391e-05 Score : 1.0\n",
      "[887/1000]\n",
      "- Train Loss : 1.780267974835523e-05 Score : 1.0\n",
      "- Val Loss : 1.1580766113183927e-05 Score : 1.0\n",
      "[888/1000]\n",
      "- Train Loss : 1.768985521266586e-05 Score : 1.0\n",
      "- Val Loss : 1.1530811207194347e-05 Score : 1.0\n",
      "[889/1000]\n",
      "- Train Loss : 1.7614923940022385e-05 Score : 1.0\n",
      "- Val Loss : 1.1452864782768302e-05 Score : 1.0\n",
      "[890/1000]\n",
      "- Train Loss : 1.7503897475358422e-05 Score : 1.0\n",
      "- Val Loss : 1.1403250027797185e-05 Score : 1.0\n",
      "[891/1000]\n",
      "- Train Loss : 1.7430750353418665e-05 Score : 1.0\n",
      "- Val Loss : 1.1325706509524025e-05 Score : 1.0\n",
      "[892/1000]\n",
      "- Train Loss : 1.731885858033719e-05 Score : 1.0\n",
      "- Val Loss : 1.1259385246376041e-05 Score : 1.0\n",
      "[893/1000]\n",
      "- Train Loss : 1.7247464559962584e-05 Score : 1.0\n",
      "- Val Loss : 1.119927219406236e-05 Score : 1.0\n",
      "[894/1000]\n",
      "- Train Loss : 1.7136764351259142e-05 Score : 1.0\n",
      "- Val Loss : 1.1141205504827667e-05 Score : 1.0\n",
      "[895/1000]\n",
      "- Train Loss : 1.7052843127708507e-05 Score : 1.0\n",
      "- Val Loss : 1.109641743823886e-05 Score : 1.0\n",
      "[896/1000]\n",
      "- Train Loss : 1.69770406601957e-05 Score : 1.0\n",
      "- Val Loss : 1.1022799299098551e-05 Score : 1.0\n",
      "[897/1000]\n",
      "- Train Loss : 1.6890286057484143e-05 Score : 1.0\n",
      "- Val Loss : 1.0950929208775051e-05 Score : 1.0\n",
      "[898/1000]\n",
      "- Train Loss : 1.6782557976411124e-05 Score : 1.0\n",
      "- Val Loss : 1.0898467735387385e-05 Score : 1.0\n",
      "[899/1000]\n",
      "- Train Loss : 1.6696758469455137e-05 Score : 1.0\n",
      "- Val Loss : 1.0848584679479245e-05 Score : 1.0\n",
      "[900/1000]\n",
      "- Train Loss : 1.6617568386815645e-05 Score : 1.0\n",
      "- Val Loss : 1.0779711374198087e-05 Score : 1.0\n",
      "[901/1000]\n",
      "- Train Loss : 1.654409313939848e-05 Score : 1.0\n",
      "- Val Loss : 1.0712878975027706e-05 Score : 1.0\n",
      "[902/1000]\n",
      "- Train Loss : 1.6428625359468344e-05 Score : 1.0\n",
      "- Val Loss : 1.0652345736161806e-05 Score : 1.0\n",
      "[903/1000]\n",
      "- Train Loss : 1.6347951524646003e-05 Score : 1.0\n",
      "- Val Loss : 1.0607215699565131e-05 Score : 1.0\n",
      "[904/1000]\n",
      "- Train Loss : 1.6274093973657324e-05 Score : 1.0\n",
      "- Val Loss : 1.0539266440900974e-05 Score : 1.0\n",
      "[905/1000]\n",
      "- Train Loss : 1.619916182941476e-05 Score : 1.0\n",
      "- Val Loss : 1.048180911311647e-05 Score : 1.0\n",
      "[906/1000]\n",
      "- Train Loss : 1.6088631822539253e-05 Score : 1.0\n",
      "- Val Loss : 1.0413170457468368e-05 Score : 1.0\n",
      "[907/1000]\n",
      "- Train Loss : 1.6005764943353522e-05 Score : 1.0\n",
      "- Val Loss : 1.037699530570535e-05 Score : 1.0\n",
      "[908/1000]\n",
      "- Train Loss : 1.5931228745103e-05 Score : 1.0\n",
      "- Val Loss : 1.030143721436616e-05 Score : 1.0\n",
      "[909/1000]\n",
      "- Train Loss : 1.5864061500299915e-05 Score : 1.0\n",
      "- Val Loss : 1.0232070962956641e-05 Score : 1.0\n",
      "[910/1000]\n",
      "- Train Loss : 1.5757798021493523e-05 Score : 1.0\n",
      "- Val Loss : 1.018515740724979e-05 Score : 1.0\n",
      "[911/1000]\n",
      "- Train Loss : 1.5670780726395606e-05 Score : 1.0\n",
      "- Val Loss : 1.0140935955860186e-05 Score : 1.0\n",
      "[912/1000]\n",
      "- Train Loss : 1.56041839242486e-05 Score : 1.0\n",
      "- Val Loss : 1.0066250069939997e-05 Score : 1.0\n",
      "[913/1000]\n",
      "- Train Loss : 1.5530860132963022e-05 Score : 1.0\n",
      "- Val Loss : 1.001477812678786e-05 Score : 1.0\n",
      "[914/1000]\n",
      "- Train Loss : 1.542949212307576e-05 Score : 1.0\n",
      "- Val Loss : 9.968261110770982e-06 Score : 1.0\n",
      "[915/1000]\n",
      "- Train Loss : 1.5344213074462863e-05 Score : 1.0\n",
      "- Val Loss : 9.915917871694546e-06 Score : 1.0\n",
      "[916/1000]\n",
      "- Train Loss : 1.5276861707510154e-05 Score : 1.0\n",
      "- Val Loss : 9.84630696621025e-06 Score : 1.0\n",
      "[917/1000]\n",
      "- Train Loss : 1.521186196542759e-05 Score : 1.0\n",
      "- Val Loss : 9.787079761736095e-06 Score : 1.0\n",
      "[918/1000]\n",
      "- Train Loss : 1.5106692090840726e-05 Score : 1.0\n",
      "- Val Loss : 9.736657375469804e-06 Score : 1.0\n",
      "[919/1000]\n",
      "- Train Loss : 1.5025078331139715e-05 Score : 1.0\n",
      "- Val Loss : 9.68893164099427e-06 Score : 1.0\n",
      "[920/1000]\n",
      "- Train Loss : 1.4957595453779504e-05 Score : 1.0\n",
      "- Val Loss : 9.624376616557129e-06 Score : 1.0\n",
      "[921/1000]\n",
      "- Train Loss : 1.4893603798150758e-05 Score : 1.0\n",
      "- Val Loss : 9.570151632942725e-06 Score : 1.0\n",
      "[922/1000]\n",
      "- Train Loss : 1.479651143654337e-05 Score : 1.0\n",
      "- Val Loss : 9.528536793368403e-06 Score : 1.0\n",
      "[923/1000]\n",
      "- Train Loss : 1.471322675822901e-05 Score : 1.0\n",
      "- Val Loss : 9.476895684201736e-06 Score : 1.0\n",
      "[924/1000]\n",
      "- Train Loss : 1.4644268149923948e-05 Score : 1.0\n",
      "- Val Loss : 9.413073712494224e-06 Score : 1.0\n",
      "[925/1000]\n",
      "- Train Loss : 1.4583183100411487e-05 Score : 1.0\n",
      "- Val Loss : 9.351041626359802e-06 Score : 1.0\n",
      "[926/1000]\n",
      "- Train Loss : 1.4488174340638378e-05 Score : 1.0\n",
      "- Val Loss : 9.309702363680117e-06 Score : 1.0\n",
      "[927/1000]\n",
      "- Train Loss : 1.440317769013038e-05 Score : 1.0\n",
      "- Val Loss : 9.26686971070012e-06 Score : 1.0\n",
      "[928/1000]\n",
      "- Train Loss : 1.4341951024713731e-05 Score : 1.0\n",
      "- Val Loss : 9.212280019710306e-06 Score : 1.0\n",
      "[929/1000]\n",
      "- Train Loss : 1.4283758496781553e-05 Score : 1.0\n",
      "- Val Loss : 9.15091823117109e-06 Score : 1.0\n",
      "[930/1000]\n",
      "- Train Loss : 1.4182183728432088e-05 Score : 1.0\n",
      "- Val Loss : 9.105590834224131e-06 Score : 1.0\n",
      "[931/1000]\n",
      "- Train Loss : 1.4106059855597172e-05 Score : 1.0\n",
      "- Val Loss : 9.063028301170561e-06 Score : 1.0\n",
      "[932/1000]\n",
      "- Train Loss : 1.4045902440153037e-05 Score : 1.0\n",
      "- Val Loss : 9.009098903334234e-06 Score : 1.0\n",
      "[933/1000]\n",
      "- Train Loss : 1.398260792484507e-05 Score : 1.0\n",
      "- Val Loss : 8.948375580075663e-06 Score : 1.0\n",
      "[934/1000]\n",
      "- Train Loss : 1.3894295067176346e-05 Score : 1.0\n",
      "- Val Loss : 8.903299203666393e-06 Score : 1.0\n",
      "[935/1000]\n",
      "- Train Loss : 1.3815783467584373e-05 Score : 1.0\n",
      "- Val Loss : 8.869561497704126e-06 Score : 1.0\n",
      "[936/1000]\n",
      "- Train Loss : 1.3750763830305206e-05 Score : 1.0\n",
      "- Val Loss : 8.803508535493165e-06 Score : 1.0\n",
      "[937/1000]\n",
      "- Train Loss : 1.3694257985561206e-05 Score : 1.0\n",
      "- Val Loss : 8.747678293730132e-06 Score : 1.0\n",
      "[938/1000]\n",
      "- Train Loss : 1.3603300708887724e-05 Score : 1.0\n",
      "- Val Loss : 8.707082088221796e-06 Score : 1.0\n",
      "[939/1000]\n",
      "- Train Loss : 1.353228630149614e-05 Score : 1.0\n",
      "- Val Loss : 8.673576303408481e-06 Score : 1.0\n",
      "[940/1000]\n",
      "- Train Loss : 1.347029251519416e-05 Score : 1.0\n",
      "- Val Loss : 8.61671105667483e-06 Score : 1.0\n",
      "[941/1000]\n",
      "- Train Loss : 1.3412647035693226e-05 Score : 1.0\n",
      "- Val Loss : 8.56151291372953e-06 Score : 1.0\n",
      "[942/1000]\n",
      "- Train Loss : 1.3321691451690438e-05 Score : 1.0\n",
      "- Val Loss : 8.52110952109797e-06 Score : 1.0\n",
      "[943/1000]\n",
      "- Train Loss : 1.3249540870674537e-05 Score : 1.0\n",
      "- Val Loss : 8.475055437884293e-06 Score : 1.0\n",
      "[944/1000]\n",
      "- Train Loss : 1.3187221674747383e-05 Score : 1.0\n",
      "- Val Loss : 8.4187840911909e-06 Score : 1.0\n",
      "[945/1000]\n",
      "- Train Loss : 1.3130988728941398e-05 Score : 1.0\n",
      "- Val Loss : 8.372646334464662e-06 Score : 1.0\n",
      "[946/1000]\n",
      "- Train Loss : 1.3049876694518995e-05 Score : 1.0\n",
      "- Val Loss : 8.332454854098614e-06 Score : 1.0\n",
      "[947/1000]\n",
      "- Train Loss : 1.2973293564755295e-05 Score : 1.0\n",
      "- Val Loss : 8.29513737699017e-06 Score : 1.0\n",
      "[948/1000]\n",
      "- Train Loss : 1.291597508649526e-05 Score : 1.0\n",
      "- Val Loss : 8.235208952100947e-06 Score : 1.0\n",
      "[949/1000]\n",
      "- Train Loss : 1.2861825287776204e-05 Score : 1.0\n",
      "- Val Loss : 8.18966691440437e-06 Score : 1.0\n",
      "[950/1000]\n",
      "- Train Loss : 1.2774057078483262e-05 Score : 1.0\n",
      "- Val Loss : 8.153918315656483e-06 Score : 1.0\n",
      "[951/1000]\n",
      "- Train Loss : 1.2706245658793907e-05 Score : 1.0\n",
      "- Val Loss : 8.112536306725815e-06 Score : 1.0\n",
      "[952/1000]\n",
      "- Train Loss : 1.2653210622678065e-05 Score : 1.0\n",
      "- Val Loss : 8.057458217081148e-06 Score : 1.0\n",
      "[953/1000]\n",
      "- Train Loss : 1.2591208347758868e-05 Score : 1.0\n",
      "- Val Loss : 8.00395446276525e-06 Score : 1.0\n",
      "[954/1000]\n",
      "- Train Loss : 1.2507327609354332e-05 Score : 1.0\n",
      "- Val Loss : 7.972639650688507e-06 Score : 1.0\n",
      "[955/1000]\n",
      "- Train Loss : 1.2444972703128264e-05 Score : 1.0\n",
      "- Val Loss : 7.935718713270035e-06 Score : 1.0\n",
      "[956/1000]\n",
      "- Train Loss : 1.2384968979656858e-05 Score : 1.0\n",
      "- Val Loss : 7.881196324888151e-06 Score : 1.0\n",
      "[957/1000]\n",
      "- Train Loss : 1.233493238335844e-05 Score : 1.0\n",
      "- Val Loss : 7.828226443962194e-06 Score : 1.0\n",
      "[958/1000]\n",
      "- Train Loss : 1.2252934109508513e-05 Score : 1.0\n",
      "- Val Loss : 7.792831638653297e-06 Score : 1.0\n",
      "[959/1000]\n",
      "- Train Loss : 1.2186128414113126e-05 Score : 1.0\n",
      "- Val Loss : 7.764641850371845e-06 Score : 1.0\n",
      "[960/1000]\n",
      "- Train Loss : 1.2133737502356073e-05 Score : 1.0\n",
      "- Val Loss : 7.706445103394799e-06 Score : 1.0\n",
      "[961/1000]\n",
      "- Train Loss : 1.2080456220549726e-05 Score : 1.0\n",
      "- Val Loss : 7.6582755355048e-06 Score : 1.0\n",
      "[962/1000]\n",
      "- Train Loss : 1.1994384180979978e-05 Score : 1.0\n",
      "- Val Loss : 7.6230321610637475e-06 Score : 1.0\n",
      "[963/1000]\n",
      "- Train Loss : 1.1933666845228194e-05 Score : 1.0\n",
      "- Val Loss : 7.5864836617256515e-06 Score : 1.0\n",
      "[964/1000]\n",
      "- Train Loss : 1.188153770373093e-05 Score : 1.0\n",
      "- Val Loss : 7.533051302743843e-06 Score : 1.0\n",
      "[965/1000]\n",
      "- Train Loss : 1.1828249209126321e-05 Score : 1.0\n",
      "- Val Loss : 7.489666586479871e-06 Score : 1.0\n",
      "[966/1000]\n",
      "- Train Loss : 1.1746032302855585e-05 Score : 1.0\n",
      "- Val Loss : 7.450311841239454e-06 Score : 1.0\n",
      "[967/1000]\n",
      "- Train Loss : 1.1692069607407677e-05 Score : 1.0\n",
      "- Val Loss : 7.4139506978099234e-06 Score : 1.0\n",
      "[968/1000]\n",
      "- Train Loss : 1.1635607140508202e-05 Score : 1.0\n",
      "- Val Loss : 7.369589184236247e-06 Score : 1.0\n",
      "[969/1000]\n",
      "- Train Loss : 1.158632255737757e-05 Score : 1.0\n",
      "- Val Loss : 7.318192274397006e-06 Score : 1.0\n",
      "[970/1000]\n",
      "- Train Loss : 1.1504631465363167e-05 Score : 1.0\n",
      "- Val Loss : 7.29177509128931e-06 Score : 1.0\n",
      "[971/1000]\n",
      "- Train Loss : 1.1449492831161479e-05 Score : 1.0\n",
      "- Val Loss : 7.264115993166342e-06 Score : 1.0\n",
      "[972/1000]\n",
      "- Train Loss : 1.1395967451082672e-05 Score : 1.0\n",
      "- Val Loss : 7.2202651608677115e-06 Score : 1.0\n",
      "[973/1000]\n",
      "- Train Loss : 1.134601135769723e-05 Score : 1.0\n",
      "- Val Loss : 7.169367563619744e-06 Score : 1.0\n",
      "[974/1000]\n",
      "- Train Loss : 1.1270131861035932e-05 Score : 1.0\n",
      "- Val Loss : 7.1303461481875274e-06 Score : 1.0\n",
      "[975/1000]\n",
      "- Train Loss : 1.1211846021473724e-05 Score : 1.0\n",
      "- Val Loss : 7.094349712133408e-06 Score : 1.0\n",
      "[976/1000]\n",
      "- Train Loss : 1.1164558713971928e-05 Score : 1.0\n",
      "- Val Loss : 7.046781774988631e-06 Score : 1.0\n",
      "[977/1000]\n",
      "- Train Loss : 1.1108652529401297e-05 Score : 1.0\n",
      "- Val Loss : 7.004874078120338e-06 Score : 1.0\n",
      "[978/1000]\n",
      "- Train Loss : 1.1033266850467448e-05 Score : 1.0\n",
      "- Val Loss : 6.9744951360917185e-06 Score : 1.0\n",
      "[979/1000]\n",
      "- Train Loss : 1.0981703806262683e-05 Score : 1.0\n",
      "- Val Loss : 6.9386310315167066e-06 Score : 1.0\n",
      "[980/1000]\n",
      "- Train Loss : 1.0931999807123955e-05 Score : 1.0\n",
      "- Val Loss : 6.900054813741008e-06 Score : 1.0\n",
      "[981/1000]\n",
      "- Train Loss : 1.0879344358727394e-05 Score : 1.0\n",
      "- Val Loss : 6.858631422801409e-06 Score : 1.0\n",
      "[982/1000]\n",
      "- Train Loss : 1.0807729116398454e-05 Score : 1.0\n",
      "- Val Loss : 6.815694177930709e-06 Score : 1.0\n",
      "[983/1000]\n",
      "- Train Loss : 1.0763744158895053e-05 Score : 1.0\n",
      "- Val Loss : 6.7795040195051115e-06 Score : 1.0\n",
      "[984/1000]\n",
      "- Train Loss : 1.0698518521065933e-05 Score : 1.0\n",
      "- Val Loss : 6.747218776581576e-06 Score : 1.0\n",
      "[985/1000]\n",
      "- Train Loss : 1.0656924018803693e-05 Score : 1.0\n",
      "- Val Loss : 6.707774446113035e-06 Score : 1.0\n",
      "[986/1000]\n",
      "- Train Loss : 1.0584832100196056e-05 Score : 1.0\n",
      "- Val Loss : 6.671662958979141e-06 Score : 1.0\n",
      "[987/1000]\n",
      "- Train Loss : 1.0541086958760894e-05 Score : 1.0\n",
      "- Val Loss : 6.628204118896974e-06 Score : 1.0\n",
      "[988/1000]\n",
      "- Train Loss : 1.0481158546503542e-05 Score : 1.0\n",
      "- Val Loss : 6.59646320855245e-06 Score : 1.0\n",
      "[989/1000]\n",
      "- Train Loss : 1.0434924812822525e-05 Score : 1.0\n",
      "- Val Loss : 6.553170806000708e-06 Score : 1.0\n",
      "[990/1000]\n",
      "- Train Loss : 1.0369203146688556e-05 Score : 1.0\n",
      "- Val Loss : 6.525767730636289e-06 Score : 1.0\n",
      "[991/1000]\n",
      "- Train Loss : 1.0327055835861958e-05 Score : 1.0\n",
      "- Val Loss : 6.4911132540146355e-06 Score : 1.0\n",
      "[992/1000]\n",
      "- Train Loss : 1.0262112255077227e-05 Score : 1.0\n",
      "- Val Loss : 6.45525233267108e-06 Score : 1.0\n",
      "[993/1000]\n",
      "- Train Loss : 1.022006169174953e-05 Score : 1.0\n",
      "- Val Loss : 6.407956789189484e-06 Score : 1.0\n",
      "[994/1000]\n",
      "- Train Loss : 1.0157237549416701e-05 Score : 1.0\n",
      "- Val Loss : 6.384950665960787e-06 Score : 1.0\n",
      "[995/1000]\n",
      "- Train Loss : 1.0111323995766321e-05 Score : 1.0\n",
      "- Val Loss : 6.346298050630139e-06 Score : 1.0\n",
      "[996/1000]\n",
      "- Train Loss : 1.0051933663311906e-05 Score : 1.0\n",
      "- Val Loss : 6.3148227127385326e-06 Score : 1.0\n",
      "[997/1000]\n",
      "- Train Loss : 1.0007421211513349e-05 Score : 1.0\n",
      "- Val Loss : 6.280541128944606e-06 Score : 1.0\n",
      "[998/1000]\n",
      "- Train Loss : 9.947478184181415e-06 Score : 1.0\n",
      "- Val Loss : 6.244892574613914e-06 Score : 1.0\n",
      "[999/1000]\n",
      "- Train Loss : 9.905747838779967e-06 Score : 1.0\n",
      "- Val Loss : 6.215032954060007e-06 Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 손실값과 성능평가값 저장 필요 \n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]],[[],[]]\n",
    "CNT = len(train_dl)\n",
    "print(f'CNT : {CNT}')\n",
    " \n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    # 학습 모드로 모델 설정 \n",
    "    model.train()\n",
    "\n",
    "    # 배치 크기 만큼 데이터 로딩해서 학습 진행 \n",
    "    loss_total, score_total = 0,0\n",
    "    for feature_ts, target_ts in train_dl: # iris_dl -> train_dl \n",
    "\n",
    "        # 학습 진행 \n",
    "        pre_y = model(feature_ts)\n",
    "\n",
    "        # 손실 계산  \n",
    "        loss = req_loss(pre_y, target_ts)\n",
    "        loss_total += loss.item() # tensor 라서 item으로 값 넣어야 함 \n",
    "\n",
    "        # 성능 평가 계산 \n",
    "        score = BinaryF1Score()(pre_y, target_ts) \n",
    "        score_total += score.item() # tensor 라서 item으로 값 넣어야 함 \n",
    "\n",
    "        # 최적화 진행 \n",
    "        optimizer.zero_grad()       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 에포크 당 검증기능 \n",
    "    # 모델 검증 모드 설정 \n",
    "    model.eval()\n",
    "    # 검증한 결과를 저장해야 함 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋 \n",
    "        val_feature_ts = torch.FloatTensor(val_ds.feature_df.values) # values쓰면 array 됨 -> torch사용 -> torch됨 \n",
    "        val_target_ts = torch.FloatTensor(val_ds.target_df.values)\n",
    "\n",
    "        # 평가 \n",
    "        pre_val =model(val_feature_ts)\n",
    "        \n",
    "        # 손실 \n",
    "        loss_val = req_loss(pre_val, val_target_ts)\n",
    "\n",
    "        # 성능 평가 \n",
    "        score_val = BinaryF1Score()(pre_val, val_target_ts)\n",
    "\n",
    "\n",
    "    # for문 다 돌면 1 epoch 종료 \n",
    "    # 손실값과 성능평가값 저장 \n",
    "    LOSS_HISTORY[0].append(loss_total/CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/CNT)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 결과 체크 => 학습과 검증의 Loss 변화, 성능 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SCORE')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THRESHOLD = 200\n",
    "\n",
    "fg, axes = plt.subplots(1,2, figsize = (10,5))\n",
    "axes[0].plot(range(1,THRESHOLD+1) ,LOSS_HISTORY[0][:THRESHOLD], label = 'Train')\n",
    "axes[0].plot(range(1,THRESHOLD+1), LOSS_HISTORY[1][:THRESHOLD], label = 'Val')\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('LOSS')\n",
    "\n",
    "axes[1].plot(range(1,THRESHOLD+1) ,SCORE_HISTORY[0][:THRESHOLD], label = 'Train')\n",
    "axes[1].plot(range(1,THRESHOLD+1), SCORE_HISTORY[1][:THRESHOLD], label = 'Val')\n",
    "axes[1].grid()\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('SCORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
