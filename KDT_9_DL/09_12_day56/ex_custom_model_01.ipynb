{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 모델 클래스 \n",
    "- 부모 클래스 : nn.Module\n",
    "- 필수 오버라이딩 \n",
    "    - _ _init_ _() : 모델층 구성 즉, 설계\n",
    "    - forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                # 텐서 관련 모듈 \n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈 \n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 (손실함수, 활성화 함수 등등)\n",
    "import torch.optim as optim                 # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘 )\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련 모듈 \n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈 \n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망 클래스 <hr>\n",
    "    - 입력층 - 입력 : 피쳐수 고정 \n",
    "    - 출력층 - 출력 : 타겟수 고정 \n",
    "    - 은닉층 - 고정     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정 \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정 \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계 \n",
    "\n",
    "# 데이터 셋 : 피쳐 4개, 타겟 1개, 회귀  \n",
    "# 입 력 층 : 입력   4개,    출력  20개,     AF      ReLU(기울기 소실)\n",
    "# 은 닉 층 : 입력  20개,    출력 100개,     AF      ReLU\n",
    "# 출 력 층 : 입력  100개,   출력   1개,     AF      [회귀] X(회귀는 필요없음) , [분류] Sigmoid(딥러닝 - 이진분류) 또는 SoftMax(딥러닝 - 다중분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    # 인스턴스/ 객체 생성 시 자동호출되는 메서드 (콜백함수 Callback Func)\n",
    "\n",
    "    def __init__(self): # __x__ : 매직메서드 (파이썬 시스템에 의해 호출됨)\n",
    "        # 부모클래스 생성 \n",
    "        super().__init__()\n",
    "\n",
    "        # 자식 클래스의 인스턴스 속성 설정 \n",
    "        self.input_layer = nn.Linear(4,20)     # w 4 + b 1 => 1퍼셉트론, 5 * 20개의 변수 생성 \n",
    "        self.hidden_layer = nn.Linear(20,100)  # w 20 + b 1 => 1퍼셉트론 , 21 * 100개의 변수 생성 \n",
    "        self.output_layer = nn.Linear(100,1)   # w 100 + b 1 => 1퍼셉트론, 101 * 1 개의 변수 생성 \n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터 셋\n",
    "\n",
    "    def forward(self, x): # 데이터를 넣어야 함\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개의 퍼셉트론 : y = x1w1 + x2w2 + x3w3 + x4w4 + b\n",
    "        y = F.relu(y)               # 활성함수에 넣어서 결과값 반환 -> 0 <= y ----> 단점 : 죽은 relu 발생 -----> LeakyReLu 사용 \n",
    "\n",
    "        y = self.hidden_layer(y)    # 1개의 퍼셉트론 : y = x1w1 + x2w2 ..... x20w20 + b\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개의 퍼셉트론 : y = x1w1 + x2w2 ..... x100w100 + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모델은 무조건 (4,1)가 되어야 하지만 아래 모델은 동적으로 변경 ㅇ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델 \n",
    "\n",
    "\n",
    "class MyModel2(nn.Module):\n",
    "\n",
    "    # 인스턴스/ 객체 생성 시 자동호출되는 메서드 (콜백함수 Callback Func)\n",
    "\n",
    "    def __init__(self, in_features): # __x__ : 매직메서드 (파이썬 시스템에 의해 호출됨)\n",
    "        # 부모클래스 생성 \n",
    "        super().__init__()\n",
    "\n",
    "        # 자식 클래스의 인스턴스 속성 설정 \n",
    "        self.input_layer = nn.Linear(in_features,20)      \n",
    "        self.hidden_layer = nn.Linear(20,100)           \n",
    "        self.output_layer = nn.Linear(100,1)            \n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터 셋\n",
    "\n",
    "    def forward(self, x): # 데이터를 넣어야 함\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     \n",
    "        y = F.relu(y)               \n",
    "\n",
    "        y = self.hidden_layer(y)   \n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수가 동적인 모델 \n",
    "\n",
    "class MyModel3(nn.Module):\n",
    "\n",
    "    # 인스턴스/ 객체 생성 시 자동호출되는 메서드 (콜백함수 Callback Func)\n",
    "\n",
    "    def __init__(self, in_features, in_out, h_out): # __x__ : 매직메서드 (파이썬 시스템에 의해 호출됨)\n",
    "        # 부모클래스 생성 \n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정 \n",
    "        self.input_layer = nn.Linear(in_features,in_out)     \n",
    "        self.hidden_layer = nn.Linear(in_out,h_out) \n",
    "        self.output_layer = nn.Linear(h_out,1)   \n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터 셋\n",
    "\n",
    "    def forward(self, x): # 데이터를 넣어야 함\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)    \n",
    "        y = F.relu(y)               \n",
    "\n",
    "        y = self.hidden_layer(y)    \n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델 \n",
    "\n",
    "class MyModel4(nn.Module):\n",
    "\n",
    "    # 인스턴스/ 객체 생성 시 자동호출되는 메서드 (콜백함수 Callback Func)\n",
    "\n",
    "    def __init__(self, in_features, in_out, h_cnt): # __x__ : 매직메서드 (파이썬 시스템에 의해 호출됨)\n",
    "        # 부모클래스 생성 \n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정 \n",
    "        self.input_layer = nn.Linear(in_features,in_out)    \n",
    "\n",
    "        n = 5\n",
    "        if h_cnt != 0: \n",
    "            for i in range(1,h_cnt+1):\n",
    "                \n",
    "                self.hidden_layer = nn.Linear(in_out + (i-1)*n, in_out + i*n)\n",
    "                print(f'(hidden_layer): Linear(in_features={in_out + (i-1)*n}, out_features={in_out + i*n}, bias=True)')\n",
    "            \n",
    "        if h_cnt == 0:\n",
    "            self.output_layer = nn.Linear(in_out,1)  \n",
    "        else:\n",
    "            self.output_layer = nn.Linear(in_out + (h_cnt)*n,1)  \n",
    "        \n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터 셋\n",
    "\n",
    "    def forward(self, x): # 데이터를 넣어야 함\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     \n",
    "        y = F.relu(y)               \n",
    "\n",
    "        y = self.hidden_layer(y)    \n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex_custom_model_02, 03 파일에서 배운 nn.ModuleList 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델 \n",
    "\n",
    "class MyModel4_1(nn.Module):\n",
    "\n",
    "    # 인스턴스/ 객체 생성 시 자동호출되는 메서드 (콜백함수 Callback Func)\n",
    "\n",
    "    def __init__(self, in_features, in_out, h_cnt): # __x__ : 매직메서드 (파이썬 시스템에 의해 호출됨)\n",
    "        # 부모클래스 생성 \n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정 \n",
    "        self.input_layer = nn.Linear(in_features,in_out)    \n",
    "\n",
    "        self.hidden_layer = nn.ModuleList()\n",
    "        n = 5\n",
    "        if h_cnt != 0: \n",
    "            for i in range(1,h_cnt+1):\n",
    "                \n",
    "                self.hidden_layer.append(nn.Linear(in_out + (i-1)*n, in_out + i*n))\n",
    "            \n",
    "        if h_cnt == 0:\n",
    "            self.output_layer = nn.Linear(in_out,1)  \n",
    "        else:\n",
    "            self.output_layer = nn.Linear(in_out + (h_cnt)*n,1)  \n",
    "        \n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback Func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터 셋\n",
    "\n",
    "    def forward(self, x): # 데이터를 넣어야 함\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     \n",
    "        y = F.relu(y)               \n",
    "\n",
    "        y = self.hidden_layer(y)    \n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성 \n",
    "m1 = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel3(\n",
       "  (input_layer): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (hidden_layer): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (output_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_3 = MyModel3(3,50,30) # 동적으로 생성 ㅇ \n",
    "m1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hidden_layer): Linear(in_features=60, out_features=65, bias=True)\n",
      "(hidden_layer): Linear(in_features=65, out_features=70, bias=True)\n",
      "(hidden_layer): Linear(in_features=70, out_features=75, bias=True)\n",
      "(hidden_layer): Linear(in_features=75, out_features=80, bias=True)\n",
      "(hidden_layer): Linear(in_features=80, out_features=85, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel4(\n",
       "  (input_layer): Linear(in_features=4, out_features=60, bias=True)\n",
       "  (hidden_layer): Linear(in_features=80, out_features=85, bias=True)\n",
       "  (output_layer): Linear(in_features=85, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = MyModel4(4,60,5)\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel4_1(\n",
       "  (input_layer): Linear(in_features=4, out_features=60, bias=True)\n",
       "  (hidden_layer): ModuleList(\n",
       "    (0): Linear(in_features=60, out_features=65, bias=True)\n",
       "    (1): Linear(in_features=65, out_features=70, bias=True)\n",
       "    (2): Linear(in_features=70, out_features=75, bias=True)\n",
       "    (3): Linear(in_features=75, out_features=80, bias=True)\n",
       "    (4): Linear(in_features=80, out_features=85, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=85, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_m = MyModel4_1(4,60,5)\n",
    "m2_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel4(\n",
       "  (input_layer): Linear(in_features=4, out_features=40, bias=True)\n",
       "  (output_layer): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = MyModel4(4,40,0)\n",
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450],\n",
      "        [ 0.0725, -0.0020,  0.4371,  0.1556],\n",
      "        [-0.1862, -0.3020, -0.0838, -0.2157],\n",
      "        [-0.1602,  0.0239,  0.2981,  0.2718],\n",
      "        [-0.4888,  0.3100,  0.1397,  0.4743],\n",
      "        [ 0.3300, -0.4556, -0.4754, -0.2412],\n",
      "        [ 0.4391, -0.0833,  0.2140, -0.2324],\n",
      "        [ 0.4906, -0.2115,  0.3750,  0.0059],\n",
      "        [-0.2634,  0.2570, -0.2654,  0.1471],\n",
      "        [-0.1444, -0.0548, -0.4807, -0.2384],\n",
      "        [ 0.2713, -0.1215,  0.4980,  0.4008],\n",
      "        [-0.0234, -0.3337,  0.3045,  0.1552],\n",
      "        [-0.3232,  0.3248,  0.3036,  0.4434],\n",
      "        [-0.2803, -0.0823, -0.0097,  0.0730],\n",
      "        [-0.3795, -0.3548,  0.2720, -0.1172],\n",
      "        [ 0.2442,  0.0285,  0.1642,  0.1099],\n",
      "        [ 0.1818,  0.2479, -0.4631,  0.2517]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.3516, -0.3773,  0.0304, -0.0852,  0.2937, -0.2896, -0.4445,  0.3639,\n",
      "        -0.0741,  0.2812,  0.1607, -0.3749,  0.1004,  0.1201, -0.3348, -0.2372,\n",
      "         0.1705,  0.0896, -0.2127, -0.1514], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2048, -0.0414,  0.1261,  ...,  0.1389, -0.1618, -0.1610],\n",
      "        [-0.1352,  0.0281,  0.2229,  ...,  0.0578, -0.1529, -0.1878],\n",
      "        [-0.1025, -0.0260, -0.1371,  ..., -0.0956,  0.1590, -0.0731],\n",
      "        ...,\n",
      "        [-0.1844, -0.1471, -0.1923,  ...,  0.1118,  0.0691,  0.0738],\n",
      "        [-0.2086, -0.0753, -0.1438,  ...,  0.1593, -0.0429,  0.0194],\n",
      "        [-0.1867,  0.1260,  0.0129,  ...,  0.0369, -0.1090, -0.0607]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.0588,  0.1044,  0.1574,  0.1782, -0.1521, -0.0239,  0.0525, -0.0874,\n",
      "         0.1470, -0.0249,  0.0239, -0.1912,  0.1616, -0.1482, -0.0054, -0.1824,\n",
      "         0.1701,  0.0129,  0.1007, -0.1758,  0.0190,  0.0916,  0.1579,  0.1643,\n",
      "        -0.0399, -0.0227,  0.0743, -0.0804, -0.0910, -0.2059, -0.2193, -0.1177,\n",
      "         0.0279,  0.0180,  0.0179,  0.0335, -0.0413, -0.2209,  0.1997, -0.1636,\n",
      "        -0.0026,  0.0888,  0.0573, -0.0971, -0.1943,  0.1790,  0.0064,  0.2039,\n",
      "         0.1043,  0.0955,  0.0484, -0.0504, -0.1311,  0.0272,  0.2149,  0.1341,\n",
      "         0.2031,  0.2097,  0.1747, -0.1255, -0.1218, -0.0077, -0.1156, -0.0607,\n",
      "        -0.0766,  0.1957, -0.0150,  0.2164, -0.1734, -0.2206,  0.0320,  0.0446,\n",
      "        -0.0902, -0.1132,  0.0637,  0.0112, -0.0300,  0.1135, -0.0951,  0.1759,\n",
      "        -0.1525, -0.1164,  0.0774,  0.1374, -0.0027, -0.1755, -0.0089, -0.0755,\n",
      "        -0.0714, -0.0413,  0.0538, -0.0520, -0.1568, -0.1116, -0.1873, -0.2233,\n",
      "         0.1081,  0.1249,  0.1690,  0.0177], requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[-0.0411, -0.0601,  0.0498, -0.0604,  0.0747, -0.0542, -0.0933, -0.0720,\n",
      "          0.0814,  0.0755,  0.0244,  0.0023,  0.0857,  0.0603,  0.0592, -0.0681,\n",
      "          0.0160,  0.0906, -0.0704,  0.0996, -0.0655, -0.0294, -0.0047,  0.0260,\n",
      "          0.0307, -0.0572,  0.0458,  0.0265, -0.0111, -0.0995,  0.0990, -0.0370,\n",
      "         -0.0533,  0.0990, -0.0507,  0.0668,  0.0610, -0.0651, -0.0806, -0.0544,\n",
      "          0.0313,  0.0943, -0.0056, -0.0299, -0.0288, -0.0696,  0.0999, -0.0011,\n",
      "         -0.0791, -0.0683,  0.0622,  0.0484,  0.0645, -0.0105, -0.0198,  0.0328,\n",
      "         -0.0010, -0.0322,  0.0238, -0.0570, -0.0692, -0.0221, -0.0713,  0.0025,\n",
      "          0.0157,  0.0979,  0.0288, -0.0006, -0.0727, -0.0402, -0.0163, -0.0720,\n",
      "          0.0195,  0.0720,  0.0572, -0.0415, -0.0244,  0.0997, -0.0160, -0.0898,\n",
      "         -0.0009,  0.0051, -0.0372, -0.0109, -0.0815,  0.0308, -0.0694,  0.0344,\n",
      "          0.0753, -0.0646, -0.0392,  0.0096,  0.0002,  0.0586, -0.0060,  0.0162,\n",
      "         -0.0276,  0.0330,  0.0302, -0.0805]], requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([0.0531], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 즉, w와 b 확인 \n",
    "\n",
    "for m in m1.named_parameters(): print(m)\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "#   pass\n",
    "\n",
    "# 만든게 없어서 출력 아무것도 안 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[-0.1395],\n",
      "        [-0.1858]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스명(데이터)\n",
    "# 임의의 데이터 생성 \n",
    "\n",
    "data_ts = torch.FloatTensor([[1,3,5,7],[2,4,6,8]]) # Linear 모듈이 float을 지원하므로 float 사용 \n",
    "# 설계한 shape에 맞게 개수 줘야 함 \n",
    "# self.input_layer = nn.Linear(4,20) -> [[1,3,5],[2,4,6]] -> 오류 발생 \n",
    "target_ts = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습 \n",
    "pre_y = m1(data_ts)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> [1,3,5,7] 로 -0.1395, [2,4,6,8] 로 -0.1858예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
