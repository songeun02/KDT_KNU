{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 모델 클래스 \n",
    "- 부모 클래스 : nn.Module\n",
    "- 필수 오버라이딩 \n",
    "    - _ _init_ _() : 모델층 구성 즉, 설계\n",
    "    - forward() : 순방향 학습 진행 코드 구현\n",
    "- 동적 모델 \n",
    "    - container 모듈 중 nn.ModuleList() 사용해서 동적으로 Layer 추가 \n",
    "        - forward 기능 미 제공 \n",
    "        - layer 인스턴스 요소 사이에 연관성 없음 \n",
    "        - layer 인스턴스 요소는 인덱싱으로 접근 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                # 텐서 관련 모듈 \n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈 \n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 (손실함수, 활성화 함수 등등)\n",
    "import torch.optim as optim                 # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘 )\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련 모듈 \n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈 \n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망 클래스 <hr>\n",
    "    - 입력층 - 입력 : 피쳐수 고정 \n",
    "    - 출력층 - 출력 : 타겟수 고정 \n",
    "    - 은닉층 - 고정     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정 \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정 \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 설계 - 동적 모델 <hr>\n",
    "    - 목표 : 은닉층의 개수가 동적인 모델 \n",
    "    - 조건 \n",
    "        - 입력층과 출력층 개수 동적 => 입력층의 입력값, 출력값의 출력값 \n",
    "        - 은닉층의 개수 동적 + 퍼셉트론 개수 고정 => 은닉층의 개수, 은닉층 마다 퍼셉트론 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이름 : DynamicModel\n",
    "# 부모 클래스 : nn.Module\n",
    "# 매개변수 : in_in, out_out, h_ins = [], n_outs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModel(nn.Module):\n",
    "\n",
    "\n",
    "    # 모델 구조 설계 함수 즉, 생성자 메서드 \n",
    "    def __init__(self, in_in,in_out, out_out , h_ins = [], h_outs = []):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer = nn.Linear(in_in, h_ins[0] if len(h_ins) else in_out)\n",
    "        \n",
    "        self.h_layers =nn.ModuleList()\n",
    "        for idx in range(len(h_ins)):\n",
    "            self.h_layers.append(nn.Linear(h_ins[idx],h_outs[idx]))\n",
    "        \n",
    "        self.out_layer = nn.Linear(h_outs[-1] if len(h_outs) else in_out, out_out)\n",
    "\n",
    "\n",
    "    # 학습 진행 콜백 메서드 \n",
    "    def forward(self, x):\n",
    "        # 입력층 \n",
    "        y = self.in_layer(x)    # y = x1w1 + x2w2 + x3w3 + b\n",
    "        y = F.relu(y)           # 결과값은 0 ~ \n",
    "        # y = F.relu(self.in_layer(x)) 로 줄여쓸 수 ㅇ \n",
    "\n",
    "        # 은닉층 \n",
    "        for linear in self.h_layers:\n",
    "            y = linear(y)\n",
    "            y = F.relu(y)\n",
    "            # y = F.relu(linear(y)) 로 줄여쓸 수 ㅇ \n",
    "\n",
    "        # 출력층  \n",
    "        return self.out_layer(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModel_h(nn.Module):\n",
    "\n",
    "\n",
    "    # 모델 구조 설계 함수 즉, 생성자 메서드 \n",
    "    def __init__(self, in_in,in_out, out_in, out_out , h_ins = [], h_outs = []):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer = nn.Linear(in_in, h_ins[0] if len(h_ins) else in_out)\n",
    "        # self.h_layers =nn.ModuleList( [nn.Linear(h_inout, h_inout) for _ in range(h_cnt)])\n",
    "        self.out_layer = nn.Linear(h_outs[-1] if len(h_outs) else out_in, out_out)\n",
    "\n",
    "\n",
    "    # 학습 진행 콜백 메서드 \n",
    "    def forward(self, x):\n",
    "        # 입력층 \n",
    "        y = self.in_layer(x)    # y = x1w1 + x2w2 + x3w3 + b\n",
    "        y = F.relu(y)           # 결과값은 0 ~ \n",
    "        # y = F.relu(self.in_layer(x)) 로 줄여쓸 수 ㅇ \n",
    "\n",
    "        # 은닉층 \n",
    "        for linear in self.h_layers:\n",
    "            y = linear(y)\n",
    "            y = F.relu(y)\n",
    "            # y = F.relu(linear(y)) 로 줄여쓸 수 ㅇ \n",
    "\n",
    "        # 출력층  \n",
    "        return self.out_layer(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicModel(\n",
      "  (in_layer): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (h_layers): ModuleList()\n",
      "  (out_layer): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m1 = DynamicModel(3,5,2) # 은닉층이 없을 때 \n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicModel_h(\n",
      "  (in_layer): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m2 = DynamicModel_h(3,5,4,2) \n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_layer.weight torch.Size([5, 3])\n",
      "in_layer.bias torch.Size([5])\n",
      "out_layer.weight torch.Size([2, 5])\n",
      "out_layer.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in m1.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성 \n",
    "h_ins , h_outs = [30,50,70],[50,70,30] # 은닉층 3개 \n",
    "#               = range(10,101,20), range(30,121,20)\n",
    "#               => 10 30 50 70 90 , 30 50 70 90 110\n",
    "# h_inouts = [10, 30, 50, 70 ,90]\n",
    "# => (10,30) (30,50), (50,70), (70,90)\n",
    "# => out은 -1: 와야 함 -> 끝엔 없으므로 \n",
    "\n",
    "m1 = DynamicModel(in_in=3, in_out=5, out_out=2, h_ins=h_ins, h_outs=h_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicModel(\n",
      "  (in_layer): Linear(in_features=3, out_features=30, bias=True)\n",
      "  (h_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=70, bias=True)\n",
      "    (2): Linear(in_features=70, out_features=30, bias=True)\n",
      "  )\n",
      "  (out_layer): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_layer.weight torch.Size([30, 3])\n",
      "in_layer.bias torch.Size([30])\n",
      "h_layers.0.weight torch.Size([50, 30])\n",
      "h_layers.0.bias torch.Size([50])\n",
      "h_layers.1.weight torch.Size([70, 50])\n",
      "h_layers.1.bias torch.Size([70])\n",
      "h_layers.2.weight torch.Size([30, 70])\n",
      "h_layers.2.bias torch.Size([30])\n",
      "out_layer.weight torch.Size([2, 30])\n",
      "out_layer.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in m1.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 데이터 생성 \n",
    "data_ts = torch.FloatTensor([[1,3,5],[2,4,6],[3,5,7],[4,6,8]])  # 2D 4x3\n",
    "target = torch.FloatTensor([[7,9],[8,10],[9,11],[10,12]])  # 2D 4x2 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0087, -0.1601],\n",
      "        [-0.0135, -0.1568],\n",
      "        [-0.0182, -0.1557],\n",
      "        [-0.0186, -0.1629]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 \n",
    "pre_y = m1(data_ts)\n",
    "print(pre_y, pre_y.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
