{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN 기반 다중분류 모델 구현 \n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐 : 4개 sepal_length, sepal_width, petal_length, petal_width\n",
    "- 타겟/라벨 : 1개 variety \n",
    "- 학습 방법 : 지도학습 - 분류 \n",
    "- 알고리즘 : 인공신경망(ANN) -> MLP(Multi Layer Perceptron), DNN ( ) : 은닉층이 많은 구성 \n",
    "- 프레임 워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 모듈 로딩 및 데이터 준비\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 성능지표 \n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 처리 및 시각화  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import * \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩 \n",
    "DATA_FILE = '../DATA/iris.csv'\n",
    "\n",
    "# csv => DF\n",
    "iris_df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 변경 => 정수화, 클래스 3개 \n",
    "iris_df['variety'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨인코더 또는 zip 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels => {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 정수화 \n",
    "labels = dict(zip(iris_df['variety'].unique().tolist(), range(3)))\n",
    "print(f'Labels => {labels}')\n",
    "\n",
    "iris_df['variety'] = iris_df['variety'].replace(labels)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 모델 클래스 설계 및 정의 \n",
    "- 클래스 목적 : iris 데이터를 학습 및 추론 목적 \n",
    "- 클래스 이름 : iris_mcf_model\n",
    "- 부모 클래스 : nn.Module \n",
    "- 매개 변수 : 층 별 입출력 개수 고정하기 때문에 필요 x \n",
    "- 속성 / 필드 : features_df, target_df, n_rows, n_features (df만들 때 사용)\n",
    "- 기능 / 역할 : __init__() : 모델 구조 생성 , forward : 순방향 학습 (오버라이딩(overriding조건 : 상속관계에서만 가능))\n",
    "- 클래스 구조 \n",
    "    - 입력층 : 입력 4개(피처)        출력 10개(퍼셉트론/뉴런 10개 존재)\n",
    "    - 은닉층 : 입력 10개            출력 5개 (퍼셉트론/뉴런 30개 존재)\n",
    "    - 출력층 : 입력 5개             출력 3개 (퍼셉트론/뉴런 3개 존재 : 다중분류)\n",
    "\n",
    "- 활성화 함수 \n",
    "    - 클래스 형태 ==> nn.MSELoss , nn.ReLU => _ _init_ _() 메서드 \n",
    "    - 함수 형태 => torch.nn.functional 아래에 => forward() 메서드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_mcf_model(nn.Module):\n",
    "    \n",
    "    # 모델 구조 구성 및 인스턴스 생성 및 메서드 \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모델 구조 구성 \n",
    "        self.in_layer = nn.Linear(4,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.out_layer = nn.Linear(5,3) # 다중분류라서 3개 (꽃 종류 3개)\n",
    "\n",
    "    # 순방향 학습 진행 메서드 \n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # 입력층 \n",
    "        y = self.in_layer(input_data) # f11w11 + f12w12 + f13w13 + b, ......., f101w101 + f102w102 + f103w103 + b\n",
    "        y = F.relu(y)             # relu => y 값의 범위 : 0 <= y \n",
    "\n",
    "        # 은닉층 : 10개의 숫자 값(>=0)\n",
    "        y = self.hidden_layer(y)  # f21w11 + f22w12 .... + f210w210 + b, ......., f230w201 + f230w202 ..... f230w210 + b\n",
    "        # 데이터 1개 기준\n",
    "        y = F.relu(y) \n",
    "\n",
    "        # 출력층 : 5개 숫자 값(>=0) => 다중분류 : 손실함수 CrossEntropyLoss 내부에서 softmax 진행 \n",
    "        # self.out_layer(y)         # f31w31 + ...... f330w330 + b\n",
    "        # 회귀라서 활성함수 사용 x -> 바로 return \n",
    "\n",
    "        return self.out_layer(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_mcf_model(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성 \n",
    "model = iris_mcf_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "iris_mcf_model                           [50000000, 3]             --\n",
       "├─Linear: 1-1                            [50000000, 10]            50\n",
       "├─Linear: 1-2                            [50000000, 5]             55\n",
       "├─Linear: 1-3                            [50000000, 3]             18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.15\n",
       "==========================================================================================\n",
       "Input size (MB): 800.00\n",
       "Forward/backward pass size (MB): 7200.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 8000.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [테스트] 모델 사용 메모리 정보 확인 \n",
    "\n",
    "summary(model, input_size=(50000000,4)) \n",
    "# 메모리 차지 용량 계산만 함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 데이터셋 클래스 설계 및 정의\n",
    "- 데이터셋 : iris.csv \n",
    "- 피쳐 개수 : 4개 \n",
    "- 타겟 개수 : 1개 \n",
    "- 클래스 이름 : iris_data_set\n",
    "- 부모 클래스 : utils.data.Dataset \n",
    "- 속성 / 필드 : feature_df, target_df \n",
    "- 필수 메서드 \n",
    "    - _ _init_ _(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정 \n",
    "    - _ _len_ _(self) : 데이터의 개수 반환 \n",
    "    - _ _ getItem_ _(self, index) : 특정 인덱스의 피쳐와 타겟 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_data_set(Dataset):\n",
    "    def __init__(self, feature_df, target_df):\n",
    "        self.feature_df = feature_df\n",
    "        self.target_df = target_df\n",
    "        self.n_rows = feature_df.shape[0]\n",
    "        self.n_features = feature_df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        # 텐서화 \n",
    "        feature_ts = torch.FloatTensor(self.feature_df.iloc[index].values) # 시리즈라서 values() 사용해서 numpy -> tensor \n",
    "        target_ts = torch.FloatTensor(self.target_df.iloc[index].values)\n",
    "                \n",
    "        # 피쳐와 타겟 반환 \n",
    "        return feature_ts, target_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 데이터셋 인스턴스 생성 \n",
    "\n",
    "# - DF 에서 피쳐와 타겟 추출 \n",
    "feature_df = iris_df[iris_df.columns[:-1]] # 2D \n",
    "target_df = iris_df[iris_df.columns[-1:]]  # 2D \n",
    "\n",
    "# 커스텀 데이터셋 인스턴스 생성 \n",
    "iris_ds = iris_data_set(feature_df, target_df)\n",
    "\n",
    "# 데이터로더 인스턴스 생성 \n",
    "iris_dl = DataLoader(iris_ds)\n",
    "for feature, label in iris_dl:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] 학습 준비 \n",
    "- 학습 횟수 : EPOCH (처음부터 끝까지 학습할 단위)\n",
    "- 배치 크기 : BATCH_SIZE (한 번에 학습할 데이터셋 양)\n",
    "- 위치 지정 : DEVICE (텐서 저장 및 실행 위치 (GPU, CPU))\n",
    "- 학습률 : 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001 ~ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정 \n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10 \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001 # hyper-parameter : 업데이트 간격  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스 / 객체 : 모델, 데이터셋, 최적화, 손실함수 ,(성능지표)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 \n",
    "model = iris_mcf_model()\n",
    "\n",
    "# 데이터셋 인스턴스 \n",
    "iris_ds = iris_data_set(feature_df, target_df)\n",
    "\n",
    "# 데이터로더 인스턴스 \n",
    "iris_dl = DataLoader(iris_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# DS과 DL 인스턴스 \n",
    "# - 학습용, 검증용, 테스트용 데이터 분리 \n",
    "\n",
    "# 데이터셋 인스턴스 \n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_df, target_df, random_state = 1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state = 1)\n",
    "print(f'{x_train.shape} {x_test.shape} {x_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "print(f'{type(x_train)} {type(x_test)} {type(x_val)}')\n",
    "# iris_ds = iris_data_set(x_train, y_train)\n",
    "\n",
    "# 학습용, 검증용 테스트용 데이터셋 \n",
    "train_ds = iris_data_set(x_train, y_train)\n",
    "val_ds = iris_data_set(x_val, y_val)\n",
    "test_ds = iris_data_set(x_test, y_test)\n",
    "\n",
    "# 학습용 데이터로더 인스턴스 \n",
    "# iris_dl = DataLoader(iris_ds, batch_size=BATCH_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => w, b 텐서 즉, model.parameters() 전달 \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 다중분류 CrossEntropyLoss \n",
    "#                            예측값은 확률값으로 전달 => AF 처리 x  \n",
    "cross_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실, 성능평가 코드 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT : 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1000]\n",
      "- Train Loss : 1.132787134912279 Score : 0.16442138453324637\n",
      "- Val Loss : 1.2544275522232056 Score : 0.10101010650396347\n",
      "[1/1000]\n",
      "- Train Loss : 1.1071438656912909 Score : 0.16442138453324637\n",
      "- Val Loss : 1.2173995971679688 Score : 0.10101010650396347\n",
      "[2/1000]\n",
      "- Train Loss : 1.0862824850612216 Score : 0.16442138453324637\n",
      "- Val Loss : 1.1853256225585938 Score : 0.10101010650396347\n",
      "[3/1000]\n",
      "- Train Loss : 1.066539274321662 Score : 0.16442138453324637\n",
      "- Val Loss : 1.1561836004257202 Score : 0.10101010650396347\n",
      "[4/1000]\n",
      "- Train Loss : 1.046477152241601 Score : 0.16442138453324637\n",
      "- Val Loss : 1.128930926322937 Score : 0.10101010650396347\n",
      "[5/1000]\n",
      "- Train Loss : 1.0280090437995062 Score : 0.16442138453324637\n",
      "- Val Loss : 1.1017404794692993 Score : 0.10101010650396347\n",
      "[6/1000]\n",
      "- Train Loss : 1.0100238985485501 Score : 0.16442138453324637\n",
      "- Val Loss : 1.0767072439193726 Score : 0.10101010650396347\n",
      "[7/1000]\n",
      "- Train Loss : 0.9944525361061096 Score : 0.16442138453324637\n",
      "- Val Loss : 1.0561965703964233 Score : 0.10101010650396347\n",
      "[8/1000]\n",
      "- Train Loss : 0.9794709748691983 Score : 0.16442138453324637\n",
      "- Val Loss : 1.0358067750930786 Score : 0.10101010650396347\n",
      "[9/1000]\n",
      "- Train Loss : 0.9640195965766907 Score : 0.16442138453324637\n",
      "- Val Loss : 1.0154943466186523 Score : 0.10101010650396347\n",
      "[10/1000]\n",
      "- Train Loss : 0.948492705821991 Score : 0.16442138453324637\n",
      "- Val Loss : 0.9955617785453796 Score : 0.10101010650396347\n",
      "[11/1000]\n",
      "- Train Loss : 0.9330657190746732 Score : 0.16442138453324637\n",
      "- Val Loss : 0.975683331489563 Score : 0.10101010650396347\n",
      "[12/1000]\n",
      "- Train Loss : 0.9175292584631178 Score : 0.16442138453324637\n",
      "- Val Loss : 0.9558545351028442 Score : 0.10101010650396347\n",
      "[13/1000]\n",
      "- Train Loss : 0.9022742311159769 Score : 0.2003015536401007\n",
      "- Val Loss : 0.9368484616279602 Score : 0.2539682686328888\n",
      "[14/1000]\n",
      "- Train Loss : 0.887614541583591 Score : 0.44510922498173183\n",
      "- Val Loss : 0.9187412261962891 Score : 0.4388888478279114\n",
      "[15/1000]\n",
      "- Train Loss : 0.8735939727889167 Score : 0.5080754690700107\n",
      "- Val Loss : 0.9016643762588501 Score : 0.4848484992980957\n",
      "[16/1000]\n",
      "- Train Loss : 0.8600362804200914 Score : 0.5282614628473917\n",
      "- Val Loss : 0.8855628967285156 Score : 0.4458247423171997\n",
      "[17/1000]\n",
      "- Train Loss : 0.8472905225223966 Score : 0.49337248338593376\n",
      "- Val Loss : 0.8703694939613342 Score : 0.4055555462837219\n",
      "[18/1000]\n",
      "- Train Loss : 0.8351676530308194 Score : 0.41954220334688824\n",
      "- Val Loss : 0.8561747670173645 Score : 0.36350876092910767\n",
      "[19/1000]\n",
      "- Train Loss : 0.823694732454088 Score : 0.304826052652465\n",
      "- Val Loss : 0.842991054058075 Score : 0.36350876092910767\n",
      "[20/1000]\n",
      "- Train Loss : 0.8127756913503011 Score : 0.4952140649159749\n",
      "- Val Loss : 0.8305143117904663 Score : 0.7445117831230164\n",
      "[21/1000]\n",
      "- Train Loss : 0.8024342060089111 Score : 0.5859788556893667\n",
      "- Val Loss : 0.8186501860618591 Score : 0.6684149503707886\n",
      "[22/1000]\n",
      "- Train Loss : 0.7925623059272766 Score : 0.5859788556893667\n",
      "- Val Loss : 0.8072105050086975 Score : 0.6684149503707886\n",
      "[23/1000]\n",
      "- Train Loss : 0.7836073769463433 Score : 0.5722623301876916\n",
      "- Val Loss : 0.7968578934669495 Score : 0.6684149503707886\n",
      "[24/1000]\n",
      "- Train Loss : 0.7748237517144945 Score : 0.5929854313532511\n",
      "- Val Loss : 0.7870528101921082 Score : 0.6961299777030945\n",
      "[25/1000]\n",
      "- Train Loss : 0.7663856877221001 Score : 0.5929854313532511\n",
      "- Val Loss : 0.7776578664779663 Score : 0.6961299777030945\n",
      "[26/1000]\n",
      "- Train Loss : 0.7582531438933479 Score : 0.6005986001756456\n",
      "- Val Loss : 0.7691023945808411 Score : 0.6961299777030945\n",
      "[27/1000]\n",
      "- Train Loss : 0.7503177457385592 Score : 0.6315108935038248\n",
      "- Val Loss : 0.7607613205909729 Score : 0.6961299777030945\n",
      "[28/1000]\n",
      "- Train Loss : 0.7422760261429681 Score : 0.6315108935038248\n",
      "- Val Loss : 0.7520542740821838 Score : 0.7788405418395996\n",
      "[29/1000]\n",
      "- Train Loss : 0.7345792651176453 Score : 0.6560258865356445\n",
      "- Val Loss : 0.7439728379249573 Score : 0.7788405418395996\n",
      "[30/1000]\n",
      "- Train Loss : 0.7270348601871066 Score : 0.7326091792848375\n",
      "- Val Loss : 0.7365133166313171 Score : 0.7788405418395996\n",
      "[31/1000]\n",
      "- Train Loss : 0.7193344036738077 Score : 0.7326091792848375\n",
      "- Val Loss : 0.7286152839660645 Score : 0.8171428442001343\n",
      "[32/1000]\n",
      "- Train Loss : 0.7117133273018731 Score : 0.7538213001357185\n",
      "- Val Loss : 0.720903217792511 Score : 0.888888955116272\n",
      "[33/1000]\n",
      "- Train Loss : 0.7042835619714525 Score : 0.7842739754252963\n",
      "- Val Loss : 0.7135427594184875 Score : 0.888888955116272\n",
      "[34/1000]\n",
      "- Train Loss : 0.6966458559036255 Score : 0.8139623867140876\n",
      "- Val Loss : 0.7059845924377441 Score : 0.888888955116272\n",
      "[35/1000]\n",
      "- Train Loss : 0.6890820728407966 Score : 0.8254262341393365\n",
      "- Val Loss : 0.6982707977294922 Score : 0.888888955116272\n",
      "[36/1000]\n",
      "- Train Loss : 0.6816178692711724 Score : 0.8254262341393365\n",
      "- Val Loss : 0.6907146573066711 Score : 0.888888955116272\n",
      "[37/1000]\n",
      "- Train Loss : 0.6741468773947822 Score : 0.8534097737736173\n",
      "- Val Loss : 0.683802604675293 Score : 0.888888955116272\n",
      "[38/1000]\n",
      "- Train Loss : 0.66649948226081 Score : 0.8618459833992852\n",
      "- Val Loss : 0.6760567426681519 Score : 0.888888955116272\n",
      "[39/1000]\n",
      "- Train Loss : 0.6590451598167419 Score : 0.8880975113974677\n",
      "- Val Loss : 0.6692384481430054 Score : 0.888888955116272\n",
      "[40/1000]\n",
      "- Train Loss : 0.6512429449293349 Score : 0.9127888679504395\n",
      "- Val Loss : 0.6613691449165344 Score : 0.888888955116272\n",
      "[41/1000]\n",
      "- Train Loss : 0.6437980863783095 Score : 0.9127888679504395\n",
      "- Val Loss : 0.6550039649009705 Score : 0.9484702348709106\n",
      "[42/1000]\n",
      "- Train Loss : 0.6357517242431641 Score : 0.9333489603466458\n",
      "- Val Loss : 0.6468514800071716 Score : 0.9484702348709106\n",
      "[43/1000]\n",
      "- Train Loss : 0.628205762969123 Score : 0.9333489603466458\n",
      "- Val Loss : 0.6393190622329712 Score : 0.9484702348709106\n",
      "[44/1000]\n",
      "- Train Loss : 0.6206475098927816 Score : 0.9333489603466458\n",
      "- Val Loss : 0.6320592164993286 Score : 0.9484702348709106\n",
      "[45/1000]\n",
      "- Train Loss : 0.6129644181993272 Score : 0.9490616454018487\n",
      "- Val Loss : 0.6244540810585022 Score : 0.9484702348709106\n",
      "[46/1000]\n",
      "- Train Loss : 0.6054596106211344 Score : 0.9490616454018487\n",
      "- Val Loss : 0.6175462603569031 Score : 0.9484702348709106\n",
      "[47/1000]\n",
      "- Train Loss : 0.5975444581773546 Score : 0.9593180550469292\n",
      "- Val Loss : 0.6094947457313538 Score : 0.9484702348709106\n",
      "[48/1000]\n",
      "- Train Loss : 0.5902750823232863 Score : 0.9593180550469292\n",
      "- Val Loss : 0.603035032749176 Score : 0.9484702348709106\n",
      "[49/1000]\n",
      "- Train Loss : 0.582428471909629 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5961476564407349 Score : 0.9484702348709106\n",
      "[50/1000]\n",
      "- Train Loss : 0.5747065908379025 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5886694192886353 Score : 0.9484702348709106\n",
      "[51/1000]\n",
      "- Train Loss : 0.5670518378416697 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5811133980751038 Score : 0.9484702348709106\n",
      "[52/1000]\n",
      "- Train Loss : 0.5595367716418372 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5744665861129761 Score : 0.9484702348709106\n",
      "[53/1000]\n",
      "- Train Loss : 0.5517361296547784 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5667441487312317 Score : 0.9484702348709106\n",
      "[54/1000]\n",
      "- Train Loss : 0.5443114274077945 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5598760843276978 Score : 0.9484702348709106\n",
      "[55/1000]\n",
      "- Train Loss : 0.5367770923508538 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5531417727470398 Score : 0.9484702348709106\n",
      "[56/1000]\n",
      "- Train Loss : 0.5290182994471656 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5457062125205994 Score : 0.9484702348709106\n",
      "[57/1000]\n",
      "- Train Loss : 0.5216933853096433 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5387880802154541 Score : 0.9484702348709106\n",
      "[58/1000]\n",
      "- Train Loss : 0.5142873856756423 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5320581793785095 Score : 0.9484702348709106\n",
      "[59/1000]\n",
      "- Train Loss : 0.5070753362443712 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5256796479225159 Score : 0.9484702348709106\n",
      "[60/1000]\n",
      "- Train Loss : 0.4995044238037533 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5185360908508301 Score : 0.9484702348709106\n",
      "[61/1000]\n",
      "- Train Loss : 0.4924068848292033 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5117154717445374 Score : 0.9484702348709106\n",
      "[62/1000]\n",
      "- Train Loss : 0.48536285758018494 Score : 0.9593180550469292\n",
      "- Val Loss : 0.5051594972610474 Score : 0.9484702348709106\n",
      "[63/1000]\n",
      "- Train Loss : 0.4783710108862983 Score : 0.9593180550469292\n",
      "- Val Loss : 0.49871090054512024 Score : 0.9484702348709106\n",
      "[64/1000]\n",
      "- Train Loss : 0.4713756839434306 Score : 0.9593180550469292\n",
      "- Val Loss : 0.49214839935302734 Score : 0.9484702348709106\n",
      "[65/1000]\n",
      "- Train Loss : 0.4647054639127519 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4860967695713043 Score : 0.9484702348709106\n",
      "[66/1000]\n",
      "- Train Loss : 0.45774171418613857 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4794369339942932 Score : 0.9484702348709106\n",
      "[67/1000]\n",
      "- Train Loss : 0.4512324564986759 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4730471074581146 Score : 0.9484702348709106\n",
      "[68/1000]\n",
      "- Train Loss : 0.4448724455303616 Score : 0.9593180550469292\n",
      "- Val Loss : 0.46702155470848083 Score : 0.9484702348709106\n",
      "[69/1000]\n",
      "- Train Loss : 0.4385800626542833 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4614884853363037 Score : 0.9484702348709106\n",
      "[70/1000]\n",
      "- Train Loss : 0.4320940872033437 Score : 0.9593180550469292\n",
      "- Val Loss : 0.45530715584754944 Score : 0.9484702348709106\n",
      "[71/1000]\n",
      "- Train Loss : 0.42599465780788 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4492602050304413 Score : 0.9484702348709106\n",
      "[72/1000]\n",
      "- Train Loss : 0.42009511921140885 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4435814321041107 Score : 0.9484702348709106\n",
      "[73/1000]\n",
      "- Train Loss : 0.41427238119973075 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4382213056087494 Score : 0.9484702348709106\n",
      "[74/1000]\n",
      "- Train Loss : 0.4085698160860274 Score : 0.9593180550469292\n",
      "- Val Loss : 0.43290218710899353 Score : 0.9484702348709106\n",
      "[75/1000]\n",
      "- Train Loss : 0.4027815924750434 Score : 0.9593180550469292\n",
      "- Val Loss : 0.42748144268989563 Score : 0.9484702348709106\n",
      "[76/1000]\n",
      "- Train Loss : 0.39753882090250653 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4224998950958252 Score : 0.9484702348709106\n",
      "[77/1000]\n",
      "- Train Loss : 0.3919492761294047 Score : 0.9593180550469292\n",
      "- Val Loss : 0.4170909821987152 Score : 0.9484702348709106\n",
      "[78/1000]\n",
      "- Train Loss : 0.3868086536725362 Score : 0.9593180550469292\n",
      "- Val Loss : 0.41188034415245056 Score : 0.9484702348709106\n",
      "[79/1000]\n",
      "- Train Loss : 0.3818349175983005 Score : 0.9593180550469292\n",
      "- Val Loss : 0.40693989396095276 Score : 0.9484702348709106\n",
      "[80/1000]\n",
      "- Train Loss : 0.37680212325519985 Score : 0.9593180550469292\n",
      "- Val Loss : 0.40208813548088074 Score : 0.9484702348709106\n",
      "[81/1000]\n",
      "- Train Loss : 0.3720123420159022 Score : 0.9593180550469292\n",
      "- Val Loss : 0.39730358123779297 Score : 0.9484702348709106\n",
      "[82/1000]\n",
      "- Train Loss : 0.3672896905077828 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3926359713077545 Score : 0.9484702348709106\n",
      "[83/1000]\n",
      "- Train Loss : 0.3626454720894496 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3880782723426819 Score : 0.9484702348709106\n",
      "[84/1000]\n",
      "- Train Loss : 0.35824038419458604 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3836504817008972 Score : 0.9484702348709106\n",
      "[85/1000]\n",
      "- Train Loss : 0.3538094229168362 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3792226314544678 Score : 0.9484702348709106\n",
      "[86/1000]\n",
      "- Train Loss : 0.34959689113828873 Score : 0.9593180550469292\n",
      "- Val Loss : 0.37499862909317017 Score : 0.9484702348709106\n",
      "[87/1000]\n",
      "- Train Loss : 0.3454483184549544 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3708786964416504 Score : 0.9484702348709106\n",
      "[88/1000]\n",
      "- Train Loss : 0.34131480422284866 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3667035400867462 Score : 0.9484702348709106\n",
      "[89/1000]\n",
      "- Train Loss : 0.33734645942846936 Score : 0.9593180550469292\n",
      "- Val Loss : 0.36263567209243774 Score : 0.9484702348709106\n",
      "[90/1000]\n",
      "- Train Loss : 0.3334447542826335 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3586772382259369 Score : 0.9484702348709106\n",
      "[91/1000]\n",
      "- Train Loss : 0.3296856813960605 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3549174964427948 Score : 0.9484702348709106\n",
      "[92/1000]\n",
      "- Train Loss : 0.3260956754287084 Score : 0.9593180550469292\n",
      "- Val Loss : 0.35130995512008667 Score : 0.9484702348709106\n",
      "[93/1000]\n",
      "- Train Loss : 0.3224412318733003 Score : 0.9593180550469292\n",
      "- Val Loss : 0.34754228591918945 Score : 0.9484702348709106\n",
      "[94/1000]\n",
      "- Train Loss : 0.31891432570086586 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3439501225948334 Score : 0.9484702348709106\n",
      "[95/1000]\n",
      "- Train Loss : 0.31548113458686405 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3404386639595032 Score : 0.9484702348709106\n",
      "[96/1000]\n",
      "- Train Loss : 0.31223131219546 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3370724022388458 Score : 0.9484702348709106\n",
      "[97/1000]\n",
      "- Train Loss : 0.3088940034310023 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3336116671562195 Score : 0.9484702348709106\n",
      "[98/1000]\n",
      "- Train Loss : 0.30572249160872567 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3302602767944336 Score : 0.9484702348709106\n",
      "[99/1000]\n",
      "- Train Loss : 0.30261164903640747 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3270159065723419 Score : 0.9484702348709106\n",
      "[100/1000]\n",
      "- Train Loss : 0.2995036260949241 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3236847221851349 Score : 0.9484702348709106\n",
      "[101/1000]\n",
      "- Train Loss : 0.296622355779012 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3206040561199188 Score : 0.9484702348709106\n",
      "[102/1000]\n",
      "- Train Loss : 0.29367095397578347 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3175073564052582 Score : 0.9484702348709106\n",
      "[103/1000]\n",
      "- Train Loss : 0.2908286352952321 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3144104778766632 Score : 0.9484702348709106\n",
      "[104/1000]\n",
      "- Train Loss : 0.28810518483320874 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3115150034427643 Score : 0.9484702348709106\n",
      "[105/1000]\n",
      "- Train Loss : 0.28534368839528823 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3085622489452362 Score : 0.9484702348709106\n",
      "[106/1000]\n",
      "- Train Loss : 0.2826882418658998 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3056339621543884 Score : 0.9484702348709106\n",
      "[107/1000]\n",
      "- Train Loss : 0.2801116754611333 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3028244972229004 Score : 0.9484702348709106\n",
      "[108/1000]\n",
      "- Train Loss : 0.2775883790519502 Score : 0.9593180550469292\n",
      "- Val Loss : 0.3000839650630951 Score : 0.9484702348709106\n",
      "[109/1000]\n",
      "- Train Loss : 0.2751317206356261 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2973821759223938 Score : 0.9484702348709106\n",
      "[110/1000]\n",
      "- Train Loss : 0.2726517419020335 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2946898937225342 Score : 0.9484702348709106\n",
      "[111/1000]\n",
      "- Train Loss : 0.27034082512060803 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2920653522014618 Score : 0.9484702348709106\n",
      "[112/1000]\n",
      "- Train Loss : 0.2679472731219398 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2894293963909149 Score : 0.9484702348709106\n",
      "[113/1000]\n",
      "- Train Loss : 0.26572038729985553 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2869426906108856 Score : 0.9484702348709106\n",
      "[114/1000]\n",
      "- Train Loss : 0.26350513183408314 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2844376862049103 Score : 0.9484702348709106\n",
      "[115/1000]\n",
      "- Train Loss : 0.2612181587351693 Score : 0.9593180550469292\n",
      "- Val Loss : 0.28190919756889343 Score : 0.9484702348709106\n",
      "[116/1000]\n",
      "- Train Loss : 0.2591774927245246 Score : 0.9593180550469292\n",
      "- Val Loss : 0.27951422333717346 Score : 0.9484702348709106\n",
      "[117/1000]\n",
      "- Train Loss : 0.2570853787991736 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2771935760974884 Score : 0.9484702348709106\n",
      "[118/1000]\n",
      "- Train Loss : 0.2549828406837251 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2748357057571411 Score : 0.9484702348709106\n",
      "[119/1000]\n",
      "- Train Loss : 0.2529957468310992 Score : 0.9593180550469292\n",
      "- Val Loss : 0.27252334356307983 Score : 0.9484702348709106\n",
      "[120/1000]\n",
      "- Train Loss : 0.2510361141628689 Score : 0.9593180550469292\n",
      "- Val Loss : 0.27023452520370483 Score : 0.9484702348709106\n",
      "[121/1000]\n",
      "- Train Loss : 0.24903780221939087 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2680153548717499 Score : 0.9484702348709106\n",
      "[122/1000]\n",
      "- Train Loss : 0.24726499120394388 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2658326327800751 Score : 0.9484702348709106\n",
      "[123/1000]\n",
      "- Train Loss : 0.245396774676111 Score : 0.9593180550469292\n",
      "- Val Loss : 0.26362210512161255 Score : 0.9484702348709106\n",
      "[124/1000]\n",
      "- Train Loss : 0.24347133934497833 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2614012360572815 Score : 0.9484702348709106\n",
      "[125/1000]\n",
      "- Train Loss : 0.24169797698656717 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2593849301338196 Score : 0.9484702348709106\n",
      "[126/1000]\n",
      "- Train Loss : 0.2400590048895942 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2573952078819275 Score : 0.9484702348709106\n",
      "[127/1000]\n",
      "- Train Loss : 0.23827435821294785 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2552742063999176 Score : 0.9484702348709106\n",
      "[128/1000]\n",
      "- Train Loss : 0.23651501950290468 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2532372772693634 Score : 0.9484702348709106\n",
      "[129/1000]\n",
      "- Train Loss : 0.23498189118173388 Score : 0.9593180550469292\n",
      "- Val Loss : 0.25135043263435364 Score : 0.9484702348709106\n",
      "[130/1000]\n",
      "- Train Loss : 0.23325737648540074 Score : 0.9593180550469292\n",
      "- Val Loss : 0.24937854707241058 Score : 0.9484702348709106\n",
      "[131/1000]\n",
      "- Train Loss : 0.2317153943909539 Score : 0.9593180550469292\n",
      "- Val Loss : 0.24743321537971497 Score : 0.9484702348709106\n",
      "[132/1000]\n",
      "- Train Loss : 0.23012558867534003 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2455284744501114 Score : 0.9484702348709106\n",
      "[133/1000]\n",
      "- Train Loss : 0.22856979568799338 Score : 0.9593180550469292\n",
      "- Val Loss : 0.24368584156036377 Score : 0.9484702348709106\n",
      "[134/1000]\n",
      "- Train Loss : 0.2270897982849015 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2418781965970993 Score : 0.9484702348709106\n",
      "[135/1000]\n",
      "- Train Loss : 0.2255139864153332 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2400505542755127 Score : 0.9484702348709106\n",
      "[136/1000]\n",
      "- Train Loss : 0.22413995116949081 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2383025735616684 Score : 0.9484702348709106\n",
      "[137/1000]\n",
      "- Train Loss : 0.22271115498410332 Score : 0.9593180550469292\n",
      "- Val Loss : 0.23656438291072845 Score : 0.9484702348709106\n",
      "[138/1000]\n",
      "- Train Loss : 0.22118730511930254 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2347710132598877 Score : 0.9484702348709106\n",
      "[139/1000]\n",
      "- Train Loss : 0.21988331526517868 Score : 0.9593180550469292\n",
      "- Val Loss : 0.23308299481868744 Score : 0.9484702348709106\n",
      "[140/1000]\n",
      "- Train Loss : 0.2184637056456672 Score : 0.9593180550469292\n",
      "- Val Loss : 0.23141585290431976 Score : 0.9484702348709106\n",
      "[141/1000]\n",
      "- Train Loss : 0.21715590523348915 Score : 0.9593180550469292\n",
      "- Val Loss : 0.22976568341255188 Score : 0.9484702348709106\n",
      "[142/1000]\n",
      "- Train Loss : 0.2158158371845881 Score : 0.9593180550469292\n",
      "- Val Loss : 0.22810755670070648 Score : 0.9484702348709106\n",
      "[143/1000]\n",
      "- Train Loss : 0.21445990105470022 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2264913022518158 Score : 0.9484702348709106\n",
      "[144/1000]\n",
      "- Train Loss : 0.2132433926065763 Score : 0.9593180550469292\n",
      "- Val Loss : 0.22492502629756927 Score : 0.9484702348709106\n",
      "[145/1000]\n",
      "- Train Loss : 0.21194846431414285 Score : 0.9593180550469292\n",
      "- Val Loss : 0.22332438826560974 Score : 0.9484702348709106\n",
      "[146/1000]\n",
      "- Train Loss : 0.21071314232216942 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2217671126127243 Score : 0.9484702348709106\n",
      "[147/1000]\n",
      "- Train Loss : 0.209497955110338 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2202487736940384 Score : 0.9484702348709106\n",
      "[148/1000]\n",
      "- Train Loss : 0.2082074276275105 Score : 0.9593180550469292\n",
      "- Val Loss : 0.21871502697467804 Score : 0.9484702348709106\n",
      "[149/1000]\n",
      "- Train Loss : 0.20711756745974222 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2172827571630478 Score : 0.9484702348709106\n",
      "[150/1000]\n",
      "- Train Loss : 0.205962383084827 Score : 0.9593180550469292\n",
      "- Val Loss : 0.21586549282073975 Score : 0.9484702348709106\n",
      "[151/1000]\n",
      "- Train Loss : 0.2046910665101475 Score : 0.9593180550469292\n",
      "- Val Loss : 0.21433457732200623 Score : 0.9484702348709106\n",
      "[152/1000]\n",
      "- Train Loss : 0.20360949552721447 Score : 0.9593180550469292\n",
      "- Val Loss : 0.21287260949611664 Score : 0.9484702348709106\n",
      "[153/1000]\n",
      "- Train Loss : 0.20252253611882529 Score : 0.9593180550469292\n",
      "- Val Loss : 0.21146762371063232 Score : 0.9484702348709106\n",
      "[154/1000]\n",
      "- Train Loss : 0.2014041385716862 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2100800722837448 Score : 0.9484702348709106\n",
      "[155/1000]\n",
      "- Train Loss : 0.20030692550871107 Score : 0.9593180550469292\n",
      "- Val Loss : 0.20870651304721832 Score : 0.9484702348709106\n",
      "[156/1000]\n",
      "- Train Loss : 0.19924674100346035 Score : 0.9593180550469292\n",
      "- Val Loss : 0.20736972987651825 Score : 0.9484702348709106\n",
      "[157/1000]\n",
      "- Train Loss : 0.19815005527602303 Score : 0.9593180550469292\n",
      "- Val Loss : 0.20598866045475006 Score : 0.9484702348709106\n",
      "[158/1000]\n",
      "- Train Loss : 0.19716694123215145 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2046833485364914 Score : 0.9484702348709106\n",
      "[159/1000]\n",
      "- Train Loss : 0.19609134064780342 Score : 0.9593180550469292\n",
      "- Val Loss : 0.2033577710390091 Score : 0.9484702348709106\n",
      "[160/1000]\n",
      "- Train Loss : 0.19509352578057182 Score : 0.9593180550469292\n",
      "- Val Loss : 0.20205101370811462 Score : 0.9484702348709106\n",
      "[161/1000]\n",
      "- Train Loss : 0.19411133809222114 Score : 0.9593180550469292\n",
      "- Val Loss : 0.20077325403690338 Score : 0.9484702348709106\n",
      "[162/1000]\n",
      "- Train Loss : 0.19311430305242538 Score : 0.9593180550469292\n",
      "- Val Loss : 0.19952614605426788 Score : 0.9484702348709106\n",
      "[163/1000]\n",
      "- Train Loss : 0.1921699316137367 Score : 0.9593180550469292\n",
      "- Val Loss : 0.19831107556819916 Score : 0.9484702348709106\n",
      "[164/1000]\n",
      "- Train Loss : 0.1911709966758887 Score : 0.9593180550469292\n",
      "- Val Loss : 0.19704881310462952 Score : 0.9484702348709106\n",
      "[165/1000]\n",
      "- Train Loss : 0.1902199395828777 Score : 0.9593180550469292\n",
      "- Val Loss : 0.19581659138202667 Score : 0.9484702348709106\n",
      "[166/1000]\n",
      "- Train Loss : 0.1893372531566355 Score : 0.9593180550469292\n",
      "- Val Loss : 0.19464139640331268 Score : 0.9484702348709106\n",
      "[167/1000]\n",
      "- Train Loss : 0.18838314587871233 Score : 0.9593180550469292\n",
      "- Val Loss : 0.19343599677085876 Score : 0.9484702348709106\n",
      "[168/1000]\n",
      "- Train Loss : 0.18746039726667935 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1922546923160553 Score : 0.9484702348709106\n",
      "[169/1000]\n",
      "- Train Loss : 0.1866091729866134 Score : 0.9346266984939575\n",
      "- Val Loss : 0.19111967086791992 Score : 0.9484702348709106\n",
      "[170/1000]\n",
      "- Train Loss : 0.18569541143046486 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1899535208940506 Score : 0.9484702348709106\n",
      "[171/1000]\n",
      "- Train Loss : 0.18480748890174759 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18881122767925262 Score : 0.9484702348709106\n",
      "[172/1000]\n",
      "- Train Loss : 0.18399049383070734 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1877153068780899 Score : 0.9484702348709106\n",
      "[173/1000]\n",
      "- Train Loss : 0.18311012867424223 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18659356236457825 Score : 0.9484702348709106\n",
      "[174/1000]\n",
      "- Train Loss : 0.18226658097571796 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1854792684316635 Score : 0.9484702348709106\n",
      "[175/1000]\n",
      "- Train Loss : 0.1814456366830402 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18440163135528564 Score : 0.9484702348709106\n",
      "[176/1000]\n",
      "- Train Loss : 0.18065200787451532 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18336151540279388 Score : 0.9484702348709106\n",
      "[177/1000]\n",
      "- Train Loss : 0.17981072515249252 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18228758871555328 Score : 0.9484702348709106\n",
      "[178/1000]\n",
      "- Train Loss : 0.17900529131293297 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18123053014278412 Score : 0.9484702348709106\n",
      "[179/1000]\n",
      "- Train Loss : 0.17825235550602278 Score : 0.9346266984939575\n",
      "- Val Loss : 0.18021847307682037 Score : 0.9484702348709106\n",
      "[180/1000]\n",
      "- Train Loss : 0.1774502011636893 Score : 0.9346266984939575\n",
      "- Val Loss : 0.179186150431633 Score : 0.9484702348709106\n",
      "[181/1000]\n",
      "- Train Loss : 0.1766678566733996 Score : 0.9346266984939575\n",
      "- Val Loss : 0.17816917598247528 Score : 0.9484702348709106\n",
      "[182/1000]\n",
      "- Train Loss : 0.17594795591301388 Score : 0.9346266984939575\n",
      "- Val Loss : 0.17718875408172607 Score : 0.9484702348709106\n",
      "[183/1000]\n",
      "- Train Loss : 0.1751725897192955 Score : 0.9346266984939575\n",
      "- Val Loss : 0.17618940770626068 Score : 0.9484702348709106\n",
      "[184/1000]\n",
      "- Train Loss : 0.17442679653565088 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1751987785100937 Score : 0.9484702348709106\n",
      "[185/1000]\n",
      "- Train Loss : 0.17369691779216131 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1742367446422577 Score : 0.9484702348709106\n",
      "[186/1000]\n",
      "- Train Loss : 0.17300299886200163 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1733044683933258 Score : 0.9484702348709106\n",
      "[187/1000]\n",
      "- Train Loss : 0.17226012547810873 Score : 0.9346266984939575\n",
      "- Val Loss : 0.17234602570533752 Score : 0.9484702348709106\n",
      "[188/1000]\n",
      "- Train Loss : 0.17154827300045225 Score : 0.9346266984939575\n",
      "- Val Loss : 0.17139340937137604 Score : 0.9484702348709106\n",
      "[189/1000]\n",
      "- Train Loss : 0.17085875984695223 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1704704761505127 Score : 0.9484702348709106\n",
      "[190/1000]\n",
      "- Train Loss : 0.1701931415332688 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16957782208919525 Score : 0.9484702348709106\n",
      "[191/1000]\n",
      "- Train Loss : 0.16948548000719812 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16866159439086914 Score : 0.9484702348709106\n",
      "[192/1000]\n",
      "- Train Loss : 0.1687987587518162 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16775953769683838 Score : 0.9484702348709106\n",
      "[193/1000]\n",
      "- Train Loss : 0.16816850461893612 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16689059138298035 Score : 0.9484702348709106\n",
      "[194/1000]\n",
      "- Train Loss : 0.1674934203426043 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16600506007671356 Score : 0.9484702348709106\n",
      "[195/1000]\n",
      "- Train Loss : 0.16683895472023222 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1651252806186676 Score : 0.9484702348709106\n",
      "[196/1000]\n",
      "- Train Loss : 0.1661929732395543 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16427095234394073 Score : 0.9484702348709106\n",
      "[197/1000]\n",
      "- Train Loss : 0.1655863287548224 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16344289481639862 Score : 0.9484702348709106\n",
      "[198/1000]\n",
      "- Train Loss : 0.16493628587987688 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16259326040744781 Score : 0.9484702348709106\n",
      "[199/1000]\n",
      "- Train Loss : 0.16431027319696215 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1617472916841507 Score : 0.9484702348709106\n",
      "[200/1000]\n",
      "- Train Loss : 0.16369733048809898 Score : 0.9346266984939575\n",
      "- Val Loss : 0.16092512011528015 Score : 0.9484702348709106\n",
      "[201/1000]\n",
      "- Train Loss : 0.16311773243877623 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1601286679506302 Score : 0.9484702348709106\n",
      "[202/1000]\n",
      "- Train Loss : 0.1624952976902326 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15931563079357147 Score : 0.9484702348709106\n",
      "[203/1000]\n",
      "- Train Loss : 0.1618966952794128 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1585061401128769 Score : 0.9484702348709106\n",
      "[204/1000]\n",
      "- Train Loss : 0.16131149149603313 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15771742165088654 Score : 0.9484702348709106\n",
      "[205/1000]\n",
      "- Train Loss : 0.16075764637854364 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15695299208164215 Score : 0.9484702348709106\n",
      "[206/1000]\n",
      "- Train Loss : 0.16017090736163986 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15617439150810242 Score : 0.9484702348709106\n",
      "[207/1000]\n",
      "- Train Loss : 0.1596030236946212 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15539483726024628 Score : 0.9484702348709106\n",
      "[208/1000]\n",
      "- Train Loss : 0.1590197446445624 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1546204537153244 Score : 0.9484702348709106\n",
      "[209/1000]\n",
      "- Train Loss : 0.15850803587171766 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1538890302181244 Score : 0.9484702348709106\n",
      "[210/1000]\n",
      "- Train Loss : 0.15795749757024977 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15315067768096924 Score : 0.9484702348709106\n",
      "[211/1000]\n",
      "- Train Loss : 0.15737279338969123 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15238507091999054 Score : 0.9484702348709106\n",
      "[212/1000]\n",
      "- Train Loss : 0.15685118113954863 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15164485573768616 Score : 0.9484702348709106\n",
      "[213/1000]\n",
      "- Train Loss : 0.15636157823933494 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15093982219696045 Score : 0.9484702348709106\n",
      "[214/1000]\n",
      "- Train Loss : 0.15580743758214843 Score : 0.9346266984939575\n",
      "- Val Loss : 0.15021871030330658 Score : 0.9484702348709106\n",
      "[215/1000]\n",
      "- Train Loss : 0.15529185285170874 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14950129389762878 Score : 0.9484702348709106\n",
      "[216/1000]\n",
      "- Train Loss : 0.15478161060147816 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1488010585308075 Score : 0.9484702348709106\n",
      "[217/1000]\n",
      "- Train Loss : 0.15428951382637024 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14812110364437103 Score : 0.9484702348709106\n",
      "[218/1000]\n",
      "- Train Loss : 0.15378438805540404 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14743612706661224 Score : 0.9484702348709106\n",
      "[219/1000]\n",
      "- Train Loss : 0.15325159372554886 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14673110842704773 Score : 0.9484702348709106\n",
      "[220/1000]\n",
      "- Train Loss : 0.1527725834813383 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1460496187210083 Score : 0.9484702348709106\n",
      "[221/1000]\n",
      "- Train Loss : 0.15232967709501585 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1454000622034073 Score : 0.9484702348709106\n",
      "[222/1000]\n",
      "- Train Loss : 0.1518253990345531 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14473286271095276 Score : 0.9484702348709106\n",
      "[223/1000]\n",
      "- Train Loss : 0.15132749494579104 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1440601646900177 Score : 0.9484702348709106\n",
      "[224/1000]\n",
      "- Train Loss : 0.15085424151685503 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1434093415737152 Score : 0.9484702348709106\n",
      "[225/1000]\n",
      "- Train Loss : 0.1504308432340622 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14278995990753174 Score : 0.9484702348709106\n",
      "[226/1000]\n",
      "- Train Loss : 0.1499707889225748 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14216700196266174 Score : 0.9484702348709106\n",
      "[227/1000]\n",
      "- Train Loss : 0.1494759428832266 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14151567220687866 Score : 0.9484702348709106\n",
      "[228/1000]\n",
      "- Train Loss : 0.1490130474170049 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14086805284023285 Score : 0.9484702348709106\n",
      "[229/1000]\n",
      "- Train Loss : 0.1485864118569427 Score : 0.9346266984939575\n",
      "- Val Loss : 0.14025087654590607 Score : 0.9484702348709106\n",
      "[230/1000]\n",
      "- Train Loss : 0.14817190087503856 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13965876400470734 Score : 0.9484702348709106\n",
      "[231/1000]\n",
      "- Train Loss : 0.14770995908313328 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13905027508735657 Score : 0.9484702348709106\n",
      "[232/1000]\n",
      "- Train Loss : 0.14727082889941004 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13843926787376404 Score : 0.9484702348709106\n",
      "[233/1000]\n",
      "- Train Loss : 0.14683127693004078 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13783982396125793 Score : 0.9484702348709106\n",
      "[234/1000]\n",
      "- Train Loss : 0.14643679642015034 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1372634917497635 Score : 0.9484702348709106\n",
      "[235/1000]\n",
      "- Train Loss : 0.1460065104895168 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1366836577653885 Score : 0.9484702348709106\n",
      "[236/1000]\n",
      "- Train Loss : 0.14557379401392406 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13609053194522858 Score : 0.9484702348709106\n",
      "[237/1000]\n",
      "- Train Loss : 0.1451541975968414 Score : 0.9346266984939575\n",
      "- Val Loss : 0.135502889752388 Score : 0.9484702348709106\n",
      "[238/1000]\n",
      "- Train Loss : 0.14475900638434622 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1349380910396576 Score : 0.9484702348709106\n",
      "[239/1000]\n",
      "- Train Loss : 0.1443846051891645 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13439352810382843 Score : 0.9484702348709106\n",
      "[240/1000]\n",
      "- Train Loss : 0.14395835623145103 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13383238017559052 Score : 0.9484702348709106\n",
      "[241/1000]\n",
      "- Train Loss : 0.14354253932833672 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13326682150363922 Score : 0.9484702348709106\n",
      "[242/1000]\n",
      "- Train Loss : 0.14316599774691793 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13271482288837433 Score : 0.9484702348709106\n",
      "[243/1000]\n",
      "- Train Loss : 0.14278683273328674 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1321805864572525 Score : 0.9484702348709106\n",
      "[244/1000]\n",
      "- Train Loss : 0.14240737300780085 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13165687024593353 Score : 0.9484702348709106\n",
      "[245/1000]\n",
      "- Train Loss : 0.14201258992155394 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13112401962280273 Score : 0.9484702348709106\n",
      "[246/1000]\n",
      "- Train Loss : 0.14162146962351269 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13058170676231384 Score : 0.9484702348709106\n",
      "[247/1000]\n",
      "- Train Loss : 0.14125109339753786 Score : 0.9346266984939575\n",
      "- Val Loss : 0.13004836440086365 Score : 0.9484702348709106\n",
      "[248/1000]\n",
      "- Train Loss : 0.14088835443059602 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12953345477581024 Score : 0.9484702348709106\n",
      "[249/1000]\n",
      "- Train Loss : 0.14053714854849708 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12903395295143127 Score : 0.9484702348709106\n",
      "[250/1000]\n",
      "- Train Loss : 0.14017405526505577 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12853077054023743 Score : 0.9484702348709106\n",
      "[251/1000]\n",
      "- Train Loss : 0.1397885266277525 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12801112234592438 Score : 0.9484702348709106\n",
      "[252/1000]\n",
      "- Train Loss : 0.13942434845699203 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1274961531162262 Score : 0.9484702348709106\n",
      "[253/1000]\n",
      "- Train Loss : 0.13908589921063846 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12700338661670685 Score : 0.9484702348709106\n",
      "[254/1000]\n",
      "- Train Loss : 0.13876530031363168 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1265287846326828 Score : 0.9484702348709106\n",
      "[255/1000]\n",
      "- Train Loss : 0.13839632686641481 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1260402947664261 Score : 0.9484702348709106\n",
      "[256/1000]\n",
      "- Train Loss : 0.1380401582767566 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12554769217967987 Score : 0.9484702348709106\n",
      "[257/1000]\n",
      "- Train Loss : 0.1377076713575257 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12506328523159027 Score : 0.9484702348709106\n",
      "[258/1000]\n",
      "- Train Loss : 0.1373661986241738 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12459113448858261 Score : 0.9484702348709106\n",
      "[259/1000]\n",
      "- Train Loss : 0.1370566135479344 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1241336241364479 Score : 0.9484702348709106\n",
      "[260/1000]\n",
      "- Train Loss : 0.13671190043290457 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1236664205789566 Score : 0.9484702348709106\n",
      "[261/1000]\n",
      "- Train Loss : 0.1363643906596634 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12319282442331314 Score : 0.9484702348709106\n",
      "[262/1000]\n",
      "- Train Loss : 0.13605261490576798 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12272977828979492 Score : 0.9484702348709106\n",
      "[263/1000]\n",
      "- Train Loss : 0.13573702383372518 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12227993458509445 Score : 0.9484702348709106\n",
      "[264/1000]\n",
      "- Train Loss : 0.13542580397592652 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12183443456888199 Score : 0.9484702348709106\n",
      "[265/1000]\n",
      "- Train Loss : 0.13510672168599236 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12138717621564865 Score : 0.9484702348709106\n",
      "[266/1000]\n",
      "- Train Loss : 0.13477225539584956 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12093294411897659 Score : 0.9484702348709106\n",
      "[267/1000]\n",
      "- Train Loss : 0.13447802638014159 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12049280107021332 Score : 0.9484702348709106\n",
      "[268/1000]\n",
      "- Train Loss : 0.13418132811784744 Score : 0.9346266984939575\n",
      "- Val Loss : 0.12006158381700516 Score : 0.9484702348709106\n",
      "[269/1000]\n",
      "- Train Loss : 0.13386175367567274 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11963014304637909 Score : 0.9484702348709106\n",
      "[270/1000]\n",
      "- Train Loss : 0.1335552055388689 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11919909715652466 Score : 0.9484702348709106\n",
      "[271/1000]\n",
      "- Train Loss : 0.13328033685684204 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11878269910812378 Score : 0.9484702348709106\n",
      "[272/1000]\n",
      "- Train Loss : 0.13296263727049032 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11835384368896484 Score : 0.9484702348709106\n",
      "[273/1000]\n",
      "- Train Loss : 0.13265892242391905 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11792651563882828 Score : 0.9484702348709106\n",
      "[274/1000]\n",
      "- Train Loss : 0.13237878763013416 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1175166592001915 Score : 0.9484702348709106\n",
      "[275/1000]\n",
      "- Train Loss : 0.13210788845188087 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11711554229259491 Score : 0.9484702348709106\n",
      "[276/1000]\n",
      "- Train Loss : 0.13181661193569502 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11671095341444016 Score : 0.9484702348709106\n",
      "[277/1000]\n",
      "- Train Loss : 0.13151713978085253 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11629762500524521 Score : 0.9484702348709106\n",
      "[278/1000]\n",
      "- Train Loss : 0.13122977026634747 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11588845402002335 Score : 0.9484702348709106\n",
      "[279/1000]\n",
      "- Train Loss : 0.13096753590636784 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11549421399831772 Score : 0.9484702348709106\n",
      "[280/1000]\n",
      "- Train Loss : 0.13069608186682066 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11510707437992096 Score : 0.9484702348709106\n",
      "[281/1000]\n",
      "- Train Loss : 0.13042095945113236 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11471523344516754 Score : 0.9484702348709106\n",
      "[282/1000]\n",
      "- Train Loss : 0.13012871187594202 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11431782692670822 Score : 0.9484702348709106\n",
      "[283/1000]\n",
      "- Train Loss : 0.12987112378080687 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11393195390701294 Score : 0.9484702348709106\n",
      "[284/1000]\n",
      "- Train Loss : 0.12961385026574135 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11355394124984741 Score : 0.9484702348709106\n",
      "[285/1000]\n",
      "- Train Loss : 0.12934084195229742 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11317617446184158 Score : 0.9484702348709106\n",
      "[286/1000]\n",
      "- Train Loss : 0.12907317984435293 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11279458552598953 Score : 0.9484702348709106\n",
      "[287/1000]\n",
      "- Train Loss : 0.12882504819167984 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11242188513278961 Score : 0.9484702348709106\n",
      "[288/1000]\n",
      "- Train Loss : 0.12857206248574787 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11205200850963593 Score : 0.9484702348709106\n",
      "[289/1000]\n",
      "- Train Loss : 0.1282931272354391 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11168401688337326 Score : 0.9484702348709106\n",
      "[290/1000]\n",
      "- Train Loss : 0.12805433509250483 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11132108420133591 Score : 0.9484702348709106\n",
      "[291/1000]\n",
      "- Train Loss : 0.1278083473443985 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11096037179231644 Score : 0.9484702348709106\n",
      "[292/1000]\n",
      "- Train Loss : 0.1275533191445801 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11060136556625366 Score : 0.9484702348709106\n",
      "[293/1000]\n",
      "- Train Loss : 0.12730604203210938 Score : 0.9346266984939575\n",
      "- Val Loss : 0.11024411767721176 Score : 0.9484702348709106\n",
      "[294/1000]\n",
      "- Train Loss : 0.1270652525126934 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10988926142454147 Score : 0.9484702348709106\n",
      "[295/1000]\n",
      "- Train Loss : 0.12681294522351688 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10953908413648605 Score : 0.9484702348709106\n",
      "[296/1000]\n",
      "- Train Loss : 0.12658411492076185 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10919417440891266 Score : 0.9484702348709106\n",
      "[297/1000]\n",
      "- Train Loss : 0.12634246278968123 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1088503748178482 Score : 0.9484702348709106\n",
      "[298/1000]\n",
      "- Train Loss : 0.12609957303437921 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10850633680820465 Score : 0.9484702348709106\n",
      "[299/1000]\n",
      "- Train Loss : 0.12586886539227432 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1081666499376297 Score : 0.9484702348709106\n",
      "[300/1000]\n",
      "- Train Loss : 0.12564244866371155 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10783233493566513 Score : 0.9484702348709106\n",
      "[301/1000]\n",
      "- Train Loss : 0.12540781021946007 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1074981689453125 Score : 0.9484702348709106\n",
      "[302/1000]\n",
      "- Train Loss : 0.12517541729741627 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1071651354432106 Score : 0.9484702348709106\n",
      "[303/1000]\n",
      "- Train Loss : 0.12495574976007144 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10683543980121613 Score : 0.9484702348709106\n",
      "[304/1000]\n",
      "- Train Loss : 0.12472087393204372 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10650762170553207 Score : 0.9484702348709106\n",
      "[305/1000]\n",
      "- Train Loss : 0.12449553484718005 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10618496686220169 Score : 0.9484702348709106\n",
      "[306/1000]\n",
      "- Train Loss : 0.12429302910135852 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10586657375097275 Score : 0.9484702348709106\n",
      "[307/1000]\n",
      "- Train Loss : 0.1240627190305127 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10554499924182892 Score : 0.9484702348709106\n",
      "[308/1000]\n",
      "- Train Loss : 0.1238425580991639 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10522723197937012 Score : 0.9484702348709106\n",
      "[309/1000]\n",
      "- Train Loss : 0.1236285629371802 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10491267591714859 Score : 0.9484702348709106\n",
      "[310/1000]\n",
      "- Train Loss : 0.12340627072585954 Score : 0.9346266984939575\n",
      "- Val Loss : 0.1046021580696106 Score : 0.9484702348709106\n",
      "[311/1000]\n",
      "- Train Loss : 0.12320955884125498 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10429523140192032 Score : 0.9484702348709106\n",
      "[312/1000]\n",
      "- Train Loss : 0.12299259503682454 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10398757457733154 Score : 0.9484702348709106\n",
      "[313/1000]\n",
      "- Train Loss : 0.12277192663815287 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10368059575557709 Score : 0.9484702348709106\n",
      "[314/1000]\n",
      "- Train Loss : 0.12257450425790416 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10337858647108078 Score : 0.9484702348709106\n",
      "[315/1000]\n",
      "- Train Loss : 0.1223659844448169 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10307826846837997 Score : 0.9484702348709106\n",
      "[316/1000]\n",
      "- Train Loss : 0.12215582260655032 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10278107225894928 Score : 0.9484702348709106\n",
      "[317/1000]\n",
      "- Train Loss : 0.12196663974059953 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10248661041259766 Score : 0.9484702348709106\n",
      "[318/1000]\n",
      "- Train Loss : 0.12175606646471554 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10218989849090576 Score : 0.9484702348709106\n",
      "[319/1000]\n",
      "- Train Loss : 0.12155607425504261 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10189767181873322 Score : 0.9484702348709106\n",
      "[320/1000]\n",
      "- Train Loss : 0.12136287097301748 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10160893946886063 Score : 0.9484702348709106\n",
      "[321/1000]\n",
      "- Train Loss : 0.12115532946255472 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10132253915071487 Score : 0.9484702348709106\n",
      "[322/1000]\n",
      "- Train Loss : 0.12096846517589357 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10103797167539597 Score : 0.9484702348709106\n",
      "[323/1000]\n",
      "- Train Loss : 0.12077566194865438 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10075527429580688 Score : 0.9484702348709106\n",
      "[324/1000]\n",
      "- Train Loss : 0.12057941634621885 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10047416388988495 Score : 0.9484702348709106\n",
      "[325/1000]\n",
      "- Train Loss : 0.12039289209577772 Score : 0.9346266984939575\n",
      "- Val Loss : 0.10019516944885254 Score : 0.9484702348709106\n",
      "[326/1000]\n",
      "- Train Loss : 0.12019442042542829 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09991840273141861 Score : 0.9484702348709106\n",
      "[327/1000]\n",
      "- Train Loss : 0.1200119602597422 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09964021295309067 Score : 0.9484702348709106\n",
      "[328/1000]\n",
      "- Train Loss : 0.11982410587370396 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09936505556106567 Score : 0.9484702348709106\n",
      "[329/1000]\n",
      "- Train Loss : 0.11964683255387677 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09909800440073013 Score : 0.9484702348709106\n",
      "[330/1000]\n",
      "- Train Loss : 0.11946710592342748 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09883307665586472 Score : 0.9484702348709106\n",
      "[331/1000]\n",
      "- Train Loss : 0.11927376956575447 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09856695681810379 Score : 0.9484702348709106\n",
      "[332/1000]\n",
      "- Train Loss : 0.11908772753344642 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09830234199762344 Score : 0.9484702348709106\n",
      "[333/1000]\n",
      "- Train Loss : 0.11891626297599739 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09803836792707443 Score : 0.9484702348709106\n",
      "[334/1000]\n",
      "- Train Loss : 0.11873653199937609 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09777359664440155 Score : 0.9484702348709106\n",
      "[335/1000]\n",
      "- Train Loss : 0.11856350551048915 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09751381725072861 Score : 0.9484702348709106\n",
      "[336/1000]\n",
      "- Train Loss : 0.11837969927324189 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09725707024335861 Score : 0.9484702348709106\n",
      "[337/1000]\n",
      "- Train Loss : 0.11821288615465164 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09700360149145126 Score : 0.9484702348709106\n",
      "[338/1000]\n",
      "- Train Loss : 0.11803813361459309 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09675081074237823 Score : 0.9484702348709106\n",
      "[339/1000]\n",
      "- Train Loss : 0.11786037849055396 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09649529308080673 Score : 0.9484702348709106\n",
      "[340/1000]\n",
      "- Train Loss : 0.11769151335789098 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09624326229095459 Score : 0.9484702348709106\n",
      "[341/1000]\n",
      "- Train Loss : 0.11753284558653831 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09599713236093521 Score : 0.9484702348709106\n",
      "[342/1000]\n",
      "- Train Loss : 0.11735850738154517 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09575106203556061 Score : 0.9484702348709106\n",
      "[343/1000]\n",
      "- Train Loss : 0.11718358451293574 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09550689905881882 Score : 0.9484702348709106\n",
      "[344/1000]\n",
      "- Train Loss : 0.11702578680382834 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09526395052671432 Score : 0.9484702348709106\n",
      "[345/1000]\n",
      "- Train Loss : 0.11685940602587329 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09502177685499191 Score : 0.9484702348709106\n",
      "[346/1000]\n",
      "- Train Loss : 0.11669738673501545 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09478185325860977 Score : 0.9484702348709106\n",
      "[347/1000]\n",
      "- Train Loss : 0.11653463335500823 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0945420190691948 Score : 0.9484702348709106\n",
      "[348/1000]\n",
      "- Train Loss : 0.11636743839416239 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09430254250764847 Score : 0.9484702348709106\n",
      "[349/1000]\n",
      "- Train Loss : 0.11621491155690616 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09406977146863937 Score : 0.9484702348709106\n",
      "[350/1000]\n",
      "- Train Loss : 0.11605868012540871 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0938379317522049 Score : 0.9484702348709106\n",
      "[351/1000]\n",
      "- Train Loss : 0.11589491636388832 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0936051532626152 Score : 0.9484702348709106\n",
      "[352/1000]\n",
      "- Train Loss : 0.11574679571721289 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09337639063596725 Score : 0.9484702348709106\n",
      "[353/1000]\n",
      "- Train Loss : 0.11558941937983036 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09314834326505661 Score : 0.9484702348709106\n",
      "[354/1000]\n",
      "- Train Loss : 0.11542881611320707 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09291743487119675 Score : 0.9484702348709106\n",
      "[355/1000]\n",
      "- Train Loss : 0.11527402326464653 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09268958121538162 Score : 0.9484702348709106\n",
      "[356/1000]\n",
      "- Train Loss : 0.11513248996602164 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0924687385559082 Score : 0.9484702348709106\n",
      "[357/1000]\n",
      "- Train Loss : 0.11497980749441518 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09224836528301239 Score : 0.9484702348709106\n",
      "[358/1000]\n",
      "- Train Loss : 0.11482330763505565 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09202650934457779 Score : 0.9484702348709106\n",
      "[359/1000]\n",
      "- Train Loss : 0.11468433029949665 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09180780500173569 Score : 0.9484702348709106\n",
      "[360/1000]\n",
      "- Train Loss : 0.11453213066690499 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09158911556005478 Score : 0.9484702348709106\n",
      "[361/1000]\n",
      "- Train Loss : 0.11437556478712294 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09137185662984848 Score : 0.9484702348709106\n",
      "[362/1000]\n",
      "- Train Loss : 0.11423449425233735 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09115875512361526 Score : 0.9484702348709106\n",
      "[363/1000]\n",
      "- Train Loss : 0.11409846486316787 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09094537049531937 Score : 0.9484702348709106\n",
      "[364/1000]\n",
      "- Train Loss : 0.1139531729535924 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09073139727115631 Score : 0.9484702348709106\n",
      "[365/1000]\n",
      "- Train Loss : 0.11380257105661763 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09051843732595444 Score : 0.9484702348709106\n",
      "[366/1000]\n",
      "- Train Loss : 0.11366713047027588 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09031100571155548 Score : 0.9484702348709106\n",
      "[367/1000]\n",
      "- Train Loss : 0.11353252331415813 Score : 0.9346266984939575\n",
      "- Val Loss : 0.09010501950979233 Score : 0.9484702348709106\n",
      "[368/1000]\n",
      "- Train Loss : 0.11338282956017388 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08989900350570679 Score : 0.9484702348709106\n",
      "[369/1000]\n",
      "- Train Loss : 0.11324278513590495 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08969301730394363 Score : 0.9484702348709106\n",
      "[370/1000]\n",
      "- Train Loss : 0.11310358179940118 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08948610723018646 Score : 0.9484702348709106\n",
      "[371/1000]\n",
      "- Train Loss : 0.11296888896160656 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0892825797200203 Score : 0.9484702348709106\n",
      "[372/1000]\n",
      "- Train Loss : 0.11284431318442027 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08908417075872421 Score : 0.9484702348709106\n",
      "[373/1000]\n",
      "- Train Loss : 0.11269991430971357 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08888411521911621 Score : 0.9484702348709106\n",
      "[374/1000]\n",
      "- Train Loss : 0.1125576537516382 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08868516236543655 Score : 0.9484702348709106\n",
      "[375/1000]\n",
      "- Train Loss : 0.11243756156828669 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08849037438631058 Score : 0.9484702348709106\n",
      "[376/1000]\n",
      "- Train Loss : 0.11230102760924233 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08829423040151596 Score : 0.9484702348709106\n",
      "[377/1000]\n",
      "- Train Loss : 0.11216202771498097 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08809687197208405 Score : 0.9484702348709106\n",
      "[378/1000]\n",
      "- Train Loss : 0.11203719096051322 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08790230751037598 Score : 0.9484702348709106\n",
      "[379/1000]\n",
      "- Train Loss : 0.11191870251463519 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0877143070101738 Score : 0.9484702348709106\n",
      "[380/1000]\n",
      "- Train Loss : 0.11178219794399208 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08752577006816864 Score : 0.9484702348709106\n",
      "[381/1000]\n",
      "- Train Loss : 0.11164292403393322 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08733496814966202 Score : 0.9484702348709106\n",
      "[382/1000]\n",
      "- Train Loss : 0.11151921873291333 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08714397251605988 Score : 0.9484702348709106\n",
      "[383/1000]\n",
      "- Train Loss : 0.11139459928704633 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08695463836193085 Score : 0.9484702348709106\n",
      "[384/1000]\n",
      "- Train Loss : 0.11127324236763848 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08676868677139282 Score : 0.9484702348709106\n",
      "[385/1000]\n",
      "- Train Loss : 0.11115391179919243 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08658721297979355 Score : 0.9484702348709106\n",
      "[386/1000]\n",
      "- Train Loss : 0.11102679371833801 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08640573173761368 Score : 0.9484702348709106\n",
      "[387/1000]\n",
      "- Train Loss : 0.11089718010690477 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08622269332408905 Score : 0.9484702348709106\n",
      "[388/1000]\n",
      "- Train Loss : 0.11076845394240485 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08603967726230621 Score : 0.9484702348709106\n",
      "[389/1000]\n",
      "- Train Loss : 0.11065081279310915 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08585689961910248 Score : 0.9484702348709106\n",
      "[390/1000]\n",
      "- Train Loss : 0.11054262663755152 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08567928522825241 Score : 0.9484702348709106\n",
      "[391/1000]\n",
      "- Train Loss : 0.1104221281905969 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0855032354593277 Score : 0.9484702348709106\n",
      "[392/1000]\n",
      "- Train Loss : 0.11029827056659593 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08532710373401642 Score : 0.9484702348709106\n",
      "[393/1000]\n",
      "- Train Loss : 0.11017236361900966 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08514981716871262 Score : 0.9484702348709106\n",
      "[394/1000]\n",
      "- Train Loss : 0.11005620203084415 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08497555553913116 Score : 0.9484702348709106\n",
      "[395/1000]\n",
      "- Train Loss : 0.10993946033219497 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08480079472064972 Score : 0.9484702348709106\n",
      "[396/1000]\n",
      "- Train Loss : 0.10982529218826029 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08462733775377274 Score : 0.9484702348709106\n",
      "[397/1000]\n",
      "- Train Loss : 0.10972011668814553 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08445920795202255 Score : 0.9484702348709106\n",
      "[398/1000]\n",
      "- Train Loss : 0.10959840669400162 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08428984135389328 Score : 0.9484702348709106\n",
      "[399/1000]\n",
      "- Train Loss : 0.1094759346710311 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0841173604130745 Score : 0.9484702348709106\n",
      "[400/1000]\n",
      "- Train Loss : 0.10937048784560627 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08394966274499893 Score : 0.9484702348709106\n",
      "[401/1000]\n",
      "- Train Loss : 0.10925404561890496 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08378355205059052 Score : 0.9484702348709106\n",
      "[402/1000]\n",
      "- Train Loss : 0.10914641328983837 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08361737430095673 Score : 0.9484702348709106\n",
      "[403/1000]\n",
      "- Train Loss : 0.1090339743014839 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08345132321119308 Score : 0.9484702348709106\n",
      "[404/1000]\n",
      "- Train Loss : 0.10892179339296287 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08328567445278168 Score : 0.9484702348709106\n",
      "[405/1000]\n",
      "- Train Loss : 0.10881456836230224 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08312362432479858 Score : 0.9484702348709106\n",
      "[406/1000]\n",
      "- Train Loss : 0.10870765460034211 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08296269923448563 Score : 0.9484702348709106\n",
      "[407/1000]\n",
      "- Train Loss : 0.10859665688541201 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08280063420534134 Score : 0.9484702348709106\n",
      "[408/1000]\n",
      "- Train Loss : 0.10847948677837849 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08263857662677765 Score : 0.9484702348709106\n",
      "[409/1000]\n",
      "- Train Loss : 0.10837912869950135 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08247806131839752 Score : 0.9484702348709106\n",
      "[410/1000]\n",
      "- Train Loss : 0.10828321592675315 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08232158422470093 Score : 0.9484702348709106\n",
      "[411/1000]\n",
      "- Train Loss : 0.10817563947704104 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08216589689254761 Score : 0.9484702348709106\n",
      "[412/1000]\n",
      "- Train Loss : 0.10806630137893888 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08200991153717041 Score : 0.9484702348709106\n",
      "[413/1000]\n",
      "- Train Loss : 0.10795494231084983 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08185295760631561 Score : 0.9484702348709106\n",
      "[414/1000]\n",
      "- Train Loss : 0.10785533425708611 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08169835805892944 Score : 0.9484702348709106\n",
      "[415/1000]\n",
      "- Train Loss : 0.1077474948639671 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08154495805501938 Score : 0.9484702348709106\n",
      "[416/1000]\n",
      "- Train Loss : 0.10764850169006321 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0813915953040123 Score : 0.9484702348709106\n",
      "[417/1000]\n",
      "- Train Loss : 0.10754930807484521 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08123885840177536 Score : 0.9484702348709106\n",
      "[418/1000]\n",
      "- Train Loss : 0.1074494861273302 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08108826726675034 Score : 0.9484702348709106\n",
      "[419/1000]\n",
      "- Train Loss : 0.10734924922386806 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08093929290771484 Score : 0.9484702348709106\n",
      "[420/1000]\n",
      "- Train Loss : 0.10724386707362202 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08078902214765549 Score : 0.9484702348709106\n",
      "[421/1000]\n",
      "- Train Loss : 0.10714494747420152 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08064011484384537 Score : 0.9484702348709106\n",
      "[422/1000]\n",
      "- Train Loss : 0.10705167448355092 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08049403131008148 Score : 0.9484702348709106\n",
      "[423/1000]\n",
      "- Train Loss : 0.10694837176965342 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08034814894199371 Score : 0.9484702348709106\n",
      "[424/1000]\n",
      "- Train Loss : 0.10684665178673135 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08020053803920746 Score : 0.9484702348709106\n",
      "[425/1000]\n",
      "- Train Loss : 0.1067510847416189 Score : 0.9346266984939575\n",
      "- Val Loss : 0.08005362004041672 Score : 0.9484702348709106\n",
      "[426/1000]\n",
      "- Train Loss : 0.10666341117272775 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07991061359643936 Score : 0.9484702348709106\n",
      "[427/1000]\n",
      "- Train Loss : 0.10656622093584803 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07976925373077393 Score : 0.9484702348709106\n",
      "[428/1000]\n",
      "- Train Loss : 0.1064655286156469 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07962685078382492 Score : 0.9484702348709106\n",
      "[429/1000]\n",
      "- Train Loss : 0.10637351487659746 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0794856920838356 Score : 0.9484702348709106\n",
      "[430/1000]\n",
      "- Train Loss : 0.10627313061720794 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07934479415416718 Score : 0.9484702348709106\n",
      "[431/1000]\n",
      "- Train Loss : 0.1061799396864242 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07920332998037338 Score : 0.9484702348709106\n",
      "[432/1000]\n",
      "- Train Loss : 0.10609294412036736 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07906383275985718 Score : 0.9484702348709106\n",
      "[433/1000]\n",
      "- Train Loss : 0.10599927759418885 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07892489433288574 Score : 0.9484702348709106\n",
      "[434/1000]\n",
      "- Train Loss : 0.10590474742154281 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07878617197275162 Score : 0.9484702348709106\n",
      "[435/1000]\n",
      "- Train Loss : 0.10581879183236095 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07865005731582642 Score : 0.9484702348709106\n",
      "[436/1000]\n",
      "- Train Loss : 0.10572632205569082 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07851525396108627 Score : 0.9484702348709106\n",
      "[437/1000]\n",
      "- Train Loss : 0.10563098732382059 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07837964594364166 Score : 0.9484702348709106\n",
      "[438/1000]\n",
      "- Train Loss : 0.1055418395747741 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0782458633184433 Score : 0.9484702348709106\n",
      "[439/1000]\n",
      "- Train Loss : 0.10545301168329185 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07811157405376434 Score : 0.9484702348709106\n",
      "[440/1000]\n",
      "- Train Loss : 0.10536268560422792 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0779772400856018 Score : 0.9484702348709106\n",
      "[441/1000]\n",
      "- Train Loss : 0.1052748476051622 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07784400880336761 Score : 0.9484702348709106\n",
      "[442/1000]\n",
      "- Train Loss : 0.105195344115297 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07771457731723785 Score : 0.9484702348709106\n",
      "[443/1000]\n",
      "- Train Loss : 0.10510437563061714 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07758504897356033 Score : 0.9484702348709106\n",
      "[444/1000]\n",
      "- Train Loss : 0.10500986832711431 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07745397835969925 Score : 0.9484702348709106\n",
      "[445/1000]\n",
      "- Train Loss : 0.10492307485805617 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07732240855693817 Score : 0.9484702348709106\n",
      "[446/1000]\n",
      "- Train Loss : 0.10483903386112717 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0771934762597084 Score : 0.9484702348709106\n",
      "[447/1000]\n",
      "- Train Loss : 0.10475800124307473 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07706590741872787 Score : 0.9484702348709106\n",
      "[448/1000]\n",
      "- Train Loss : 0.10467388180808888 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07693865150213242 Score : 0.9484702348709106\n",
      "[449/1000]\n",
      "- Train Loss : 0.10458922261993091 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07681240141391754 Score : 0.9484702348709106\n",
      "[450/1000]\n",
      "- Train Loss : 0.10450509459608132 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07668731361627579 Score : 0.9484702348709106\n",
      "[451/1000]\n",
      "- Train Loss : 0.1044167239839832 Score : 0.9346266984939575\n",
      "- Val Loss : 0.076560840010643 Score : 0.9484702348709106\n",
      "[452/1000]\n",
      "- Train Loss : 0.10433323298477465 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0764349028468132 Score : 0.9484702348709106\n",
      "[453/1000]\n",
      "- Train Loss : 0.10425952728837729 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07631272077560425 Score : 0.9484702348709106\n",
      "[454/1000]\n",
      "- Train Loss : 0.10417349264025688 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07619112730026245 Score : 0.9484702348709106\n",
      "[455/1000]\n",
      "- Train Loss : 0.1040848061028454 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07606754451990128 Score : 0.9484702348709106\n",
      "[456/1000]\n",
      "- Train Loss : 0.10400583181116316 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07594417035579681 Score : 0.9484702348709106\n",
      "[457/1000]\n",
      "- Train Loss : 0.10393292146424453 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07582366466522217 Score : 0.9484702348709106\n",
      "[458/1000]\n",
      "- Train Loss : 0.10385448651181327 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07570499926805496 Score : 0.9484702348709106\n",
      "[459/1000]\n",
      "- Train Loss : 0.10376791624973218 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0755852460861206 Score : 0.9484702348709106\n",
      "[460/1000]\n",
      "- Train Loss : 0.1036836514249444 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07546473294496536 Score : 0.9484702348709106\n",
      "[461/1000]\n",
      "- Train Loss : 0.10360635775658819 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07534648478031158 Score : 0.9484702348709106\n",
      "[462/1000]\n",
      "- Train Loss : 0.10353103642248446 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07522831857204437 Score : 0.9484702348709106\n",
      "[463/1000]\n",
      "- Train Loss : 0.10345350154158142 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07511045038700104 Score : 0.9484702348709106\n",
      "[464/1000]\n",
      "- Train Loss : 0.10337514978730017 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07499318569898605 Score : 0.9484702348709106\n",
      "[465/1000]\n",
      "- Train Loss : 0.10329818208184507 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0748765617609024 Score : 0.9484702348709106\n",
      "[466/1000]\n",
      "- Train Loss : 0.10322072667380174 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07476043701171875 Score : 0.9484702348709106\n",
      "[467/1000]\n",
      "- Train Loss : 0.10314937567131387 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07464627176523209 Score : 0.9484702348709106\n",
      "[468/1000]\n",
      "- Train Loss : 0.10307274820903937 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07453346252441406 Score : 0.9484702348709106\n",
      "[469/1000]\n",
      "- Train Loss : 0.10299295228388575 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07441987842321396 Score : 0.9484702348709106\n",
      "[470/1000]\n",
      "- Train Loss : 0.10291249905195501 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07430577278137207 Score : 0.9484702348709106\n",
      "[471/1000]\n",
      "- Train Loss : 0.10284101652602355 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07419246435165405 Score : 0.9484702348709106\n",
      "[472/1000]\n",
      "- Train Loss : 0.10276955210914214 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07408015429973602 Score : 0.9484702348709106\n",
      "[473/1000]\n",
      "- Train Loss : 0.10269670281559229 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07396895438432693 Score : 0.9484702348709106\n",
      "[474/1000]\n",
      "- Train Loss : 0.10262359958142042 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07385923713445663 Score : 0.9484702348709106\n",
      "[475/1000]\n",
      "- Train Loss : 0.10255269882165724 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07375044375658035 Score : 0.9484702348709106\n",
      "[476/1000]\n",
      "- Train Loss : 0.10247576381597254 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0736411064863205 Score : 0.9484702348709106\n",
      "[477/1000]\n",
      "- Train Loss : 0.10239994577649567 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0735311508178711 Score : 0.9484702348709106\n",
      "[478/1000]\n",
      "- Train Loss : 0.10232485541039044 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07342173904180527 Score : 0.9484702348709106\n",
      "[479/1000]\n",
      "- Train Loss : 0.10225906605935758 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07331348210573196 Score : 0.9484702348709106\n",
      "[480/1000]\n",
      "- Train Loss : 0.10218959063705471 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0732058510184288 Score : 0.9484702348709106\n",
      "[481/1000]\n",
      "- Train Loss : 0.10211698586742084 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07309869676828384 Score : 0.9484702348709106\n",
      "[482/1000]\n",
      "- Train Loss : 0.10205099948992331 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07299287617206573 Score : 0.9484702348709106\n",
      "[483/1000]\n",
      "- Train Loss : 0.10197911390827762 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07288750261068344 Score : 0.9484702348709106\n",
      "[484/1000]\n",
      "- Train Loss : 0.10190834022230572 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07278276234865189 Score : 0.9484702348709106\n",
      "[485/1000]\n",
      "- Train Loss : 0.10183579619559976 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07267723977565765 Score : 0.9484702348709106\n",
      "[486/1000]\n",
      "- Train Loss : 0.10176624212827948 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07257219403982162 Score : 0.9484702348709106\n",
      "[487/1000]\n",
      "- Train Loss : 0.10169666094912423 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07246856391429901 Score : 0.9484702348709106\n",
      "[488/1000]\n",
      "- Train Loss : 0.10163295983026426 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07236587256193161 Score : 0.9484702348709106\n",
      "[489/1000]\n",
      "- Train Loss : 0.10156813822686672 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07226406037807465 Score : 0.9484702348709106\n",
      "[490/1000]\n",
      "- Train Loss : 0.10149663107262717 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07216215133666992 Score : 0.9484702348709106\n",
      "[491/1000]\n",
      "- Train Loss : 0.1014292279465331 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07206102460622787 Score : 0.9484702348709106\n",
      "[492/1000]\n",
      "- Train Loss : 0.10136527919934855 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07196098566055298 Score : 0.9484702348709106\n",
      "[493/1000]\n",
      "- Train Loss : 0.10129524198257261 Score : 0.9346266984939575\n",
      "- Val Loss : 0.071860671043396 Score : 0.9484702348709106\n",
      "[494/1000]\n",
      "- Train Loss : 0.10122678304711978 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07176004350185394 Score : 0.9484702348709106\n",
      "[495/1000]\n",
      "- Train Loss : 0.10116060388584931 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0716591328382492 Score : 0.9484702348709106\n",
      "[496/1000]\n",
      "- Train Loss : 0.10109186317357752 Score : 0.9346266984939575\n",
      "- Val Loss : 0.071559377014637 Score : 0.9484702348709106\n",
      "[497/1000]\n",
      "- Train Loss : 0.1010319163194961 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07146097719669342 Score : 0.9484702348709106\n",
      "[498/1000]\n",
      "- Train Loss : 0.10097436979413033 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07136417180299759 Score : 0.9484702348709106\n",
      "[499/1000]\n",
      "- Train Loss : 0.10090850873125924 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07126732170581818 Score : 0.9484702348709106\n",
      "[500/1000]\n",
      "- Train Loss : 0.1008404702362087 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07116997987031937 Score : 0.9484702348709106\n",
      "[501/1000]\n",
      "- Train Loss : 0.10077308045907153 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07107235491275787 Score : 0.9484702348709106\n",
      "[502/1000]\n",
      "- Train Loss : 0.10071056936350134 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07097626477479935 Score : 0.9484702348709106\n",
      "[503/1000]\n",
      "- Train Loss : 0.10065165534615517 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07088155299425125 Score : 0.9484702348709106\n",
      "[504/1000]\n",
      "- Train Loss : 0.10058397075368299 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07078728079795837 Score : 0.9484702348709106\n",
      "[505/1000]\n",
      "- Train Loss : 0.10051904639436139 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07069186866283417 Score : 0.9484702348709106\n",
      "[506/1000]\n",
      "- Train Loss : 0.10045862125439776 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0705968588590622 Score : 0.9484702348709106\n",
      "[507/1000]\n",
      "- Train Loss : 0.10040061569048299 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07050300389528275 Score : 0.9484702348709106\n",
      "[508/1000]\n",
      "- Train Loss : 0.10033773496333095 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07040946185588837 Score : 0.9484702348709106\n",
      "[509/1000]\n",
      "- Train Loss : 0.10027465524358882 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0703161209821701 Score : 0.9484702348709106\n",
      "[510/1000]\n",
      "- Train Loss : 0.10021790023893118 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07022406905889511 Score : 0.9484702348709106\n",
      "[511/1000]\n",
      "- Train Loss : 0.10015847088976039 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07013332843780518 Score : 0.9484702348709106\n",
      "[512/1000]\n",
      "- Train Loss : 0.10009297687146398 Score : 0.9346266984939575\n",
      "- Val Loss : 0.07004164159297943 Score : 0.9484702348709106\n",
      "[513/1000]\n",
      "- Train Loss : 0.10002734801835483 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06994910538196564 Score : 0.9484702348709106\n",
      "[514/1000]\n",
      "- Train Loss : 0.09997277282592323 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06985922902822495 Score : 0.9484702348709106\n",
      "[515/1000]\n",
      "- Train Loss : 0.09991534985601902 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06976936757564545 Score : 0.9484702348709106\n",
      "[516/1000]\n",
      "- Train Loss : 0.09985627565119001 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06967960298061371 Score : 0.9484702348709106\n",
      "[517/1000]\n",
      "- Train Loss : 0.09979489580210713 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06958960741758347 Score : 0.9484702348709106\n",
      "[518/1000]\n",
      "- Train Loss : 0.09973521954897377 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06950008124113083 Score : 0.9484702348709106\n",
      "[519/1000]\n",
      "- Train Loss : 0.09968200532926454 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06941192597150803 Score : 0.9484702348709106\n",
      "[520/1000]\n",
      "- Train Loss : 0.09962462385495503 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06932498514652252 Score : 0.9484702348709106\n",
      "[521/1000]\n",
      "- Train Loss : 0.09956324007362127 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06923755258321762 Score : 0.9484702348709106\n",
      "[522/1000]\n",
      "- Train Loss : 0.09950297688030535 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06914926320314407 Score : 0.9484702348709106\n",
      "[523/1000]\n",
      "- Train Loss : 0.09944361055062877 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06906197220087051 Score : 0.9484702348709106\n",
      "[524/1000]\n",
      "- Train Loss : 0.09939141726742189 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06897545605897903 Score : 0.9484702348709106\n",
      "[525/1000]\n",
      "- Train Loss : 0.09933459696670373 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06888939440250397 Score : 0.9484702348709106\n",
      "[526/1000]\n",
      "- Train Loss : 0.09927837985257308 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06880363821983337 Score : 0.9484702348709106\n",
      "[527/1000]\n",
      "- Train Loss : 0.09922635327610704 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06871871650218964 Score : 0.9484702348709106\n",
      "[528/1000]\n",
      "- Train Loss : 0.09916973217493957 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06863411515951157 Score : 0.9484702348709106\n",
      "[529/1000]\n",
      "- Train Loss : 0.09911391987568802 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06855025142431259 Score : 0.9484702348709106\n",
      "[530/1000]\n",
      "- Train Loss : 0.09905531215998861 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06846590340137482 Score : 0.9484702348709106\n",
      "[531/1000]\n",
      "- Train Loss : 0.09899608563217852 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0683809444308281 Score : 0.9484702348709106\n",
      "[532/1000]\n",
      "- Train Loss : 0.09894505598478848 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06829692423343658 Score : 0.9484702348709106\n",
      "[533/1000]\n",
      "- Train Loss : 0.09889483638107777 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0682145208120346 Score : 0.9484702348709106\n",
      "[534/1000]\n",
      "- Train Loss : 0.09883530541426605 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06813203543424606 Score : 0.9484702348709106\n",
      "[535/1000]\n",
      "- Train Loss : 0.09878281089994642 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06804969906806946 Score : 0.9484702348709106\n",
      "[536/1000]\n",
      "- Train Loss : 0.09873089380562305 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06796764582395554 Score : 0.9484702348709106\n",
      "[537/1000]\n",
      "- Train Loss : 0.09867774964206749 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06788599491119385 Score : 0.9484702348709106\n",
      "[538/1000]\n",
      "- Train Loss : 0.09862246954192717 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06780444085597992 Score : 0.9484702348709106\n",
      "[539/1000]\n",
      "- Train Loss : 0.09856901855932342 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06772349029779434 Score : 0.9484702348709106\n",
      "[540/1000]\n",
      "- Train Loss : 0.09852305002924469 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06764432787895203 Score : 0.9484702348709106\n",
      "[541/1000]\n",
      "- Train Loss : 0.09846890428000027 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06756500899791718 Score : 0.9484702348709106\n",
      "[542/1000]\n",
      "- Train Loss : 0.09841127155555619 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06748459488153458 Score : 0.9484702348709106\n",
      "[543/1000]\n",
      "- Train Loss : 0.09836079594161776 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06740511953830719 Score : 0.9484702348709106\n",
      "[544/1000]\n",
      "- Train Loss : 0.09830752677387661 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06732654571533203 Score : 0.9484702348709106\n",
      "[545/1000]\n",
      "- Train Loss : 0.09825576779743035 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06724759936332703 Score : 0.9484702348709106\n",
      "[546/1000]\n",
      "- Train Loss : 0.09820562156124248 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06716896593570709 Score : 0.9484702348709106\n",
      "[547/1000]\n",
      "- Train Loss : 0.09815626281003158 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06709075719118118 Score : 0.9484702348709106\n",
      "[548/1000]\n",
      "- Train Loss : 0.09810792592664559 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06701351702213287 Score : 0.9484702348709106\n",
      "[549/1000]\n",
      "- Train Loss : 0.09805823903944758 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06693693995475769 Score : 0.9484702348709106\n",
      "[550/1000]\n",
      "- Train Loss : 0.09800299236343966 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06685984134674072 Score : 0.9484702348709106\n",
      "[551/1000]\n",
      "- Train Loss : 0.0979543501097295 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06678285449743271 Score : 0.9484702348709106\n",
      "[552/1000]\n",
      "- Train Loss : 0.09790579943607251 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06670648604631424 Score : 0.9484702348709106\n",
      "[553/1000]\n",
      "- Train Loss : 0.09785835620843703 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06663122773170471 Score : 0.9484702348709106\n",
      "[554/1000]\n",
      "- Train Loss : 0.09780476842489508 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06655652821063995 Score : 0.9484702348709106\n",
      "[555/1000]\n",
      "- Train Loss : 0.09775426031814681 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06648102402687073 Score : 0.9484702348709106\n",
      "[556/1000]\n",
      "- Train Loss : 0.09770721673137611 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06640569865703583 Score : 0.9484702348709106\n",
      "[557/1000]\n",
      "- Train Loss : 0.09766024444252253 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06633095443248749 Score : 0.9484702348709106\n",
      "[558/1000]\n",
      "- Train Loss : 0.0976118818960256 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06625663489103317 Score : 0.9484702348709106\n",
      "[559/1000]\n",
      "- Train Loss : 0.09756286607848273 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06618271768093109 Score : 0.9484702348709106\n",
      "[560/1000]\n",
      "- Train Loss : 0.09751659041891496 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06610924750566483 Score : 0.9484702348709106\n",
      "[561/1000]\n",
      "- Train Loss : 0.09746873285621405 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06603590399026871 Score : 0.9484702348709106\n",
      "[562/1000]\n",
      "- Train Loss : 0.09742044295287794 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0659627839922905 Score : 0.9484702348709106\n",
      "[563/1000]\n",
      "- Train Loss : 0.09737214425371753 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06589005887508392 Score : 0.9484702348709106\n",
      "[564/1000]\n",
      "- Train Loss : 0.09732624277886417 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06581826508045197 Score : 0.9484702348709106\n",
      "[565/1000]\n",
      "- Train Loss : 0.09727830232845412 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06574613600969315 Score : 0.9484702348709106\n",
      "[566/1000]\n",
      "- Train Loss : 0.09722896841251188 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06567472219467163 Score : 0.9484702348709106\n",
      "[567/1000]\n",
      "- Train Loss : 0.09718561669190724 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06560371816158295 Score : 0.9484702348709106\n",
      "[568/1000]\n",
      "- Train Loss : 0.09714074619114399 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06553283333778381 Score : 0.9484702348709106\n",
      "[569/1000]\n",
      "- Train Loss : 0.09709327667951584 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06546205282211304 Score : 0.9484702348709106\n",
      "[570/1000]\n",
      "- Train Loss : 0.09704695155637132 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06539128720760345 Score : 0.9484702348709106\n",
      "[571/1000]\n",
      "- Train Loss : 0.0970032454157869 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06532134860754013 Score : 0.9484702348709106\n",
      "[572/1000]\n",
      "- Train Loss : 0.09695952302879757 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06525219231843948 Score : 0.9484702348709106\n",
      "[573/1000]\n",
      "- Train Loss : 0.09691198966983292 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06518303602933884 Score : 0.9484702348709106\n",
      "[574/1000]\n",
      "- Train Loss : 0.0968639479122228 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06511370837688446 Score : 0.9484702348709106\n",
      "[575/1000]\n",
      "- Train Loss : 0.09681622953050667 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06504397094249725 Score : 0.9484702348709106\n",
      "[576/1000]\n",
      "- Train Loss : 0.0967754094551007 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06497497111558914 Score : 0.9484702348709106\n",
      "[577/1000]\n",
      "- Train Loss : 0.09673335910257366 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06490657478570938 Score : 0.9484702348709106\n",
      "[578/1000]\n",
      "- Train Loss : 0.0966875698003504 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06483916938304901 Score : 0.9484702348709106\n",
      "[579/1000]\n",
      "- Train Loss : 0.09664459992200136 Score : 0.9346266984939575\n",
      "- Val Loss : 0.064771868288517 Score : 0.9484702348709106\n",
      "[580/1000]\n",
      "- Train Loss : 0.09660112992342976 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0647045224905014 Score : 0.9484702348709106\n",
      "[581/1000]\n",
      "- Train Loss : 0.09655589889734983 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0646371990442276 Score : 0.9484702348709106\n",
      "[582/1000]\n",
      "- Train Loss : 0.09651224139249986 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06457020342350006 Score : 0.9484702348709106\n",
      "[583/1000]\n",
      "- Train Loss : 0.09647265718215042 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0645042210817337 Score : 0.9484702348709106\n",
      "[584/1000]\n",
      "- Train Loss : 0.09642807963407701 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06443827599287033 Score : 0.9484702348709106\n",
      "[585/1000]\n",
      "- Train Loss : 0.0963802616008454 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06437163800001144 Score : 0.9484702348709106\n",
      "[586/1000]\n",
      "- Train Loss : 0.09633768318841855 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0643053874373436 Score : 0.9484702348709106\n",
      "[587/1000]\n",
      "- Train Loss : 0.09629897628393438 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06423983722925186 Score : 0.9484702348709106\n",
      "[588/1000]\n",
      "- Train Loss : 0.0962579394173291 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06417481601238251 Score : 0.9484702348709106\n",
      "[589/1000]\n",
      "- Train Loss : 0.0962145287129614 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06411097943782806 Score : 0.9484702348709106\n",
      "[590/1000]\n",
      "- Train Loss : 0.09617081149998638 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06404654681682587 Score : 0.9484702348709106\n",
      "[591/1000]\n",
      "- Train Loss : 0.09612924222730929 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06398194283246994 Score : 0.9484702348709106\n",
      "[592/1000]\n",
      "- Train Loss : 0.09608842691199647 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06391754001379013 Score : 0.9484702348709106\n",
      "[593/1000]\n",
      "- Train Loss : 0.09604655796041091 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06385349482297897 Score : 0.9484702348709106\n",
      "[594/1000]\n",
      "- Train Loss : 0.09600552171468735 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06378970295190811 Score : 0.9484702348709106\n",
      "[595/1000]\n",
      "- Train Loss : 0.0959663905410303 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06372648477554321 Score : 0.9484702348709106\n",
      "[596/1000]\n",
      "- Train Loss : 0.09592634491208527 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06366407871246338 Score : 0.9484702348709106\n",
      "[597/1000]\n",
      "- Train Loss : 0.09588311478081676 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06360168009996414 Score : 0.9484702348709106\n",
      "[598/1000]\n",
      "- Train Loss : 0.09584045234239763 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0635388046503067 Score : 0.9484702348709106\n",
      "[599/1000]\n",
      "- Train Loss : 0.095800899900496 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06347601860761642 Score : 0.9484702348709106\n",
      "[600/1000]\n",
      "- Train Loss : 0.09576214332547453 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06341373175382614 Score : 0.9484702348709106\n",
      "[601/1000]\n",
      "- Train Loss : 0.09572287369519472 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06335193663835526 Score : 0.9484702348709106\n",
      "[602/1000]\n",
      "- Train Loss : 0.0956829817344745 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06329136341810226 Score : 0.9484702348709106\n",
      "[603/1000]\n",
      "- Train Loss : 0.09564123892535766 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06323033571243286 Score : 0.9484702348709106\n",
      "[604/1000]\n",
      "- Train Loss : 0.09560203107280864 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0631692111492157 Score : 0.9484702348709106\n",
      "[605/1000]\n",
      "- Train Loss : 0.09556266479194164 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06310828775167465 Score : 0.9484702348709106\n",
      "[606/1000]\n",
      "- Train Loss : 0.09552394484894143 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06304754316806793 Score : 0.9484702348709106\n",
      "[607/1000]\n",
      "- Train Loss : 0.09548599324706528 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06298738718032837 Score : 0.9484702348709106\n",
      "[608/1000]\n",
      "- Train Loss : 0.09544713960753547 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06292755156755447 Score : 0.9484702348709106\n",
      "[609/1000]\n",
      "- Train Loss : 0.09540952204002275 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06286785751581192 Score : 0.9484702348709106\n",
      "[610/1000]\n",
      "- Train Loss : 0.09537051369746526 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0628083124756813 Score : 0.9484702348709106\n",
      "[611/1000]\n",
      "- Train Loss : 0.09533155285235909 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06274892389774323 Score : 0.9484702348709106\n",
      "[612/1000]\n",
      "- Train Loss : 0.09529468148118919 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06269000470638275 Score : 0.9484702348709106\n",
      "[613/1000]\n",
      "- Train Loss : 0.09525256003770563 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06263109296560287 Score : 0.9484702348709106\n",
      "[614/1000]\n",
      "- Train Loss : 0.09521624580439594 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06257273256778717 Score : 0.9484702348709106\n",
      "[615/1000]\n",
      "- Train Loss : 0.09517809976306227 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06251426786184311 Score : 0.9484702348709106\n",
      "[616/1000]\n",
      "- Train Loss : 0.0951412118350466 Score : 0.9346266984939575\n",
      "- Val Loss : 0.062455952167510986 Score : 0.9484702348709106\n",
      "[617/1000]\n",
      "- Train Loss : 0.09510298476864894 Score : 0.9346266984939575\n",
      "- Val Loss : 0.062398504465818405 Score : 0.9484702348709106\n",
      "[618/1000]\n",
      "- Train Loss : 0.09506736664722364 Score : 0.9346266984939575\n",
      "- Val Loss : 0.062341224402189255 Score : 0.9484702348709106\n",
      "[619/1000]\n",
      "- Train Loss : 0.09503095131367445 Score : 0.9346266984939575\n",
      "- Val Loss : 0.062283873558044434 Score : 0.9484702348709106\n",
      "[620/1000]\n",
      "- Train Loss : 0.09499347416890992 Score : 0.9346266984939575\n",
      "- Val Loss : 0.062226686626672745 Score : 0.9484702348709106\n",
      "[621/1000]\n",
      "- Train Loss : 0.09495609326081143 Score : 0.9346266984939575\n",
      "- Val Loss : 0.062169577926397324 Score : 0.9484702348709106\n",
      "[622/1000]\n",
      "- Train Loss : 0.0949185151192877 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06211258843541145 Score : 0.9484702348709106\n",
      "[623/1000]\n",
      "- Train Loss : 0.0948821631156736 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06205592304468155 Score : 0.9484702348709106\n",
      "[624/1000]\n",
      "- Train Loss : 0.09484773843238752 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06199975684285164 Score : 0.9484702348709106\n",
      "[625/1000]\n",
      "- Train Loss : 0.09481283070312606 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0619441457092762 Score : 0.9484702348709106\n",
      "[626/1000]\n",
      "- Train Loss : 0.09477259270432922 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061888255178928375 Score : 0.9484702348709106\n",
      "[627/1000]\n",
      "- Train Loss : 0.09473766013979912 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061832547187805176 Score : 0.9484702348709106\n",
      "[628/1000]\n",
      "- Train Loss : 0.09470296651124954 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061776984483003616 Score : 0.9484702348709106\n",
      "[629/1000]\n",
      "- Train Loss : 0.09466569249828656 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06172230467200279 Score : 0.9484702348709106\n",
      "[630/1000]\n",
      "- Train Loss : 0.09463211242109537 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06166771799325943 Score : 0.9484702348709106\n",
      "[631/1000]\n",
      "- Train Loss : 0.09459729430576165 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06161303073167801 Score : 0.9484702348709106\n",
      "[632/1000]\n",
      "- Train Loss : 0.09456171167807446 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06155845895409584 Score : 0.9484702348709106\n",
      "[633/1000]\n",
      "- Train Loss : 0.09452620624668068 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061503950506448746 Score : 0.9484702348709106\n",
      "[634/1000]\n",
      "- Train Loss : 0.09448958633260594 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06144975498318672 Score : 0.9484702348709106\n",
      "[635/1000]\n",
      "- Train Loss : 0.0944550402669443 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061395999044179916 Score : 0.9484702348709106\n",
      "[636/1000]\n",
      "- Train Loss : 0.09442400673611297 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061342496424913406 Score : 0.9484702348709106\n",
      "[637/1000]\n",
      "- Train Loss : 0.094389824093216 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061289090663194656 Score : 0.9484702348709106\n",
      "[638/1000]\n",
      "- Train Loss : 0.09435431710961792 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061235737055540085 Score : 0.9484702348709106\n",
      "[639/1000]\n",
      "- Train Loss : 0.09431927092373371 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06118260696530342 Score : 0.9484702348709106\n",
      "[640/1000]\n",
      "- Train Loss : 0.0942866535236438 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061129871755838394 Score : 0.9484702348709106\n",
      "[641/1000]\n",
      "- Train Loss : 0.09424966190838152 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06107693538069725 Score : 0.9484702348709106\n",
      "[642/1000]\n",
      "- Train Loss : 0.09421499187333716 Score : 0.9346266984939575\n",
      "- Val Loss : 0.061024244874715805 Score : 0.9484702348709106\n",
      "[643/1000]\n",
      "- Train Loss : 0.09418253559205267 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06097206845879555 Score : 0.9484702348709106\n",
      "[644/1000]\n",
      "- Train Loss : 0.09415146853360865 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06092004105448723 Score : 0.9484702348709106\n",
      "[645/1000]\n",
      "- Train Loss : 0.09411654124657313 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06086881831288338 Score : 0.9484702348709106\n",
      "[646/1000]\n",
      "- Train Loss : 0.0940842509476675 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06081757694482803 Score : 0.9484702348709106\n",
      "[647/1000]\n",
      "- Train Loss : 0.09405114149881734 Score : 0.9346266984939575\n",
      "- Val Loss : 0.060766253620386124 Score : 0.9484702348709106\n",
      "[648/1000]\n",
      "- Train Loss : 0.09401773433718416 Score : 0.9346266984939575\n",
      "- Val Loss : 0.060714930295944214 Score : 0.9484702348709106\n",
      "[649/1000]\n",
      "- Train Loss : 0.09398462623357773 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06066372990608215 Score : 0.9484702348709106\n",
      "[650/1000]\n",
      "- Train Loss : 0.09395180756433143 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06061270833015442 Score : 0.9484702348709106\n",
      "[651/1000]\n",
      "- Train Loss : 0.09391856990340683 Score : 0.9346266984939575\n",
      "- Val Loss : 0.060561876744031906 Score : 0.9484702348709106\n",
      "[652/1000]\n",
      "- Train Loss : 0.09388634034742911 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06051129847764969 Score : 0.9484702348709106\n",
      "[653/1000]\n",
      "- Train Loss : 0.0938556929015451 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06046108156442642 Score : 0.9484702348709106\n",
      "[654/1000]\n",
      "- Train Loss : 0.09382464353823 Score : 0.9346266984939575\n",
      "- Val Loss : 0.060411300510168076 Score : 0.9484702348709106\n",
      "[655/1000]\n",
      "- Train Loss : 0.09378855085621278 Score : 0.9346266984939575\n",
      "- Val Loss : 0.060361504554748535 Score : 0.9484702348709106\n",
      "[656/1000]\n",
      "- Train Loss : 0.09375742988453971 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06031173840165138 Score : 0.9484702348709106\n",
      "[657/1000]\n",
      "- Train Loss : 0.09372676050083505 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06026208773255348 Score : 0.9484702348709106\n",
      "[658/1000]\n",
      "- Train Loss : 0.09369547396070427 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06021266430616379 Score : 0.9484702348709106\n",
      "[659/1000]\n",
      "- Train Loss : 0.09366202902876669 Score : 0.9346266984939575\n",
      "- Val Loss : 0.060164086520671844 Score : 0.9484702348709106\n",
      "[660/1000]\n",
      "- Train Loss : 0.09363197504232328 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06011535972356796 Score : 0.9484702348709106\n",
      "[661/1000]\n",
      "- Train Loss : 0.09360105823725462 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06006669998168945 Score : 0.9484702348709106\n",
      "[662/1000]\n",
      "- Train Loss : 0.09356962775604592 Score : 0.9346266984939575\n",
      "- Val Loss : 0.06001807004213333 Score : 0.9484702348709106\n",
      "[663/1000]\n",
      "- Train Loss : 0.09353738495459159 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05996951833367348 Score : 0.9484702348709106\n",
      "[664/1000]\n",
      "- Train Loss : 0.09350649050126474 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05992121249437332 Score : 0.9484702348709106\n",
      "[665/1000]\n",
      "- Train Loss : 0.09347735779980819 Score : 0.9346266984939575\n",
      "- Val Loss : 0.059873104095458984 Score : 0.9484702348709106\n",
      "[666/1000]\n",
      "- Train Loss : 0.0934462205817302 Score : 0.9346266984939575\n",
      "- Val Loss : 0.059825293719768524 Score : 0.9484702348709106\n",
      "[667/1000]\n",
      "- Train Loss : 0.09341494325134489 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05977776646614075 Score : 0.9484702348709106\n",
      "[668/1000]\n",
      "- Train Loss : 0.09338556530161037 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05973023548722267 Score : 0.9484702348709106\n",
      "[669/1000]\n",
      "- Train Loss : 0.09335520377175675 Score : 0.9346266984939575\n",
      "- Val Loss : 0.059682849794626236 Score : 0.9484702348709106\n",
      "[670/1000]\n",
      "- Train Loss : 0.09332454390823841 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05963551253080368 Score : 0.9484702348709106\n",
      "[671/1000]\n",
      "- Train Loss : 0.09329416509717703 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05958845093846321 Score : 0.9484702348709106\n",
      "[672/1000]\n",
      "- Train Loss : 0.09326347584525745 Score : 0.9346266984939575\n",
      "- Val Loss : 0.059541601687669754 Score : 0.9484702348709106\n",
      "[673/1000]\n",
      "- Train Loss : 0.0932335118866629 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05949510261416435 Score : 0.9484702348709106\n",
      "[674/1000]\n",
      "- Train Loss : 0.09320531888968414 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05944868177175522 Score : 0.9484702348709106\n",
      "[675/1000]\n",
      "- Train Loss : 0.09317571193807656 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05940235033631325 Score : 0.9484702348709106\n",
      "[676/1000]\n",
      "- Train Loss : 0.09314569303145011 Score : 0.9346266984939575\n",
      "- Val Loss : 0.059356119483709335 Score : 0.9484702348709106\n",
      "[677/1000]\n",
      "- Train Loss : 0.09311460786395603 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05931059643626213 Score : 0.9484702348709106\n",
      "[678/1000]\n",
      "- Train Loss : 0.09308616496208641 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05926511809229851 Score : 0.9484702348709106\n",
      "[679/1000]\n",
      "- Train Loss : 0.09305761568248272 Score : 0.9346266984939575\n",
      "- Val Loss : 0.059219591319561005 Score : 0.9484702348709106\n",
      "[680/1000]\n",
      "- Train Loss : 0.0930275780459245 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05917411670088768 Score : 0.9484702348709106\n",
      "[681/1000]\n",
      "- Train Loss : 0.09299853133658569 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05912882089614868 Score : 0.9484702348709106\n",
      "[682/1000]\n",
      "- Train Loss : 0.0929709557029936 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0590837188065052 Score : 0.9484702348709106\n",
      "[683/1000]\n",
      "- Train Loss : 0.09294228524797493 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05903863161802292 Score : 0.9484702348709106\n",
      "[684/1000]\n",
      "- Train Loss : 0.09291320376925999 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05899374559521675 Score : 0.9484702348709106\n",
      "[685/1000]\n",
      "- Train Loss : 0.09288409962836239 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05894910544157028 Score : 0.9484702348709106\n",
      "[686/1000]\n",
      "- Train Loss : 0.09285505447122785 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05890475958585739 Score : 0.9484702348709106\n",
      "[687/1000]\n",
      "- Train Loss : 0.09282826290776332 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05886051431298256 Score : 0.9484702348709106\n",
      "[688/1000]\n",
      "- Train Loss : 0.09280014151914252 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05881630256772041 Score : 0.9484702348709106\n",
      "[689/1000]\n",
      "- Train Loss : 0.09277157154348162 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05877220630645752 Score : 0.9484702348709106\n",
      "[690/1000]\n",
      "- Train Loss : 0.09274322239475118 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05872826650738716 Score : 0.9484702348709106\n",
      "[691/1000]\n",
      "- Train Loss : 0.0927148282320963 Score : 0.9346266984939575\n",
      "- Val Loss : 0.058684609830379486 Score : 0.9484702348709106\n",
      "[692/1000]\n",
      "- Train Loss : 0.09268679387039608 Score : 0.9346266984939575\n",
      "- Val Loss : 0.058641280978918076 Score : 0.9484702348709106\n",
      "[693/1000]\n",
      "- Train Loss : 0.09266052881462707 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05859788879752159 Score : 0.9484702348709106\n",
      "[694/1000]\n",
      "- Train Loss : 0.09263297066920334 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05855465307831764 Score : 0.9484702348709106\n",
      "[695/1000]\n",
      "- Train Loss : 0.09260353394266632 Score : 0.9346266984939575\n",
      "- Val Loss : 0.058512132614851 Score : 0.9484702348709106\n",
      "[696/1000]\n",
      "- Train Loss : 0.09257701587759787 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05846957489848137 Score : 0.9484702348709106\n",
      "[697/1000]\n",
      "- Train Loss : 0.09255020154847039 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0584268681704998 Score : 0.9484702348709106\n",
      "[698/1000]\n",
      "- Train Loss : 0.09252233813620275 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05838429555296898 Score : 0.9484702348709106\n",
      "[699/1000]\n",
      "- Train Loss : 0.09249503413836162 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05834190174937248 Score : 0.9484702348709106\n",
      "[700/1000]\n",
      "- Train Loss : 0.09246937320050266 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05829959735274315 Score : 0.9484702348709106\n",
      "[701/1000]\n",
      "- Train Loss : 0.09244260823147164 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05825736001133919 Score : 0.9484702348709106\n",
      "[702/1000]\n",
      "- Train Loss : 0.09241529398908217 Score : 0.9346266984939575\n",
      "- Val Loss : 0.058215294033288956 Score : 0.9484702348709106\n",
      "[703/1000]\n",
      "- Train Loss : 0.09238834689474767 Score : 0.9346266984939575\n",
      "- Val Loss : 0.058173391968011856 Score : 0.9484702348709106\n",
      "[704/1000]\n",
      "- Train Loss : 0.09236088208854198 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05813175439834595 Score : 0.9484702348709106\n",
      "[705/1000]\n",
      "- Train Loss : 0.09233411867171526 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05809050425887108 Score : 0.9484702348709106\n",
      "[706/1000]\n",
      "- Train Loss : 0.09231062098923656 Score : 0.9346266984939575\n",
      "- Val Loss : 0.058049194514751434 Score : 0.9484702348709106\n",
      "[707/1000]\n",
      "- Train Loss : 0.09228456527408627 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05800793692469597 Score : 0.9484702348709106\n",
      "[708/1000]\n",
      "- Train Loss : 0.09225738007161352 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05796670541167259 Score : 0.9484702348709106\n",
      "[709/1000]\n",
      "- Train Loss : 0.09223057515919209 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057925719767808914 Score : 0.9484702348709106\n",
      "[710/1000]\n",
      "- Train Loss : 0.09220443552152978 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0578848198056221 Score : 0.9484702348709106\n",
      "[711/1000]\n",
      "- Train Loss : 0.09217863188435634 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05784403905272484 Score : 0.9484702348709106\n",
      "[712/1000]\n",
      "- Train Loss : 0.0921514036340846 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05780399218201637 Score : 0.9484702348709106\n",
      "[713/1000]\n",
      "- Train Loss : 0.09212702533437146 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0577639602124691 Score : 0.9484702348709106\n",
      "[714/1000]\n",
      "- Train Loss : 0.09210175907032357 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05772370845079422 Score : 0.9484702348709106\n",
      "[715/1000]\n",
      "- Train Loss : 0.09207600334452258 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057683706283569336 Score : 0.9484702348709106\n",
      "[716/1000]\n",
      "- Train Loss : 0.09204906608081526 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057643741369247437 Score : 0.9484702348709106\n",
      "[717/1000]\n",
      "- Train Loss : 0.09202372044738796 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05760423466563225 Score : 0.9484702348709106\n",
      "[718/1000]\n",
      "- Train Loss : 0.09200115160395701 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057564642280340195 Score : 0.9484702348709106\n",
      "[719/1000]\n",
      "- Train Loss : 0.09197623065362374 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057525068521499634 Score : 0.9484702348709106\n",
      "[720/1000]\n",
      "- Train Loss : 0.09195036968837182 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05748555064201355 Score : 0.9484702348709106\n",
      "[721/1000]\n",
      "- Train Loss : 0.0919246920901868 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05744628235697746 Score : 0.9484702348709106\n",
      "[722/1000]\n",
      "- Train Loss : 0.09189966672824489 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057407014071941376 Score : 0.9484702348709106\n",
      "[723/1000]\n",
      "- Train Loss : 0.09187500520298879 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05736792832612991 Score : 0.9484702348709106\n",
      "[724/1000]\n",
      "- Train Loss : 0.0918504668192731 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05732898786664009 Score : 0.9484702348709106\n",
      "[725/1000]\n",
      "- Train Loss : 0.0918258847668767 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05729022994637489 Score : 0.9484702348709106\n",
      "[726/1000]\n",
      "- Train Loss : 0.0918011170708471 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057251621037721634 Score : 0.9484702348709106\n",
      "[727/1000]\n",
      "- Train Loss : 0.09177613268709844 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05721340700984001 Score : 0.9484702348709106\n",
      "[728/1000]\n",
      "- Train Loss : 0.09175299666821957 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05717507749795914 Score : 0.9484702348709106\n",
      "[729/1000]\n",
      "- Train Loss : 0.09172874751190345 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057136811316013336 Score : 0.9484702348709106\n",
      "[730/1000]\n",
      "- Train Loss : 0.09170422309802638 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05709874629974365 Score : 0.9484702348709106\n",
      "[731/1000]\n",
      "- Train Loss : 0.09167986632221276 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05706069618463516 Score : 0.9484702348709106\n",
      "[732/1000]\n",
      "- Train Loss : 0.09165483837326367 Score : 0.9346266984939575\n",
      "- Val Loss : 0.057023026049137115 Score : 0.9484702348709106\n",
      "[733/1000]\n",
      "- Train Loss : 0.0916309046248595 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056985706090927124 Score : 0.9484702348709106\n",
      "[734/1000]\n",
      "- Train Loss : 0.09160970824046268 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0569482147693634 Score : 0.9484702348709106\n",
      "[735/1000]\n",
      "- Train Loss : 0.09158615788651837 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056910742074251175 Score : 0.9484702348709106\n",
      "[736/1000]\n",
      "- Train Loss : 0.09156165427217881 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0568733848631382 Score : 0.9484702348709106\n",
      "[737/1000]\n",
      "- Train Loss : 0.09153742943372992 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056836117058992386 Score : 0.9484702348709106\n",
      "[738/1000]\n",
      "- Train Loss : 0.09151380219393307 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056799035519361496 Score : 0.9484702348709106\n",
      "[739/1000]\n",
      "- Train Loss : 0.09148919665151173 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05676259845495224 Score : 0.9484702348709106\n",
      "[740/1000]\n",
      "- Train Loss : 0.0914669678443008 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056726180016994476 Score : 0.9484702348709106\n",
      "[741/1000]\n",
      "- Train Loss : 0.09144413036604722 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0566895492374897 Score : 0.9484702348709106\n",
      "[742/1000]\n",
      "- Train Loss : 0.09142079463021623 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056652944535017014 Score : 0.9484702348709106\n",
      "[743/1000]\n",
      "- Train Loss : 0.09139732261084849 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056616511195898056 Score : 0.9484702348709106\n",
      "[744/1000]\n",
      "- Train Loss : 0.09137374193718036 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05658021569252014 Score : 0.9484702348709106\n",
      "[745/1000]\n",
      "- Train Loss : 0.09135088262458642 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05654407665133476 Score : 0.9484702348709106\n",
      "[746/1000]\n",
      "- Train Loss : 0.09132944047451019 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056507982313632965 Score : 0.9484702348709106\n",
      "[747/1000]\n",
      "- Train Loss : 0.0913067873981264 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05647199600934982 Score : 0.9484702348709106\n",
      "[748/1000]\n",
      "- Train Loss : 0.09128313925531176 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056436244398355484 Score : 0.9484702348709106\n",
      "[749/1000]\n",
      "- Train Loss : 0.09126014034781191 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05640076473355293 Score : 0.9484702348709106\n",
      "[750/1000]\n",
      "- Train Loss : 0.09123870740748113 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056365255266427994 Score : 0.9484702348709106\n",
      "[751/1000]\n",
      "- Train Loss : 0.09121645759377214 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05632975697517395 Score : 0.9484702348709106\n",
      "[752/1000]\n",
      "- Train Loss : 0.09119376716100508 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056294407695531845 Score : 0.9484702348709106\n",
      "[753/1000]\n",
      "- Train Loss : 0.09117127758347326 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05625906214118004 Score : 0.9484702348709106\n",
      "[754/1000]\n",
      "- Train Loss : 0.09114902611407968 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05622395873069763 Score : 0.9484702348709106\n",
      "[755/1000]\n",
      "- Train Loss : 0.0911259741211931 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05618911609053612 Score : 0.9484702348709106\n",
      "[756/1000]\n",
      "- Train Loss : 0.09110395651724604 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05615466833114624 Score : 0.9484702348709106\n",
      "[757/1000]\n",
      "- Train Loss : 0.09108441488610373 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0561198927462101 Score : 0.9484702348709106\n",
      "[758/1000]\n",
      "- Train Loss : 0.09106264253043467 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056085240095853806 Score : 0.9484702348709106\n",
      "[759/1000]\n",
      "- Train Loss : 0.09104009852227238 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05605066195130348 Score : 0.9484702348709106\n",
      "[760/1000]\n",
      "- Train Loss : 0.09101780048675007 Score : 0.9346266984939575\n",
      "- Val Loss : 0.056016262620687485 Score : 0.9484702348709106\n",
      "[761/1000]\n",
      "- Train Loss : 0.09099601002203093 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05598191171884537 Score : 0.9484702348709106\n",
      "[762/1000]\n",
      "- Train Loss : 0.09097450267937449 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05594766139984131 Score : 0.9484702348709106\n",
      "[763/1000]\n",
      "- Train Loss : 0.09095312427315447 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055913571268320084 Score : 0.9484702348709106\n",
      "[764/1000]\n",
      "- Train Loss : 0.09093169785208172 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05587957426905632 Score : 0.9484702348709106\n",
      "[765/1000]\n",
      "- Train Loss : 0.09090922721144226 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05584615096449852 Score : 0.9484702348709106\n",
      "[766/1000]\n",
      "- Train Loss : 0.0908885614739524 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055812787264585495 Score : 0.9484702348709106\n",
      "[767/1000]\n",
      "- Train Loss : 0.09086723315219085 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05577932670712471 Score : 0.9484702348709106\n",
      "[768/1000]\n",
      "- Train Loss : 0.09084596350375149 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05574590340256691 Score : 0.9484702348709106\n",
      "[769/1000]\n",
      "- Train Loss : 0.09082590312593514 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05571252480149269 Score : 0.9484702348709106\n",
      "[770/1000]\n",
      "- Train Loss : 0.09080482708911101 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05567913129925728 Score : 0.9484702348709106\n",
      "[771/1000]\n",
      "- Train Loss : 0.09078357534276114 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05564591661095619 Score : 0.9484702348709106\n",
      "[772/1000]\n",
      "- Train Loss : 0.0907619980474313 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05561291053891182 Score : 0.9484702348709106\n",
      "[773/1000]\n",
      "- Train Loss : 0.09074108312941259 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055580172687768936 Score : 0.9484702348709106\n",
      "[774/1000]\n",
      "- Train Loss : 0.09072145229826371 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05554733797907829 Score : 0.9484702348709106\n",
      "[775/1000]\n",
      "- Train Loss : 0.09070099590139256 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05551450699567795 Score : 0.9484702348709106\n",
      "[776/1000]\n",
      "- Train Loss : 0.09068014555507237 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05548190325498581 Score : 0.9484702348709106\n",
      "[777/1000]\n",
      "- Train Loss : 0.09065936495446497 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055449310690164566 Score : 0.9484702348709106\n",
      "[778/1000]\n",
      "- Train Loss : 0.09063838474038574 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05541693791747093 Score : 0.9484702348709106\n",
      "[779/1000]\n",
      "- Train Loss : 0.09061816986650229 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05538472160696983 Score : 0.9484702348709106\n",
      "[780/1000]\n",
      "- Train Loss : 0.09059913684096602 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05535251647233963 Score : 0.9484702348709106\n",
      "[781/1000]\n",
      "- Train Loss : 0.09057906166546875 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05532032251358032 Score : 0.9484702348709106\n",
      "[782/1000]\n",
      "- Train Loss : 0.09055846329364511 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05528824403882027 Score : 0.9484702348709106\n",
      "[783/1000]\n",
      "- Train Loss : 0.0905379680916667 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055256426334381104 Score : 0.9484702348709106\n",
      "[784/1000]\n",
      "- Train Loss : 0.09051746378342311 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055224839597940445 Score : 0.9484702348709106\n",
      "[785/1000]\n",
      "- Train Loss : 0.09049856900754902 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0551932267844677 Score : 0.9484702348709106\n",
      "[786/1000]\n",
      "- Train Loss : 0.09047884545806381 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05516154691576958 Score : 0.9484702348709106\n",
      "[787/1000]\n",
      "- Train Loss : 0.09045877307653427 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05513007193803787 Score : 0.9484702348709106\n",
      "[788/1000]\n",
      "- Train Loss : 0.09043880986670653 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05509856343269348 Score : 0.9484702348709106\n",
      "[789/1000]\n",
      "- Train Loss : 0.09041859333713849 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055067338049411774 Score : 0.9484702348709106\n",
      "[790/1000]\n",
      "- Train Loss : 0.09039905118859476 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05503625050187111 Score : 0.9484702348709106\n",
      "[791/1000]\n",
      "- Train Loss : 0.09038074149025811 Score : 0.9346266984939575\n",
      "- Val Loss : 0.055005136877298355 Score : 0.9484702348709106\n",
      "[792/1000]\n",
      "- Train Loss : 0.09036134586979945 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05497406795620918 Score : 0.9484702348709106\n",
      "[793/1000]\n",
      "- Train Loss : 0.09034113834301631 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05494321510195732 Score : 0.9484702348709106\n",
      "[794/1000]\n",
      "- Train Loss : 0.09032142389979628 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05491270869970322 Score : 0.9484702348709106\n",
      "[795/1000]\n",
      "- Train Loss : 0.09030304642187224 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05488201975822449 Score : 0.9484702348709106\n",
      "[796/1000]\n",
      "- Train Loss : 0.09028395120468405 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054851435124874115 Score : 0.9484702348709106\n",
      "[797/1000]\n",
      "- Train Loss : 0.09026457793596718 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05482080951333046 Score : 0.9484702348709106\n",
      "[798/1000]\n",
      "- Train Loss : 0.09024528703755802 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05479046329855919 Score : 0.9484702348709106\n",
      "[799/1000]\n",
      "- Train Loss : 0.09022620889461702 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054760146886110306 Score : 0.9484702348709106\n",
      "[800/1000]\n",
      "- Train Loss : 0.09020690423332983 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054729901254177094 Score : 0.9484702348709106\n",
      "[801/1000]\n",
      "- Train Loss : 0.09018803812149498 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054699938744306564 Score : 0.9484702348709106\n",
      "[802/1000]\n",
      "- Train Loss : 0.09017036782784595 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05466987565159798 Score : 0.9484702348709106\n",
      "[803/1000]\n",
      "- Train Loss : 0.09015065783427821 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05464038997888565 Score : 0.9484702348709106\n",
      "[804/1000]\n",
      "- Train Loss : 0.09013214779810773 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05461081489920616 Score : 0.9484702348709106\n",
      "[805/1000]\n",
      "- Train Loss : 0.09011350934290224 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05458114668726921 Score : 0.9484702348709106\n",
      "[806/1000]\n",
      "- Train Loss : 0.09009471318374078 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05455153062939644 Score : 0.9484702348709106\n",
      "[807/1000]\n",
      "- Train Loss : 0.09007559209648106 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05452198535203934 Score : 0.9484702348709106\n",
      "[808/1000]\n",
      "- Train Loss : 0.0900572578733166 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054492659866809845 Score : 0.9484702348709106\n",
      "[809/1000]\n",
      "- Train Loss : 0.09004001495324904 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0544632188975811 Score : 0.9484702348709106\n",
      "[810/1000]\n",
      "- Train Loss : 0.09002176413519515 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0544339083135128 Score : 0.9484702348709106\n",
      "[811/1000]\n",
      "- Train Loss : 0.09000314927349488 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054404664784669876 Score : 0.9484702348709106\n",
      "[812/1000]\n",
      "- Train Loss : 0.08998445607721806 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054375648498535156 Score : 0.9484702348709106\n",
      "[813/1000]\n",
      "- Train Loss : 0.08996595380206902 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054346948862075806 Score : 0.9484702348709106\n",
      "[814/1000]\n",
      "- Train Loss : 0.0899488494421045 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05431802198290825 Score : 0.9484702348709106\n",
      "[815/1000]\n",
      "- Train Loss : 0.08993096163289414 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05428913235664368 Score : 0.9484702348709106\n",
      "[816/1000]\n",
      "- Train Loss : 0.08991267904639244 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05426037684082985 Score : 0.9484702348709106\n",
      "[817/1000]\n",
      "- Train Loss : 0.08989452394760317 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05423169583082199 Score : 0.9484702348709106\n",
      "[818/1000]\n",
      "- Train Loss : 0.08987610647454858 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054203253239393234 Score : 0.9484702348709106\n",
      "[819/1000]\n",
      "- Train Loss : 0.08985853753983974 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054174888879060745 Score : 0.9484702348709106\n",
      "[820/1000]\n",
      "- Train Loss : 0.08984195979105102 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05414650961756706 Score : 0.9484702348709106\n",
      "[821/1000]\n",
      "- Train Loss : 0.08982436053661837 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05411814525723457 Score : 0.9484702348709106\n",
      "[822/1000]\n",
      "- Train Loss : 0.08980642657520042 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054089874029159546 Score : 0.9484702348709106\n",
      "[823/1000]\n",
      "- Train Loss : 0.08978869440034032 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054061759263277054 Score : 0.9484702348709106\n",
      "[824/1000]\n",
      "- Train Loss : 0.08977113824544682 Score : 0.9346266984939575\n",
      "- Val Loss : 0.054033707827329636 Score : 0.9484702348709106\n",
      "[825/1000]\n",
      "- Train Loss : 0.08975381595599982 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05400572344660759 Score : 0.9484702348709106\n",
      "[826/1000]\n",
      "- Train Loss : 0.08973655347815818 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053977880626916885 Score : 0.9484702348709106\n",
      "[827/1000]\n",
      "- Train Loss : 0.08971925493743685 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053950123488903046 Score : 0.9484702348709106\n",
      "[828/1000]\n",
      "- Train Loss : 0.08970201983013087 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053922343999147415 Score : 0.9484702348709106\n",
      "[829/1000]\n",
      "- Train Loss : 0.08968481700867414 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0538947694003582 Score : 0.9484702348709106\n",
      "[830/1000]\n",
      "- Train Loss : 0.08966767394708262 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053867291659116745 Score : 0.9484702348709106\n",
      "[831/1000]\n",
      "- Train Loss : 0.0896506327100926 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05383976921439171 Score : 0.9484702348709106\n",
      "[832/1000]\n",
      "- Train Loss : 0.08963366680675083 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053812380880117416 Score : 0.9484702348709106\n",
      "[833/1000]\n",
      "- Train Loss : 0.08961665604470505 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05378515273332596 Score : 0.9484702348709106\n",
      "[834/1000]\n",
      "- Train Loss : 0.08959975321259764 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05375789478421211 Score : 0.9484702348709106\n",
      "[835/1000]\n",
      "- Train Loss : 0.08958292235102919 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05373086780309677 Score : 0.9484702348709106\n",
      "[836/1000]\n",
      "- Train Loss : 0.08956617023795843 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053703781217336655 Score : 0.9484702348709106\n",
      "[837/1000]\n",
      "- Train Loss : 0.08954941771096653 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053676825016736984 Score : 0.9484702348709106\n",
      "[838/1000]\n",
      "- Train Loss : 0.08953270528258549 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053649988025426865 Score : 0.9484702348709106\n",
      "[839/1000]\n",
      "- Train Loss : 0.08951608929783106 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05362318083643913 Score : 0.9484702348709106\n",
      "[840/1000]\n",
      "- Train Loss : 0.08949949907966787 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05359645560383797 Score : 0.9484702348709106\n",
      "[841/1000]\n",
      "- Train Loss : 0.08948303541789453 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05356985330581665 Score : 0.9484702348709106\n",
      "[842/1000]\n",
      "- Train Loss : 0.08946608193218708 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05354338511824608 Score : 0.9484702348709106\n",
      "[843/1000]\n",
      "- Train Loss : 0.08944980955372255 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05351712182164192 Score : 0.9484702348709106\n",
      "[844/1000]\n",
      "- Train Loss : 0.08943445452799399 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053490638732910156 Score : 0.9484702348709106\n",
      "[845/1000]\n",
      "- Train Loss : 0.0894183589455982 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05346427485346794 Score : 0.9484702348709106\n",
      "[846/1000]\n",
      "- Train Loss : 0.08940164543067415 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053437959402799606 Score : 0.9484702348709106\n",
      "[847/1000]\n",
      "- Train Loss : 0.08938494976609945 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053411904722452164 Score : 0.9484702348709106\n",
      "[848/1000]\n",
      "- Train Loss : 0.0893687258164088 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05338584631681442 Score : 0.9484702348709106\n",
      "[849/1000]\n",
      "- Train Loss : 0.08935272336627047 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05335989594459534 Score : 0.9484702348709106\n",
      "[850/1000]\n",
      "- Train Loss : 0.08933676950012644 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05333405360579491 Score : 0.9484702348709106\n",
      "[851/1000]\n",
      "- Train Loss : 0.08932083607133892 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053308166563510895 Score : 0.9484702348709106\n",
      "[852/1000]\n",
      "- Train Loss : 0.08930490207340983 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05328243598341942 Score : 0.9484702348709106\n",
      "[853/1000]\n",
      "- Train Loss : 0.08928896057315999 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05325676128268242 Score : 0.9484702348709106\n",
      "[854/1000]\n",
      "- Train Loss : 0.08927308649031652 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053231190890073776 Score : 0.9484702348709106\n",
      "[855/1000]\n",
      "- Train Loss : 0.08925728810330232 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05320579186081886 Score : 0.9484702348709106\n",
      "[856/1000]\n",
      "- Train Loss : 0.08924158362464772 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05318033695220947 Score : 0.9484702348709106\n",
      "[857/1000]\n",
      "- Train Loss : 0.08922584515271915 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05315503105521202 Score : 0.9484702348709106\n",
      "[858/1000]\n",
      "- Train Loss : 0.08921020441792077 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05312970653176308 Score : 0.9484702348709106\n",
      "[859/1000]\n",
      "- Train Loss : 0.0891946346188585 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05310458317399025 Score : 0.9484702348709106\n",
      "[860/1000]\n",
      "- Train Loss : 0.08917912390703957 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05307945981621742 Score : 0.9484702348709106\n",
      "[861/1000]\n",
      "- Train Loss : 0.08916359958756301 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053054455667734146 Score : 0.9484702348709106\n",
      "[862/1000]\n",
      "- Train Loss : 0.08914813606275453 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053029488772153854 Score : 0.9484702348709106\n",
      "[863/1000]\n",
      "- Train Loss : 0.08913277436254753 Score : 0.9346266984939575\n",
      "- Val Loss : 0.053004585206508636 Score : 0.9484702348709106\n",
      "[864/1000]\n",
      "- Train Loss : 0.08911738798229231 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05297982320189476 Score : 0.9484702348709106\n",
      "[865/1000]\n",
      "- Train Loss : 0.0891021775185234 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0529550276696682 Score : 0.9484702348709106\n",
      "[866/1000]\n",
      "- Train Loss : 0.08908693388932282 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05293041467666626 Score : 0.9484702348709106\n",
      "[867/1000]\n",
      "- Train Loss : 0.08907168513784806 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05290578678250313 Score : 0.9484702348709106\n",
      "[868/1000]\n",
      "- Train Loss : 0.0890561022485296 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052881307899951935 Score : 0.9484702348709106\n",
      "[869/1000]\n",
      "- Train Loss : 0.08904116486923562 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05285701900720596 Score : 0.9484702348709106\n",
      "[870/1000]\n",
      "- Train Loss : 0.08902695035147998 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05283264443278313 Score : 0.9484702348709106\n",
      "[871/1000]\n",
      "- Train Loss : 0.08901196076638168 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0528082512319088 Score : 0.9484702348709106\n",
      "[872/1000]\n",
      "- Train Loss : 0.08899675226873821 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0527840293943882 Score : 0.9484702348709106\n",
      "[873/1000]\n",
      "- Train Loss : 0.08898154542677933 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052759863436222076 Score : 0.9484702348709106\n",
      "[874/1000]\n",
      "- Train Loss : 0.08896661539458567 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05273578315973282 Score : 0.9484702348709106\n",
      "[875/1000]\n",
      "- Train Loss : 0.08895188435498211 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05271174758672714 Score : 0.9484702348709106\n",
      "[876/1000]\n",
      "- Train Loss : 0.08893712454785903 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052687790244817734 Score : 0.9484702348709106\n",
      "[877/1000]\n",
      "- Train Loss : 0.0889230574377709 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05266391485929489 Score : 0.9484702348709106\n",
      "[878/1000]\n",
      "- Train Loss : 0.08890827517542574 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05264001339673996 Score : 0.9484702348709106\n",
      "[879/1000]\n",
      "- Train Loss : 0.0888933802747892 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05261620506644249 Score : 0.9484702348709106\n",
      "[880/1000]\n",
      "- Train Loss : 0.08887863614492947 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05259265750646591 Score : 0.9484702348709106\n",
      "[881/1000]\n",
      "- Train Loss : 0.08886404547633396 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052569031715393066 Score : 0.9484702348709106\n",
      "[882/1000]\n",
      "- Train Loss : 0.08884960314672855 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05254554748535156 Score : 0.9484702348709106\n",
      "[883/1000]\n",
      "- Train Loss : 0.08883515186607838 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05252214893698692 Score : 0.9484702348709106\n",
      "[884/1000]\n",
      "- Train Loss : 0.08882077912696534 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05249875783920288 Score : 0.9484702348709106\n",
      "[885/1000]\n",
      "- Train Loss : 0.08880638010385963 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05247547850012779 Score : 0.9484702348709106\n",
      "[886/1000]\n",
      "- Train Loss : 0.0887920464285546 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05245218053460121 Score : 0.9484702348709106\n",
      "[887/1000]\n",
      "- Train Loss : 0.0887777977105644 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05242910608649254 Score : 0.9484702348709106\n",
      "[888/1000]\n",
      "- Train Loss : 0.08876358940162593 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05240599066019058 Score : 0.9484702348709106\n",
      "[889/1000]\n",
      "- Train Loss : 0.08874943774814407 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05238289013504982 Score : 0.9484702348709106\n",
      "[890/1000]\n",
      "- Train Loss : 0.0887352427881625 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0523599274456501 Score : 0.9484702348709106\n",
      "[891/1000]\n",
      "- Train Loss : 0.08872067633395393 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05233711749315262 Score : 0.9484702348709106\n",
      "[892/1000]\n",
      "- Train Loss : 0.08870693865335649 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052314478904008865 Score : 0.9484702348709106\n",
      "[893/1000]\n",
      "- Train Loss : 0.08869371371757653 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05229160934686661 Score : 0.9484702348709106\n",
      "[894/1000]\n",
      "- Train Loss : 0.08867971185180876 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05226884409785271 Score : 0.9484702348709106\n",
      "[895/1000]\n",
      "- Train Loss : 0.08866550023150113 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05224618315696716 Score : 0.9484702348709106\n",
      "[896/1000]\n",
      "- Train Loss : 0.08865137936340438 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05222359299659729 Score : 0.9484702348709106\n",
      "[897/1000]\n",
      "- Train Loss : 0.08863749261945486 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052201103419065475 Score : 0.9484702348709106\n",
      "[898/1000]\n",
      "- Train Loss : 0.08862378008456694 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052178673446178436 Score : 0.9484702348709106\n",
      "[899/1000]\n",
      "- Train Loss : 0.08861005047543181 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05215630307793617 Score : 0.9484702348709106\n",
      "[900/1000]\n",
      "- Train Loss : 0.08859637767697374 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0521339550614357 Score : 0.9484702348709106\n",
      "[901/1000]\n",
      "- Train Loss : 0.088582760343949 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052111685276031494 Score : 0.9484702348709106\n",
      "[902/1000]\n",
      "- Train Loss : 0.08856910834502843 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05208953469991684 Score : 0.9484702348709106\n",
      "[903/1000]\n",
      "- Train Loss : 0.0885554964447187 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05206742882728577 Score : 0.9484702348709106\n",
      "[904/1000]\n",
      "- Train Loss : 0.08854193136923844 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05204534903168678 Score : 0.9484702348709106\n",
      "[905/1000]\n",
      "- Train Loss : 0.08852847681070368 Score : 0.9346266984939575\n",
      "- Val Loss : 0.052023328840732574 Score : 0.9484702348709106\n",
      "[906/1000]\n",
      "- Train Loss : 0.08851495338603854 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05200142785906792 Score : 0.9484702348709106\n",
      "[907/1000]\n",
      "- Train Loss : 0.08850153737391035 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051979511976242065 Score : 0.9484702348709106\n",
      "[908/1000]\n",
      "- Train Loss : 0.08848817682721549 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051957763731479645 Score : 0.9484702348709106\n",
      "[909/1000]\n",
      "- Train Loss : 0.08847479429095984 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051935967057943344 Score : 0.9484702348709106\n",
      "[910/1000]\n",
      "- Train Loss : 0.08846154224334492 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05191423371434212 Score : 0.9484702348709106\n",
      "[911/1000]\n",
      "- Train Loss : 0.08844828098598453 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051892686635255814 Score : 0.9484702348709106\n",
      "[912/1000]\n",
      "- Train Loss : 0.08843468238289158 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051871173083782196 Score : 0.9484702348709106\n",
      "[913/1000]\n",
      "- Train Loss : 0.08842165840582715 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0518498457968235 Score : 0.9484702348709106\n",
      "[914/1000]\n",
      "- Train Loss : 0.08840929070073697 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05182838812470436 Score : 0.9484702348709106\n",
      "[915/1000]\n",
      "- Train Loss : 0.08839622646984127 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05180687457323074 Score : 0.9484702348709106\n",
      "[916/1000]\n",
      "- Train Loss : 0.08838288315261404 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05178549140691757 Score : 0.9484702348709106\n",
      "[917/1000]\n",
      "- Train Loss : 0.08836968620825145 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05176427960395813 Score : 0.9484702348709106\n",
      "[918/1000]\n",
      "- Train Loss : 0.08835667977109551 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05174308642745018 Score : 0.9484702348709106\n",
      "[919/1000]\n",
      "- Train Loss : 0.08834376931190491 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05172192305326462 Score : 0.9484702348709106\n",
      "[920/1000]\n",
      "- Train Loss : 0.08833089025898112 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051700886338949203 Score : 0.9484702348709106\n",
      "[921/1000]\n",
      "- Train Loss : 0.08831806594712867 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0516798198223114 Score : 0.9484702348709106\n",
      "[922/1000]\n",
      "- Train Loss : 0.08830523775476548 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05165886506438255 Score : 0.9484702348709106\n",
      "[923/1000]\n",
      "- Train Loss : 0.0882924255501065 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05163789540529251 Score : 0.9484702348709106\n",
      "[924/1000]\n",
      "- Train Loss : 0.08827971097909743 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051617030054330826 Score : 0.9484702348709106\n",
      "[925/1000]\n",
      "- Train Loss : 0.08826701518976027 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05159621313214302 Score : 0.9484702348709106\n",
      "[926/1000]\n",
      "- Train Loss : 0.0882543977867398 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051575515419244766 Score : 0.9484702348709106\n",
      "[927/1000]\n",
      "- Train Loss : 0.08824173324844903 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051554806530475616 Score : 0.9484702348709106\n",
      "[928/1000]\n",
      "- Train Loss : 0.08822912567605574 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05153423920273781 Score : 0.9484702348709106\n",
      "[929/1000]\n",
      "- Train Loss : 0.08821663379462229 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05151364579796791 Score : 0.9484702348709106\n",
      "[930/1000]\n",
      "- Train Loss : 0.08820409498487909 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05149316415190697 Score : 0.9484702348709106\n",
      "[931/1000]\n",
      "- Train Loss : 0.08819165432618724 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05147264152765274 Score : 0.9484702348709106\n",
      "[932/1000]\n",
      "- Train Loss : 0.08817922013501327 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05145226791501045 Score : 0.9484702348709106\n",
      "[933/1000]\n",
      "- Train Loss : 0.0881667969127496 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051431961357593536 Score : 0.9484702348709106\n",
      "[934/1000]\n",
      "- Train Loss : 0.08815445704385638 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051411665976047516 Score : 0.9484702348709106\n",
      "[935/1000]\n",
      "- Train Loss : 0.08814213621533579 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05139140412211418 Score : 0.9484702348709106\n",
      "[936/1000]\n",
      "- Train Loss : 0.08812982092301051 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051371265202760696 Score : 0.9484702348709106\n",
      "[937/1000]\n",
      "- Train Loss : 0.08811758215435678 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05135121941566467 Score : 0.9484702348709106\n",
      "[938/1000]\n",
      "- Train Loss : 0.08810537142886056 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05133110657334328 Score : 0.9484702348709106\n",
      "[939/1000]\n",
      "- Train Loss : 0.08809281689011389 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051311198621988297 Score : 0.9484702348709106\n",
      "[940/1000]\n",
      "- Train Loss : 0.08808080707159308 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05129135400056839 Score : 0.9484702348709106\n",
      "[941/1000]\n",
      "- Train Loss : 0.08806944736796948 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051271356642246246 Score : 0.9484702348709106\n",
      "[942/1000]\n",
      "- Train Loss : 0.0880574010209077 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05125150829553604 Score : 0.9484702348709106\n",
      "[943/1000]\n",
      "- Train Loss : 0.08804510078496403 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05123163014650345 Score : 0.9484702348709106\n",
      "[944/1000]\n",
      "- Train Loss : 0.08803287629658978 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05121190473437309 Score : 0.9484702348709106\n",
      "[945/1000]\n",
      "- Train Loss : 0.08802088370753659 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05119224637746811 Score : 0.9484702348709106\n",
      "[946/1000]\n",
      "- Train Loss : 0.08800904075097707 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0511726476252079 Score : 0.9484702348709106\n",
      "[947/1000]\n",
      "- Train Loss : 0.08799718335891764 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051153045147657394 Score : 0.9484702348709106\n",
      "[948/1000]\n",
      "- Train Loss : 0.0879853375049101 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051133543252944946 Score : 0.9484702348709106\n",
      "[949/1000]\n",
      "- Train Loss : 0.08797343892769681 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05111405998468399 Score : 0.9484702348709106\n",
      "[950/1000]\n",
      "- Train Loss : 0.08796169148344132 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0510946586728096 Score : 0.9484702348709106\n",
      "[951/1000]\n",
      "- Train Loss : 0.0879499539732933 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05107523873448372 Score : 0.9484702348709106\n",
      "[952/1000]\n",
      "- Train Loss : 0.08793822458634774 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05105594918131828 Score : 0.9484702348709106\n",
      "[953/1000]\n",
      "- Train Loss : 0.08792656504859526 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05103672295808792 Score : 0.9484702348709106\n",
      "[954/1000]\n",
      "- Train Loss : 0.08791495523312026 Score : 0.9346266984939575\n",
      "- Val Loss : 0.051017384976148605 Score : 0.9484702348709106\n",
      "[955/1000]\n",
      "- Train Loss : 0.08790333036126362 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05099828913807869 Score : 0.9484702348709106\n",
      "[956/1000]\n",
      "- Train Loss : 0.08789141652070814 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05097924917936325 Score : 0.9484702348709106\n",
      "[957/1000]\n",
      "- Train Loss : 0.08788003663842876 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05096036195755005 Score : 0.9484702348709106\n",
      "[958/1000]\n",
      "- Train Loss : 0.08786912143437399 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05094132199883461 Score : 0.9484702348709106\n",
      "[959/1000]\n",
      "- Train Loss : 0.08785768619014157 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050922293215990067 Score : 0.9484702348709106\n",
      "[960/1000]\n",
      "- Train Loss : 0.08784604626190332 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050903331488370895 Score : 0.9484702348709106\n",
      "[961/1000]\n",
      "- Train Loss : 0.08783436582113306 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05088449642062187 Score : 0.9484702348709106\n",
      "[962/1000]\n",
      "- Train Loss : 0.08782300192655788 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05086569860577583 Score : 0.9484702348709106\n",
      "[963/1000]\n",
      "- Train Loss : 0.08781178538791007 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05084693804383278 Score : 0.9484702348709106\n",
      "[964/1000]\n",
      "- Train Loss : 0.08780048327106568 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050828274339437485 Score : 0.9484702348709106\n",
      "[965/1000]\n",
      "- Train Loss : 0.08778922942777474 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05080962926149368 Score : 0.9484702348709106\n",
      "[966/1000]\n",
      "- Train Loss : 0.08777794355733527 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05079101398587227 Score : 0.9484702348709106\n",
      "[967/1000]\n",
      "- Train Loss : 0.08776674554165867 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050772447139024734 Score : 0.9484702348709106\n",
      "[968/1000]\n",
      "- Train Loss : 0.08775559424733122 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05075393244624138 Score : 0.9484702348709106\n",
      "[969/1000]\n",
      "- Train Loss : 0.08774443726158804 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05073552578687668 Score : 0.9484702348709106\n",
      "[970/1000]\n",
      "- Train Loss : 0.08773330707723896 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05071710795164108 Score : 0.9484702348709106\n",
      "[971/1000]\n",
      "- Train Loss : 0.08772224669034283 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050698697566986084 Score : 0.9484702348709106\n",
      "[972/1000]\n",
      "- Train Loss : 0.08771115339671572 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05068039149045944 Score : 0.9484702348709106\n",
      "[973/1000]\n",
      "- Train Loss : 0.08770017600100902 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05066215991973877 Score : 0.9484702348709106\n",
      "[974/1000]\n",
      "- Train Loss : 0.08768920812548862 Score : 0.9346266984939575\n",
      "- Val Loss : 0.0506439283490181 Score : 0.9484702348709106\n",
      "[975/1000]\n",
      "- Train Loss : 0.08767826767224404 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05062577873468399 Score : 0.9484702348709106\n",
      "[976/1000]\n",
      "- Train Loss : 0.08766735350299212 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05060770362615585 Score : 0.9484702348709106\n",
      "[977/1000]\n",
      "- Train Loss : 0.08765645899499457 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05058962106704712 Score : 0.9484702348709106\n",
      "[978/1000]\n",
      "- Train Loss : 0.0876455730241206 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050571661442518234 Score : 0.9484702348709106\n",
      "[979/1000]\n",
      "- Train Loss : 0.08763474370870325 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05055369809269905 Score : 0.9484702348709106\n",
      "[980/1000]\n",
      "- Train Loss : 0.08762367224941651 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05053579807281494 Score : 0.9484702348709106\n",
      "[981/1000]\n",
      "- Train Loss : 0.08761298894468281 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050518058240413666 Score : 0.9484702348709106\n",
      "[982/1000]\n",
      "- Train Loss : 0.08760297081122796 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050500206649303436 Score : 0.9484702348709106\n",
      "[983/1000]\n",
      "- Train Loss : 0.08759233655614986 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05048229917883873 Score : 0.9484702348709106\n",
      "[984/1000]\n",
      "- Train Loss : 0.08758140541613102 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05046456679701805 Score : 0.9484702348709106\n",
      "[985/1000]\n",
      "- Train Loss : 0.0875705340359774 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05044689401984215 Score : 0.9484702348709106\n",
      "[986/1000]\n",
      "- Train Loss : 0.08755998380689158 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05042927712202072 Score : 0.9484702348709106\n",
      "[987/1000]\n",
      "- Train Loss : 0.08754943249126275 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05041169375181198 Score : 0.9484702348709106\n",
      "[988/1000]\n",
      "- Train Loss : 0.08753892215382722 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05039411410689354 Score : 0.9484702348709106\n",
      "[989/1000]\n",
      "- Train Loss : 0.08752836794075039 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05037664249539375 Score : 0.9484702348709106\n",
      "[990/1000]\n",
      "- Train Loss : 0.08751792305459578 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05035916715860367 Score : 0.9484702348709106\n",
      "[991/1000]\n",
      "- Train Loss : 0.08750742896356517 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050341732800006866 Score : 0.9484702348709106\n",
      "[992/1000]\n",
      "- Train Loss : 0.08749702360687985 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05032437667250633 Score : 0.9484702348709106\n",
      "[993/1000]\n",
      "- Train Loss : 0.08748662983998656 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050307128578424454 Score : 0.9484702348709106\n",
      "[994/1000]\n",
      "- Train Loss : 0.08747602745683657 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05028983950614929 Score : 0.9484702348709106\n",
      "[995/1000]\n",
      "- Train Loss : 0.08746583325167497 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05027269944548607 Score : 0.9484702348709106\n",
      "[996/1000]\n",
      "- Train Loss : 0.08745606326394612 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05025549605488777 Score : 0.9484702348709106\n",
      "[997/1000]\n",
      "- Train Loss : 0.08744577137339446 Score : 0.9346266984939575\n",
      "- Val Loss : 0.05023825541138649 Score : 0.9484702348709106\n",
      "[998/1000]\n",
      "- Train Loss : 0.08743535566868053 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050221122801303864 Score : 0.9484702348709106\n",
      "[999/1000]\n",
      "- Train Loss : 0.0874249924802118 Score : 0.9346266984939575\n",
      "- Val Loss : 0.050204087048769 Score : 0.9484702348709106\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 손실값과 성능평가값 저장 필요 \n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]],[[],[]]\n",
    "CNT = len(train_dl)\n",
    "print(f'CNT : {CNT}')\n",
    " \n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    # 학습 모드로 모델 설정 \n",
    "    model.train()\n",
    "\n",
    "    # 배치 크기 만큼 데이터 로딩해서 학습 진행 \n",
    "    loss_total, score_total = 0,0\n",
    "    for feature_ts, target_ts in train_dl: # iris_dl -> train_dl \n",
    "\n",
    "        # 학습 진행 \n",
    "        pre_y = model(feature_ts)\n",
    "\n",
    "        # 손실 계산 : nn.CrossEntropyLoss 요구사항 : 정답/타겟은 0D 또는 1D, 타입은 long \n",
    "        loss = cross_loss(pre_y, target_ts.reshape(-1).long())\n",
    "        loss_total += loss.item() # tensor 라서 item으로 값 넣어야 함 \n",
    "\n",
    "        # 성능 평가 계산 \n",
    "        score = MulticlassF1Score(num_classes=3)(pre_y, target_ts.reshape(-1)) \n",
    "        score_total += score.item() # tensor 라서 item으로 값 넣어야 함 \n",
    "\n",
    "        # 최적화 진행 \n",
    "        optimizer.zero_grad()       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 에포크 당 검증기능 \n",
    "    # 모델 검증 모드 설정 \n",
    "    model.eval()\n",
    "    # 검증한 결과를 저장해야 함 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋 \n",
    "        val_feature_ts = torch.FloatTensor(val_ds.feature_df.values) # values쓰면 array 됨 -> torch사용 -> torch됨 \n",
    "        val_target_ts = torch.FloatTensor(val_ds.target_df.values)\n",
    "\n",
    "        # 평가 \n",
    "        pre_val =model(val_feature_ts)\n",
    "        \n",
    "        # 손실 \n",
    "        loss_val = cross_loss(pre_val, val_target_ts.reshape(-1).long())\n",
    "\n",
    "        # 성능 평가 \n",
    "        score_val = MulticlassF1Score(num_classes=3)(pre_val, val_target_ts.reshape(-1))\n",
    "\n",
    "\n",
    "    # for문 다 돌면 1 epoch 종료 \n",
    "    # 손실값과 성능평가값 저장 \n",
    "    LOSS_HISTORY[0].append(loss_total/CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/CNT)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 할 땐 아래 코드 사용 \n",
    "- no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.2778,  -1.1960,  -1.4552],\n",
      "        [ -5.2407,   2.9612,  -3.6799],\n",
      "        [ -9.7771,   3.4448,  -3.2478],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [-13.7002,  -0.2120,   1.1347],\n",
      "        [ -9.9959,   1.9890,  -1.7718],\n",
      "        [-11.6691,  -1.8092,   2.3163],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [-12.0476,  -3.1064,   3.6636],\n",
      "        [ -8.1003,   1.9100,  -2.0757],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [-12.0297,  -2.8622,   3.4154],\n",
      "        [ -9.8307,   2.8903,  -2.6916],\n",
      "        [ -9.1313,   1.4113,  -1.3760],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [ -8.0605,   2.9852,  -3.1413],\n",
      "        [ -8.6954,   0.9762,  -1.0370],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [ -7.8926,   1.4244,  -1.6429],\n",
      "        [ -8.4445,   0.6282,  -0.7454],\n",
      "        [-10.5768,   0.6542,  -0.3392],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [-11.9773,  -1.0195,   1.5837],\n",
      "        [ -8.2395,   2.6047,  -2.7308],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [ -8.9333,   2.2113,  -2.2102],\n",
      "        [ -9.9868,  -2.2843,   2.4399],\n",
      "        [ -9.2541,   2.0554,  -1.9884],\n",
      "        [-13.1277,  -3.2751,   4.0465],\n",
      "        [ -7.5711,   3.1047,  -3.3585],\n",
      "        [-11.1522,  -3.4729,   3.8190],\n",
      "        [-11.3202,  -3.6614,   3.9683],\n",
      "        [  4.2778,  -1.1960,  -1.4552],\n",
      "        [ -7.6071,   1.2115,  -1.4839],\n",
      "        [  4.2778,  -1.1960,  -1.4552]])\n"
     ]
    }
   ],
   "source": [
    "# 모델 검증 모드 설정 \n",
    "model.eval()\n",
    "# 검증한 결과를 저장해야 함 \n",
    "\n",
    "with torch.no_grad():\n",
    "    # 검증 데이터셋 \n",
    "    test_feature_ts = torch.FloatTensor(test_ds.feature_df.values) # values쓰면 array 됨 -> torch사용 -> torch됨 \n",
    "    test_target_ts = torch.FloatTensor(test_ds.target_df.values)\n",
    "\n",
    "    # 추론/평가 \n",
    "    pre_val =model(test_feature_ts)\n",
    "    print(pre_val)\n",
    "    \n",
    "    # 손실 \n",
    "    loss_test = cross_loss(pre_val, test_target_ts.reshape(-1).long())\n",
    "    score_test = MulticlassF1Score(num_classes=3)(pre_val, test_target_ts.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
