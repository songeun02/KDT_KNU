{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN 기반 이진분류 모델 구현 \n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐 : 4개 sepal_length, sepal_width, petal_length, petal_width\n",
    "- 타겟/라벨 : 1개 variety \n",
    "- 학습 방법 : 지도학습 - 회귀 \n",
    "- 알고리즘 : 인공신경망(ANN) -> MLP(Multi Layer Perceptron), DNN ( ) : 은닉층이 많은 구성 \n",
    "- 프레임 워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 모듈 로딩 및 데이터 준비\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 성능지표 \n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 처리 및 시각화  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import * \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch v.2.4.1\n",
      "pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 => 사용자 정의 함수로 구현하기 \n",
    "print(f'torch v.{torch.__version__}')\n",
    "print(f'pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩 \n",
    "DATA_FILE = '../DATA/iris.csv'\n",
    "\n",
    "# csv => DF\n",
    "iris_df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 변경 => 정수화, 클래스 3개 => 2개 \n",
    "iris_df['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels => {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n"
     ]
    }
   ],
   "source": [
    "# 타겟 정수화 \n",
    "labels = dict(zip(iris_df['variety'].unique().tolist(), range(3)))\n",
    "print(f'Labels => {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['variety'] =iris_df['variety'].replace(labels)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 모델 클래스 설계 및 정의 \n",
    "- 클래스 목적 : iris 데이터를 학습 및 추론 목적 \n",
    "- 클래스 이름 : iris_bs_model\n",
    "- 부모 클래스 : nn.Module \n",
    "- 매개 변수 : 층 별 입출력 개수 고정하기 때문에 필요 x \n",
    "- 속성 / 필드 : features_df, target_df, n_rows, n_features (df만들 때 사용)\n",
    "- 기능 / 역할 : __init__() : 모델 구조 생성 , forward : 순방향 학습 (오버라이딩(overriding조건 : 상속관계에서만 가능))\n",
    "- 클래스 구조 \n",
    "    - 입력층 : 입력 4개(피처)        출력 10개(퍼셉트론/뉴런 10개 존재)\n",
    "    - 은닉층 : 입력 10개            출력 5개 (퍼셉트론/뉴런 30개 존재)\n",
    "    - 출력층 : 입력 5개             출력 1개 (퍼셉트론/뉴런 1개 존재 : 이진분류)\n",
    "\n",
    "- 활성화 함수 \n",
    "    - 클래스 형태 ==> nn.MSELoss , nn.ReLU => _ _init_ _() 메서드 \n",
    "    - 함수 형태 => torch.nn.functional 아래에 => forward() 메서드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_bs_model(nn.Module):\n",
    "    \n",
    "    # 모델 구조 구성 및 인스턴스 생성 및 메서드 \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모델 구조 구성 \n",
    "        self.in_layer = nn.Linear(4,10)\n",
    "        self.hidden_layer = nn.Linear(10,5)\n",
    "        self.out_layer = nn.Linear(5,3)\n",
    "\n",
    "    # 순방향 학습 진행 메서드 \n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # 입력층 \n",
    "        y = self.in_layer(input_data) # f11w11 + f12w12 + f13w13 + b, ......., f101w101 + f102w102 + f103w103 + b\n",
    "        y = F.relu(y)             # relu => y 값의 범위 : 0 <= y \n",
    "\n",
    "        # 은닉층 : 10개의 숫자 값(>=0)\n",
    "        y = self.hidden_layer(y)  # f21w11 + f22w12 .... + f210w210 + b, ......., f230w201 + f230w202 ..... f230w210 + b\n",
    "        # 데이터 1개 기준\n",
    "        y = F.relu(y) \n",
    "\n",
    "        # 출력층 : 5개의 숫자 값(>=0)\n",
    "        # self.out_layer(y)         # f31w31 + ...... f330w330 + b\n",
    "        # 회귀라서 활성함수 사용 x -> 바로 return \n",
    "\n",
    "        return F.sigmoid(self.out_layer(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_bs_model(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성 \n",
    "model = iris_bs_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "iris_bs_model                            [17, 3]                   --\n",
       "├─Linear: 1-1                            [17, 10]                  50\n",
       "├─Linear: 1-2                            [17, 5]                   55\n",
       "├─Linear: 1-3                            [17, 3]                   18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 메모리 정보 확인 \n",
    "\n",
    "summary(model, input_size=(17,4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 데이터셋 클래스 설계 및 정의\n",
    "- 데이터셋 : iris.csv \n",
    "- 피쳐 개수 : 4개 \n",
    "- 타겟 개수 : 1개 \n",
    "- 클래스 이름 : iris_data_set\n",
    "- 부모 클래스 : utils.data.Dataset \n",
    "- 속성 / 필드 : feature_df, target_df \n",
    "- 필수 메서드 \n",
    "    - _ _init_ _(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정 \n",
    "    - _ _len_ _(self) : 데이터의 개수 반환 \n",
    "    - _ _ getItem_ _(self, index) : 특정 인덱스의 피쳐와 타겟 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_data_set(Dataset):\n",
    "    def __init__(self, feature_df, target_df):\n",
    "        self.feature_df = feature_df\n",
    "        self.target_df = target_df\n",
    "        self.n_rows = feature_df.shape[0]\n",
    "        self.n_features = feature_df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        # 텐서화 \n",
    "        feature_ts = torch.FloatTensor(self.feature_df.iloc[index].values) # 시리즈라서 values() 사용해서 numpy -> tensor \n",
    "        target_ts = torch.FloatTensor(self.target_df.iloc[index].values)\n",
    "                \n",
    "        # 피쳐와 타겟 반환 \n",
    "        return feature_ts, target_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 데이터셋 인스턴스 생성 \n",
    "\n",
    "# - DF 에서 피쳐와 타겟 추출 \n",
    "feature_df = iris_df[iris_df.columns[:-1]] # 2D \n",
    "target_df = iris_df[iris_df.columns[-1:]]  # 2D \n",
    "\n",
    "# 커스텀 데이터셋 인스턴스 생성 \n",
    "iris_ds = iris_data_set(feature_df, target_df)\n",
    "\n",
    "# 데이터로더 인스턴스 생성 \n",
    "iris_dl = DataLoader(iris_ds)\n",
    "for feature, label in iris_dl:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] 학습 준비 \n",
    "- 학습 횟수 : EPOCH (처음부터 끝까지 학습할 단위)\n",
    "- 배치 크기 : BATCH_SIZE (한 번에 학습할 데이터셋 양)\n",
    "- 위치 지정 : DEVICE (텐서 저장 및 실행 위치 (GPU, CPU))\n",
    "- 학습률 : 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001 ~ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정 \n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10 \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001 # hyper-parameter : 업데이트 간격  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스 / 객체 : 모델, 데이터셋, 최적화, 손실함수 ,(성능지표)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 \n",
    "model = iris_bs_model()\n",
    "\n",
    "# 데이터셋 인스턴스 \n",
    "iris_ds = iris_data_set(feature_df, target_df)\n",
    "\n",
    "# 데이터로더 인스턴스 \n",
    "iris_dl = DataLoader(iris_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# DS과 DL 인스턴스 \n",
    "# - 학습용, 검증용, 테스트용 데이터 분리 \n",
    "\n",
    "# 데이터셋 인스턴스 \n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_df, target_df, random_state = 1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state = 1)\n",
    "print(f'{x_train.shape} {x_test.shape} {x_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "print(f'{type(x_train)} {type(x_test)} {type(x_val)}')\n",
    "# iris_ds = iris_data_set(x_train, y_train)\n",
    "\n",
    "# 학습용, 검증용 테스트용 데이터셋 \n",
    "train_ds = iris_data_set(x_train, y_train)\n",
    "val_ds = iris_data_set(x_val, y_val)\n",
    "test_ds = iris_data_set(x_test, y_test)\n",
    "\n",
    "# 학습용 데이터로더 인스턴스 \n",
    "# iris_dl = DataLoader(iris_ds, batch_size=BATCH_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => w, b 텐서 즉, model.parameters() 전달 \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 다중분류 BinaryCrossEntropyLoss => CrossEntropyLoss \n",
    "#                            예측값은 확률값으로 전달 => softmax() AF 처리 후 전달 \n",
    "req_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] 학습 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl), train_dl.__len__() # __len__ 쓰면 len() 이 불림 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- >>> 모델 저장 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models 폴더 아래 프로젝트 폴더 아래 모델 파일 저장 \n",
    "import os \n",
    "\n",
    "# 저장 경로 \n",
    "SAVE_PATH = '../MODELS/MCF/'  \n",
    "\n",
    "# 저장 파일명 \n",
    "SAVE_FILE = 'model_train_wbs.pth'\n",
    "\n",
    "# 모델 구조 및 파라미터 모두 저장 파일명 \n",
    "SAVE_MODEL = 'model_all.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로상 폴더 존재 여부 체크 \n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)   # 폴더 / 폴더 / ... 하위 폴더까지 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.n_rows / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 4개 학습이 덜 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.4, 9, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math as m\n",
    "train_ds.n_rows / BATCH_SIZE, m.ceil(train_ds.n_rows/BATCH_SIZE), train_ds.feature_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT : 15.0\n",
      "[0/1000]\n",
      "- Train Loss : 0.6595599095026652 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1191288232803345 Score : 0.10101010650396347\n",
      "[1/1000]\n",
      "- Train Loss : 0.6592563231786092 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1186161041259766 Score : 0.10101010650396347\n",
      "[2/1000]\n",
      "- Train Loss : 0.6590458393096924 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1180331707000732 Score : 0.10101010650396347\n",
      "[3/1000]\n",
      "- Train Loss : 0.6588543097178141 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1174379587173462 Score : 0.10101010650396347\n",
      "[4/1000]\n",
      "- Train Loss : 0.6586674769719442 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1168550252914429 Score : 0.10101010650396347\n",
      "[5/1000]\n",
      "- Train Loss : 0.6584754387537638 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1162835359573364 Score : 0.10101010650396347\n",
      "[6/1000]\n",
      "- Train Loss : 0.6582649230957032 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1156997680664062 Score : 0.10101010650396347\n",
      "[7/1000]\n",
      "- Train Loss : 0.6580178340276083 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1150404214859009 Score : 0.10101010650396347\n",
      "[8/1000]\n",
      "- Train Loss : 0.6577065547307333 Score : 0.09865283071994782\n",
      "- Val Loss : 1.113727331161499 Score : 0.10101010650396347\n",
      "[9/1000]\n",
      "- Train Loss : 0.656295911471049 Score : 0.09865283071994782\n",
      "- Val Loss : 1.1035526990890503 Score : 0.10101010650396347\n",
      "[10/1000]\n",
      "- Train Loss : 0.6535344600677491 Score : 0.09865283071994782\n",
      "- Val Loss : 1.0952848196029663 Score : 0.10101010650396347\n",
      "[11/1000]\n",
      "- Train Loss : 0.6503363529841105 Score : 0.09865283071994782\n",
      "- Val Loss : 1.0887430906295776 Score : 0.10101010650396347\n",
      "[12/1000]\n",
      "- Train Loss : 0.6469537893931071 Score : 0.09865283071994782\n",
      "- Val Loss : 1.079635500907898 Score : 0.24444445967674255\n",
      "[13/1000]\n",
      "- Train Loss : 0.6435097773869832 Score : 0.2302102893590927\n",
      "- Val Loss : 1.0705811977386475 Score : 0.447957843542099\n",
      "[14/1000]\n",
      "- Train Loss : 0.6400103410085042 Score : 0.31248677968978883\n",
      "- Val Loss : 1.0620782375335693 Score : 0.4920635223388672\n",
      "[15/1000]\n",
      "- Train Loss : 0.6362209479014079 Score : 0.3227174162864685\n",
      "- Val Loss : 1.052794098854065 Score : 0.4920635223388672\n",
      "[16/1000]\n",
      "- Train Loss : 0.6321592330932617 Score : 0.3227174162864685\n",
      "- Val Loss : 1.0427182912826538 Score : 0.4920635223388672\n",
      "[17/1000]\n",
      "- Train Loss : 0.627878737449646 Score : 0.32712658643722536\n",
      "- Val Loss : 1.0322462320327759 Score : 0.4920635223388672\n",
      "[18/1000]\n",
      "- Train Loss : 0.6233419259389241 Score : 0.32712658643722536\n",
      "- Val Loss : 1.0214518308639526 Score : 0.4920635223388672\n",
      "[19/1000]\n",
      "- Train Loss : 0.6185311079025269 Score : 0.32712658643722536\n",
      "- Val Loss : 1.0097297430038452 Score : 0.4920635223388672\n",
      "[20/1000]\n",
      "- Train Loss : 0.6135383129119873 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9977899193763733 Score : 0.4920635223388672\n",
      "[21/1000]\n",
      "- Train Loss : 0.6083824475606282 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9858964085578918 Score : 0.4920635223388672\n",
      "[22/1000]\n",
      "- Train Loss : 0.6030731081962586 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9740579724311829 Score : 0.4920635223388672\n",
      "[23/1000]\n",
      "- Train Loss : 0.597706135114034 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9619339108467102 Score : 0.4920635223388672\n",
      "[24/1000]\n",
      "- Train Loss : 0.5921967625617981 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9490249752998352 Score : 0.4920635223388672\n",
      "[25/1000]\n",
      "- Train Loss : 0.5868738532066345 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9372626543045044 Score : 0.4920635223388672\n",
      "[26/1000]\n",
      "- Train Loss : 0.5816854079564412 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9267628788948059 Score : 0.4920635223388672\n",
      "[27/1000]\n",
      "- Train Loss : 0.5763978719711303 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9157058596611023 Score : 0.4920635223388672\n",
      "[28/1000]\n",
      "- Train Loss : 0.5712934891382854 Score : 0.32712658643722536\n",
      "- Val Loss : 0.9047239422798157 Score : 0.5555555820465088\n",
      "[29/1000]\n",
      "- Train Loss : 0.5663914521535237 Score : 0.4188437581062317\n",
      "- Val Loss : 0.8943338394165039 Score : 1.0\n",
      "[30/1000]\n",
      "- Train Loss : 0.561659840742747 Score : 0.5612506031990051\n",
      "- Val Loss : 0.8841779828071594 Score : 0.888888955116272\n",
      "[31/1000]\n",
      "- Train Loss : 0.5571644743283589 Score : 0.5418871323267619\n",
      "- Val Loss : 0.8746557831764221 Score : 0.888888955116272\n",
      "[32/1000]\n",
      "- Train Loss : 0.5528839190800985 Score : 0.49227649768193565\n",
      "- Val Loss : 0.8658532500267029 Score : 0.8171428442001343\n",
      "[33/1000]\n",
      "- Train Loss : 0.548786191145579 Score : 0.4634038885434469\n",
      "- Val Loss : 0.8574777245521545 Score : 0.7264957427978516\n",
      "[34/1000]\n",
      "- Train Loss : 0.5448686877886454 Score : 0.4171669125556946\n",
      "- Val Loss : 0.8494935631752014 Score : 0.7264957427978516\n",
      "[35/1000]\n",
      "- Train Loss : 0.5411393483479817 Score : 0.3849206447601318\n",
      "- Val Loss : 0.8418673872947693 Score : 0.7264957427978516\n",
      "[36/1000]\n",
      "- Train Loss : 0.5375766515731811 Score : 0.3613227645556132\n",
      "- Val Loss : 0.8345794677734375 Score : 0.7264957427978516\n",
      "[37/1000]\n",
      "- Train Loss : 0.5341875990231831 Score : 0.3456328988075256\n",
      "- Val Loss : 0.8276939392089844 Score : 0.7264957427978516\n",
      "[38/1000]\n",
      "- Train Loss : 0.5309422294298808 Score : 0.33266993761062624\n",
      "- Val Loss : 0.8211380243301392 Score : 0.604938268661499\n",
      "[39/1000]\n",
      "- Train Loss : 0.527825939655304 Score : 0.33266993761062624\n",
      "- Val Loss : 0.814844012260437 Score : 0.604938268661499\n",
      "[40/1000]\n",
      "- Train Loss : 0.5248421351114909 Score : 0.33266993761062624\n",
      "- Val Loss : 0.8088105320930481 Score : 0.604938268661499\n",
      "[41/1000]\n",
      "- Train Loss : 0.5219841678937276 Score : 0.33266993761062624\n",
      "- Val Loss : 0.8030298948287964 Score : 0.604938268661499\n",
      "[42/1000]\n",
      "- Train Loss : 0.5192416628201802 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7974593043327332 Score : 0.604938268661499\n",
      "[43/1000]\n",
      "- Train Loss : 0.5165963252385457 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7919760942459106 Score : 0.604938268661499\n",
      "[44/1000]\n",
      "- Train Loss : 0.5141013701756795 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7868242859840393 Score : 0.604938268661499\n",
      "[45/1000]\n",
      "- Train Loss : 0.5117116014162699 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7819518446922302 Score : 0.604938268661499\n",
      "[46/1000]\n",
      "- Train Loss : 0.5094309647878011 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7773385643959045 Score : 0.604938268661499\n",
      "[47/1000]\n",
      "- Train Loss : 0.5072675625483195 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7729787826538086 Score : 0.604938268661499\n",
      "[48/1000]\n",
      "- Train Loss : 0.5052194952964782 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7688771486282349 Score : 0.604938268661499\n",
      "[49/1000]\n",
      "- Train Loss : 0.5032846768697102 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7650235295295715 Score : 0.604938268661499\n",
      "[50/1000]\n",
      "- Train Loss : 0.5014608979225159 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7614222168922424 Score : 0.604938268661499\n",
      "[51/1000]\n",
      "- Train Loss : 0.4997438629468282 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7580742835998535 Score : 0.604938268661499\n",
      "[52/1000]\n",
      "- Train Loss : 0.498139230410258 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7550142407417297 Score : 0.604938268661499\n",
      "[53/1000]\n",
      "- Train Loss : 0.496634304523468 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7522562146186829 Score : 0.604938268661499\n",
      "[54/1000]\n",
      "- Train Loss : 0.495213786760966 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7497350573539734 Score : 0.604938268661499\n",
      "[55/1000]\n",
      "- Train Loss : 0.49388095140457156 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7474721670150757 Score : 0.604938268661499\n",
      "[56/1000]\n",
      "- Train Loss : 0.49262606700261435 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7454938292503357 Score : 0.604938268661499\n",
      "[57/1000]\n",
      "- Train Loss : 0.49143641789754233 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7437508702278137 Score : 0.604938268661499\n",
      "[58/1000]\n",
      "- Train Loss : 0.4903194268544515 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7422842979431152 Score : 0.604938268661499\n",
      "[59/1000]\n",
      "- Train Loss : 0.48925824960072833 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7408890128135681 Score : 0.604938268661499\n",
      "[60/1000]\n",
      "- Train Loss : 0.4882371187210083 Score : 0.33266993761062624\n",
      "- Val Loss : 0.739700436592102 Score : 0.604938268661499\n",
      "[61/1000]\n",
      "- Train Loss : 0.48725342750549316 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7386255860328674 Score : 0.604938268661499\n",
      "[62/1000]\n",
      "- Train Loss : 0.48629322052001955 Score : 0.33266993761062624\n",
      "- Val Loss : 0.7376183867454529 Score : 0.604938268661499\n",
      "[63/1000]\n",
      "- Train Loss : 0.48534313042958577 Score : 0.3456328988075256\n",
      "- Val Loss : 0.7366872429847717 Score : 0.7264957427978516\n",
      "[64/1000]\n",
      "- Train Loss : 0.4844085534413656 Score : 0.35277575651804605\n",
      "- Val Loss : 0.735854983329773 Score : 0.7264957427978516\n",
      "[65/1000]\n",
      "- Train Loss : 0.48348180055618284 Score : 0.35277575651804605\n",
      "- Val Loss : 0.735137403011322 Score : 0.7264957427978516\n",
      "[66/1000]\n",
      "- Train Loss : 0.4825750231742859 Score : 0.37155845562616985\n",
      "- Val Loss : 0.7344669699668884 Score : 0.7264957427978516\n",
      "[67/1000]\n",
      "- Train Loss : 0.48166870276133217 Score : 0.37910054127375287\n",
      "- Val Loss : 0.7338042259216309 Score : 0.7264957427978516\n",
      "[68/1000]\n",
      "- Train Loss : 0.48074564933776853 Score : 0.39232805172602336\n",
      "- Val Loss : 0.7332108616828918 Score : 0.7264957427978516\n",
      "[69/1000]\n",
      "- Train Loss : 0.47983104387919107 Score : 0.401375671227773\n",
      "- Val Loss : 0.7326902151107788 Score : 0.7264957427978516\n",
      "[70/1000]\n",
      "- Train Loss : 0.47891127268473305 Score : 0.41383358240127566\n",
      "- Val Loss : 0.7322527766227722 Score : 0.7264957427978516\n",
      "[71/1000]\n",
      "- Train Loss : 0.4779878298441569 Score : 0.43584416309992474\n",
      "- Val Loss : 0.7318843603134155 Score : 0.7264957427978516\n",
      "[72/1000]\n",
      "- Train Loss : 0.4770753741264343 Score : 0.4526342868804932\n",
      "- Val Loss : 0.7315787076950073 Score : 0.7264957427978516\n",
      "[73/1000]\n",
      "- Train Loss : 0.4761744856834412 Score : 0.4900705536206563\n",
      "- Val Loss : 0.7313052415847778 Score : 0.8171428442001343\n",
      "[74/1000]\n",
      "- Train Loss : 0.4752673864364624 Score : 0.4952557404836019\n",
      "- Val Loss : 0.7310377955436707 Score : 0.888888955116272\n",
      "[75/1000]\n",
      "- Train Loss : 0.47434068520863854 Score : 0.4952557404836019\n",
      "- Val Loss : 0.7307958006858826 Score : 0.888888955116272\n",
      "[76/1000]\n",
      "- Train Loss : 0.47341137329737343 Score : 0.4952557404836019\n",
      "- Val Loss : 0.7305951714515686 Score : 0.888888955116272\n",
      "[77/1000]\n",
      "- Train Loss : 0.4724907875061035 Score : 0.5120458642641703\n",
      "- Val Loss : 0.7304123640060425 Score : 0.888888955116272\n",
      "[78/1000]\n",
      "- Train Loss : 0.4715771754582723 Score : 0.5456098278363546\n",
      "- Val Loss : 0.7302601933479309 Score : 0.888888955116272\n",
      "[79/1000]\n",
      "- Train Loss : 0.47062298059463503 Score : 0.5456098278363546\n",
      "- Val Loss : 0.7301927208900452 Score : 0.888888955116272\n",
      "[80/1000]\n",
      "- Train Loss : 0.4697522600491842 Score : 0.5456098278363546\n",
      "- Val Loss : 0.730039656162262 Score : 0.888888955116272\n",
      "[81/1000]\n",
      "- Train Loss : 0.4687779227892558 Score : 0.5617636720339457\n",
      "- Val Loss : 0.7299936413764954 Score : 0.888888955116272\n",
      "[82/1000]\n",
      "- Train Loss : 0.46784884532292687 Score : 0.5617636720339457\n",
      "- Val Loss : 0.7299568057060242 Score : 0.888888955116272\n",
      "[83/1000]\n",
      "- Train Loss : 0.4669345736503601 Score : 0.5716402133305868\n",
      "- Val Loss : 0.7298933267593384 Score : 0.888888955116272\n",
      "[84/1000]\n",
      "- Train Loss : 0.4659966786702474 Score : 0.5716402133305868\n",
      "- Val Loss : 0.7298172116279602 Score : 0.888888955116272\n",
      "[85/1000]\n",
      "- Train Loss : 0.46504075129826866 Score : 0.5854673743247986\n",
      "- Val Loss : 0.7297531962394714 Score : 0.888888955116272\n",
      "[86/1000]\n",
      "- Train Loss : 0.4640833576520284 Score : 0.5854673743247986\n",
      "- Val Loss : 0.7296926379203796 Score : 0.888888955116272\n",
      "[87/1000]\n",
      "- Train Loss : 0.4631218075752258 Score : 0.5854673743247986\n",
      "- Val Loss : 0.7296470403671265 Score : 0.888888955116272\n",
      "[88/1000]\n",
      "- Train Loss : 0.46214495102564496 Score : 0.5854673743247986\n",
      "- Val Loss : 0.7296086549758911 Score : 0.888888955116272\n",
      "[89/1000]\n",
      "- Train Loss : 0.4611767371495565 Score : 0.5854673743247986\n",
      "- Val Loss : 0.7295445203781128 Score : 0.888888955116272\n",
      "[90/1000]\n",
      "- Train Loss : 0.4601881186167399 Score : 0.5802725712458293\n",
      "- Val Loss : 0.7294573187828064 Score : 0.888888955116272\n",
      "[91/1000]\n",
      "- Train Loss : 0.45917768478393556 Score : 0.5726535240809123\n",
      "- Val Loss : 0.7293514609336853 Score : 0.888888955116272\n",
      "[92/1000]\n",
      "- Train Loss : 0.45815243721008303 Score : 0.5578387101491292\n",
      "- Val Loss : 0.7292200922966003 Score : 0.9484702348709106\n",
      "[93/1000]\n",
      "- Train Loss : 0.4571134010950724 Score : 0.5578387101491292\n",
      "- Val Loss : 0.7290528416633606 Score : 0.9484702348709106\n",
      "[94/1000]\n",
      "- Train Loss : 0.45604952176411945 Score : 0.5523713390032451\n",
      "- Val Loss : 0.728857159614563 Score : 0.9484702348709106\n",
      "[95/1000]\n",
      "- Train Loss : 0.4549738049507141 Score : 0.546551231543223\n",
      "- Val Loss : 0.7286097407341003 Score : 0.9484702348709106\n",
      "[96/1000]\n",
      "- Train Loss : 0.45388407707214357 Score : 0.539355464776357\n",
      "- Val Loss : 0.7282947897911072 Score : 0.9484702348709106\n",
      "[97/1000]\n",
      "- Train Loss : 0.45276674032211306 Score : 0.539355464776357\n",
      "- Val Loss : 0.727921187877655 Score : 0.9484702348709106\n",
      "[98/1000]\n",
      "- Train Loss : 0.45162901480992634 Score : 0.5325829784075419\n",
      "- Val Loss : 0.7274900078773499 Score : 0.9484702348709106\n",
      "[99/1000]\n",
      "- Train Loss : 0.4504775365193685 Score : 0.5457816322644552\n",
      "- Val Loss : 0.7270293831825256 Score : 0.9030303359031677\n",
      "[100/1000]\n",
      "- Train Loss : 0.4493223508199056 Score : 0.5457816322644552\n",
      "- Val Loss : 0.7265053987503052 Score : 0.8614718914031982\n",
      "[101/1000]\n",
      "- Train Loss : 0.4481367031733195 Score : 0.5351996223131815\n",
      "- Val Loss : 0.7259097099304199 Score : 0.8614718914031982\n",
      "[102/1000]\n",
      "- Train Loss : 0.4469571948051453 Score : 0.474918794631958\n",
      "- Val Loss : 0.7252454161643982 Score : 0.9111111164093018\n",
      "[103/1000]\n",
      "- Train Loss : 0.44579036633173624 Score : 0.47011359930038454\n",
      "- Val Loss : 0.7245169281959534 Score : 0.9111111164093018\n",
      "[104/1000]\n",
      "- Train Loss : 0.44461151758829753 Score : 0.47011359930038454\n",
      "- Val Loss : 0.7237392067909241 Score : 0.8704453706741333\n",
      "[105/1000]\n",
      "- Train Loss : 0.4434352397918701 Score : 0.439161217212677\n",
      "- Val Loss : 0.7229214310646057 Score : 0.8704453706741333\n",
      "[106/1000]\n",
      "- Train Loss : 0.44227677981058755 Score : 0.431965450445811\n",
      "- Val Loss : 0.7220643758773804 Score : 0.8704453706741333\n",
      "[107/1000]\n",
      "- Train Loss : 0.4411402424176534 Score : 0.4217297593752543\n",
      "- Val Loss : 0.7211794853210449 Score : 0.75\n",
      "[108/1000]\n",
      "- Train Loss : 0.44002019564310707 Score : 0.4141107122103373\n",
      "- Val Loss : 0.7202783823013306 Score : 0.7071895599365234\n",
      "[109/1000]\n",
      "- Train Loss : 0.4389098683993022 Score : 0.3753853638966878\n",
      "- Val Loss : 0.7193743586540222 Score : 0.4920635223388672\n",
      "[110/1000]\n",
      "- Train Loss : 0.437874436378479 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7184468507766724 Score : 0.4920635223388672\n",
      "[111/1000]\n",
      "- Train Loss : 0.4368480126063029 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7175204157829285 Score : 0.4920635223388672\n",
      "[112/1000]\n",
      "- Train Loss : 0.435843821366628 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7166184186935425 Score : 0.4920635223388672\n",
      "[113/1000]\n",
      "- Train Loss : 0.43490074475606283 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7157326936721802 Score : 0.4920635223388672\n",
      "[114/1000]\n",
      "- Train Loss : 0.4339909553527832 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7148540616035461 Score : 0.4920635223388672\n",
      "[115/1000]\n",
      "- Train Loss : 0.4330947160720825 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7140063047409058 Score : 0.4920635223388672\n",
      "[116/1000]\n",
      "- Train Loss : 0.4322621822357178 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7131742835044861 Score : 0.4920635223388672\n",
      "[117/1000]\n",
      "- Train Loss : 0.431456728776296 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7123786807060242 Score : 0.4920635223388672\n",
      "[118/1000]\n",
      "- Train Loss : 0.43069275220235187 Score : 0.32712658643722536\n",
      "- Val Loss : 0.711614191532135 Score : 0.4920635223388672\n",
      "[119/1000]\n",
      "- Train Loss : 0.42996036211649574 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7108803987503052 Score : 0.4920635223388672\n",
      "[120/1000]\n",
      "- Train Loss : 0.4292598565419515 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7101748585700989 Score : 0.4920635223388672\n",
      "[121/1000]\n",
      "- Train Loss : 0.42859739462534585 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7094904780387878 Score : 0.4920635223388672\n",
      "[122/1000]\n",
      "- Train Loss : 0.42795055707295737 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7088446617126465 Score : 0.4920635223388672\n",
      "[123/1000]\n",
      "- Train Loss : 0.4273496667544047 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7082282304763794 Score : 0.4920635223388672\n",
      "[124/1000]\n",
      "- Train Loss : 0.42677486737569176 Score : 0.32712658643722536\n",
      "- Val Loss : 0.707639217376709 Score : 0.4920635223388672\n",
      "[125/1000]\n",
      "- Train Loss : 0.4262207309405009 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7070779800415039 Score : 0.4920635223388672\n",
      "[126/1000]\n",
      "- Train Loss : 0.4256929636001587 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7065426707267761 Score : 0.4920635223388672\n",
      "[127/1000]\n",
      "- Train Loss : 0.42519132296244305 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7060297727584839 Score : 0.4920635223388672\n",
      "[128/1000]\n",
      "- Train Loss : 0.42471514145533246 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7055334448814392 Score : 0.4920635223388672\n",
      "[129/1000]\n",
      "- Train Loss : 0.42424590984980265 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7050676345825195 Score : 0.4920635223388672\n",
      "[130/1000]\n",
      "- Train Loss : 0.4238109787305196 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7046233415603638 Score : 0.4920635223388672\n",
      "[131/1000]\n",
      "- Train Loss : 0.42339491844177246 Score : 0.32712658643722536\n",
      "- Val Loss : 0.704197883605957 Score : 0.4920635223388672\n",
      "[132/1000]\n",
      "- Train Loss : 0.4229909539222717 Score : 0.32712658643722536\n",
      "- Val Loss : 0.70379239320755 Score : 0.4920635223388672\n",
      "[133/1000]\n",
      "- Train Loss : 0.4226040442784627 Score : 0.32712658643722536\n",
      "- Val Loss : 0.703405499458313 Score : 0.4920635223388672\n",
      "[134/1000]\n",
      "- Train Loss : 0.4222355643908183 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7030352354049683 Score : 0.4920635223388672\n",
      "[135/1000]\n",
      "- Train Loss : 0.42188091278076173 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7026810050010681 Score : 0.4920635223388672\n",
      "[136/1000]\n",
      "- Train Loss : 0.42154038747151695 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7023406028747559 Score : 0.4920635223388672\n",
      "[137/1000]\n",
      "- Train Loss : 0.42121663093566897 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7020097374916077 Score : 0.4920635223388672\n",
      "[138/1000]\n",
      "- Train Loss : 0.42089443604151405 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7016997337341309 Score : 0.4920635223388672\n",
      "[139/1000]\n",
      "- Train Loss : 0.4205956657727559 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7014017701148987 Score : 0.4920635223388672\n",
      "[140/1000]\n",
      "- Train Loss : 0.42030653556187947 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7011147737503052 Score : 0.4920635223388672\n",
      "[141/1000]\n",
      "- Train Loss : 0.4200250705083211 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7008399963378906 Score : 0.4920635223388672\n",
      "[142/1000]\n",
      "- Train Loss : 0.4197539528210958 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7005766034126282 Score : 0.4920635223388672\n",
      "[143/1000]\n",
      "- Train Loss : 0.4194934924443563 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7003232836723328 Score : 0.4920635223388672\n",
      "[144/1000]\n",
      "- Train Loss : 0.4192420323689779 Score : 0.32712658643722536\n",
      "- Val Loss : 0.7000795006752014 Score : 0.4920635223388672\n",
      "[145/1000]\n",
      "- Train Loss : 0.4189988176027934 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6998449563980103 Score : 0.4920635223388672\n",
      "[146/1000]\n",
      "- Train Loss : 0.4187641064325968 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6996188759803772 Score : 0.4920635223388672\n",
      "[147/1000]\n",
      "- Train Loss : 0.41853713194529213 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6994005441665649 Score : 0.4920635223388672\n",
      "[148/1000]\n",
      "- Train Loss : 0.41831684509913125 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6991904377937317 Score : 0.4920635223388672\n",
      "[149/1000]\n",
      "- Train Loss : 0.41810431083043414 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6989877820014954 Score : 0.4920635223388672\n",
      "[150/1000]\n",
      "- Train Loss : 0.41789865096410117 Score : 0.32712658643722536\n",
      "- Val Loss : 0.698792040348053 Score : 0.4920635223388672\n",
      "[151/1000]\n",
      "- Train Loss : 0.417699138323466 Score : 0.32712658643722536\n",
      "- Val Loss : 0.698603093624115 Score : 0.4920635223388672\n",
      "[152/1000]\n",
      "- Train Loss : 0.4175056338310242 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6984208226203918 Score : 0.4920635223388672\n",
      "[153/1000]\n",
      "- Train Loss : 0.4173180421193441 Score : 0.32712658643722536\n",
      "- Val Loss : 0.698244571685791 Score : 0.4920635223388672\n",
      "[154/1000]\n",
      "- Train Loss : 0.417136013507843 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6980742812156677 Score : 0.4920635223388672\n",
      "[155/1000]\n",
      "- Train Loss : 0.416959273815155 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6979095339775085 Score : 0.4920635223388672\n",
      "[156/1000]\n",
      "- Train Loss : 0.41678756872812905 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6977500915527344 Score : 0.4920635223388672\n",
      "[157/1000]\n",
      "- Train Loss : 0.4166207273801168 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6975957155227661 Score : 0.4920635223388672\n",
      "[158/1000]\n",
      "- Train Loss : 0.41645854314168296 Score : 0.32712658643722536\n",
      "- Val Loss : 0.697446346282959 Score : 0.4920635223388672\n",
      "[159/1000]\n",
      "- Train Loss : 0.4163007974624634 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6973013877868652 Score : 0.4920635223388672\n",
      "[160/1000]\n",
      "- Train Loss : 0.4161473075548808 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6971610188484192 Score : 0.4920635223388672\n",
      "[161/1000]\n",
      "- Train Loss : 0.41599788665771487 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6970248818397522 Score : 0.4920635223388672\n",
      "[162/1000]\n",
      "- Train Loss : 0.4158523956934611 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6968927383422852 Score : 0.4920635223388672\n",
      "[163/1000]\n",
      "- Train Loss : 0.41571067571640014 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6967644691467285 Score : 0.4920635223388672\n",
      "[164/1000]\n",
      "- Train Loss : 0.41557253201802574 Score : 0.32712658643722536\n",
      "- Val Loss : 0.696639895439148 Score : 0.4920635223388672\n",
      "[165/1000]\n",
      "- Train Loss : 0.41543789704640705 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6965189576148987 Score : 0.4920635223388672\n",
      "[166/1000]\n",
      "- Train Loss : 0.415306556224823 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6964014768600464 Score : 0.4920635223388672\n",
      "[167/1000]\n",
      "- Train Loss : 0.4151784658432007 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6962870955467224 Score : 0.4920635223388672\n",
      "[168/1000]\n",
      "- Train Loss : 0.41505346695582074 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6961759328842163 Score : 0.4920635223388672\n",
      "[169/1000]\n",
      "- Train Loss : 0.41493141651153564 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6960676908493042 Score : 0.4920635223388672\n",
      "[170/1000]\n",
      "- Train Loss : 0.4148122549057007 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6959624290466309 Score : 0.4920635223388672\n",
      "[171/1000]\n",
      "- Train Loss : 0.41469585498174033 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6958598494529724 Score : 0.4920635223388672\n",
      "[172/1000]\n",
      "- Train Loss : 0.41458212534586586 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6957600712776184 Score : 0.4920635223388672\n",
      "[173/1000]\n",
      "- Train Loss : 0.4144709388415019 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6956626176834106 Score : 0.4920635223388672\n",
      "[174/1000]\n",
      "- Train Loss : 0.4143622676531474 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6955677270889282 Score : 0.4920635223388672\n",
      "[175/1000]\n",
      "- Train Loss : 0.41425597270329795 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6954751014709473 Score : 0.4920635223388672\n",
      "[176/1000]\n",
      "- Train Loss : 0.4141519784927368 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6953849792480469 Score : 0.4920635223388672\n",
      "[177/1000]\n",
      "- Train Loss : 0.41405024925867717 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6952969431877136 Score : 0.4920635223388672\n",
      "[178/1000]\n",
      "- Train Loss : 0.41395068168640137 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6952109932899475 Score : 0.4920635223388672\n",
      "[179/1000]\n",
      "- Train Loss : 0.41385319630304973 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6951271295547485 Score : 0.4920635223388672\n",
      "[180/1000]\n",
      "- Train Loss : 0.4137577931086222 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6950445175170898 Score : 0.4920635223388672\n",
      "[181/1000]\n",
      "- Train Loss : 0.4136654098828634 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6949619650840759 Score : 0.4920635223388672\n",
      "[182/1000]\n",
      "- Train Loss : 0.41357123851776123 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6948860287666321 Score : 0.4920635223388672\n",
      "[183/1000]\n",
      "- Train Loss : 0.4134822368621826 Score : 0.32712658643722536\n",
      "- Val Loss : 0.694810688495636 Score : 0.4920635223388672\n",
      "[184/1000]\n",
      "- Train Loss : 0.41339539289474486 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6947352290153503 Score : 0.4920635223388672\n",
      "[185/1000]\n",
      "- Train Loss : 0.4133090337117513 Score : 0.32712658643722536\n",
      "- Val Loss : 0.694661021232605 Score : 0.4920635223388672\n",
      "[186/1000]\n",
      "- Train Loss : 0.41322505474090576 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6945875287055969 Score : 0.4920635223388672\n",
      "[187/1000]\n",
      "- Train Loss : 0.4131397763888041 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6945199966430664 Score : 0.4920635223388672\n",
      "[188/1000]\n",
      "- Train Loss : 0.4130591909090678 Score : 0.32712658643722536\n",
      "- Val Loss : 0.694452702999115 Score : 0.4920635223388672\n",
      "[189/1000]\n",
      "- Train Loss : 0.4129804531733195 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6943849921226501 Score : 0.4920635223388672\n",
      "[190/1000]\n",
      "- Train Loss : 0.4129020094871521 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6943182349205017 Score : 0.4920635223388672\n",
      "[191/1000]\n",
      "- Train Loss : 0.4128255724906921 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6942521333694458 Score : 0.4920635223388672\n",
      "[192/1000]\n",
      "- Train Loss : 0.41274800300598147 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6941914558410645 Score : 0.4920635223388672\n",
      "[193/1000]\n",
      "- Train Loss : 0.41267457405726116 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6941303014755249 Score : 0.4920635223388672\n",
      "[194/1000]\n",
      "- Train Loss : 0.41260356108347573 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6940671801567078 Score : 0.4920635223388672\n",
      "[195/1000]\n",
      "- Train Loss : 0.41253003279368083 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6940093636512756 Score : 0.4920635223388672\n",
      "[196/1000]\n",
      "- Train Loss : 0.41245988607406614 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6939526200294495 Score : 0.4920635223388672\n",
      "[197/1000]\n",
      "- Train Loss : 0.4123916586240133 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6938949227333069 Score : 0.4920635223388672\n",
      "[198/1000]\n",
      "- Train Loss : 0.4123246510823568 Score : 0.32712658643722536\n",
      "- Val Loss : 0.693836510181427 Score : 0.4920635223388672\n",
      "[199/1000]\n",
      "- Train Loss : 0.4122559348742167 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6937837600708008 Score : 0.4920635223388672\n",
      "[200/1000]\n",
      "- Train Loss : 0.4121907273928324 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6937307715415955 Score : 0.4920635223388672\n",
      "[201/1000]\n",
      "- Train Loss : 0.4121277729670207 Score : 0.32712658643722536\n",
      "- Val Loss : 0.693675696849823 Score : 0.4920635223388672\n",
      "[202/1000]\n",
      "- Train Loss : 0.4120627880096436 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6936246156692505 Score : 0.4920635223388672\n",
      "[203/1000]\n",
      "- Train Loss : 0.41200120449066163 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6935731172561646 Score : 0.4920635223388672\n",
      "[204/1000]\n",
      "- Train Loss : 0.4119389017422994 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6935247182846069 Score : 0.4920635223388672\n",
      "[205/1000]\n",
      "- Train Loss : 0.411879030863444 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6934758424758911 Score : 0.4920635223388672\n",
      "[206/1000]\n",
      "- Train Loss : 0.41182071367899575 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6934255361557007 Score : 0.4920635223388672\n",
      "[207/1000]\n",
      "- Train Loss : 0.41176071961720784 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6933791041374207 Score : 0.4920635223388672\n",
      "[208/1000]\n",
      "- Train Loss : 0.41170392831166586 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6933321356773376 Score : 0.4920635223388672\n",
      "[209/1000]\n",
      "- Train Loss : 0.41164676745732626 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6932868957519531 Score : 0.4920635223388672\n",
      "[210/1000]\n",
      "- Train Loss : 0.4115915616353353 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6932412385940552 Score : 0.4920635223388672\n",
      "[211/1000]\n",
      "- Train Loss : 0.4115357041358948 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6931983232498169 Score : 0.4920635223388672\n",
      "[212/1000]\n",
      "- Train Loss : 0.41148258447647096 Score : 0.32712658643722536\n",
      "- Val Loss : 0.693153977394104 Score : 0.4920635223388672\n",
      "[213/1000]\n",
      "- Train Loss : 0.41142874558766684 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6931114196777344 Score : 0.4920635223388672\n",
      "[214/1000]\n",
      "- Train Loss : 0.4113766312599182 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6930685639381409 Score : 0.4920635223388672\n",
      "[215/1000]\n",
      "- Train Loss : 0.4113246122996012 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6930274367332458 Score : 0.4920635223388672\n",
      "[216/1000]\n",
      "- Train Loss : 0.41127287546793617 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6929885149002075 Score : 0.4920635223388672\n",
      "[217/1000]\n",
      "- Train Loss : 0.41122414271036783 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6929470300674438 Score : 0.4920635223388672\n",
      "[218/1000]\n",
      "- Train Loss : 0.4111735423405965 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6929084062576294 Score : 0.4920635223388672\n",
      "[219/1000]\n",
      "- Train Loss : 0.41112536589304605 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6928688287734985 Score : 0.4920635223388672\n",
      "[220/1000]\n",
      "- Train Loss : 0.4110772768656413 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6928301453590393 Score : 0.4920635223388672\n",
      "[221/1000]\n",
      "- Train Loss : 0.41102949778238934 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6927928924560547 Score : 0.4920635223388672\n",
      "[222/1000]\n",
      "- Train Loss : 0.410983141263326 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6927551627159119 Score : 0.4920635223388672\n",
      "[223/1000]\n",
      "- Train Loss : 0.41093701918919884 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6927183866500854 Score : 0.4920635223388672\n",
      "[224/1000]\n",
      "- Train Loss : 0.41089054346084597 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6926846504211426 Score : 0.4920635223388672\n",
      "[225/1000]\n",
      "- Train Loss : 0.41084678173065187 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6926485300064087 Score : 0.4920635223388672\n",
      "[226/1000]\n",
      "- Train Loss : 0.4108027974764506 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6926122307777405 Score : 0.4920635223388672\n",
      "[227/1000]\n",
      "- Train Loss : 0.41075841983159384 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6925784349441528 Score : 0.4920635223388672\n",
      "[228/1000]\n",
      "- Train Loss : 0.4107154607772827 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6925449967384338 Score : 0.4920635223388672\n",
      "[229/1000]\n",
      "- Train Loss : 0.4106729030609131 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6925122141838074 Score : 0.4920635223388672\n",
      "[230/1000]\n",
      "- Train Loss : 0.41063131888707477 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6924794316291809 Score : 0.4920635223388672\n",
      "[231/1000]\n",
      "- Train Loss : 0.4105894923210144 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6924481987953186 Score : 0.4920635223388672\n",
      "[232/1000]\n",
      "- Train Loss : 0.4105494578679403 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6924151182174683 Score : 0.4920635223388672\n",
      "[233/1000]\n",
      "- Train Loss : 0.4105091094970703 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6923823952674866 Score : 0.4920635223388672\n",
      "[234/1000]\n",
      "- Train Loss : 0.41046882470448814 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6923516988754272 Score : 0.4920635223388672\n",
      "[235/1000]\n",
      "- Train Loss : 0.4104292114575704 Score : 0.32712658643722536\n",
      "- Val Loss : 0.692322313785553 Score : 0.4920635223388672\n",
      "[236/1000]\n",
      "- Train Loss : 0.41039069096247355 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6922926306724548 Score : 0.4920635223388672\n",
      "[237/1000]\n",
      "- Train Loss : 0.41035277048746743 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6922622323036194 Score : 0.4920635223388672\n",
      "[238/1000]\n",
      "- Train Loss : 0.4103147824605306 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6922325491905212 Score : 0.4920635223388672\n",
      "[239/1000]\n",
      "- Train Loss : 0.41027700901031494 Score : 0.32712658643722536\n",
      "- Val Loss : 0.692204475402832 Score : 0.4920635223388672\n",
      "[240/1000]\n",
      "- Train Loss : 0.4102402170499166 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6921765208244324 Score : 0.4920635223388672\n",
      "[241/1000]\n",
      "- Train Loss : 0.41020410855611167 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6921477913856506 Score : 0.4920635223388672\n",
      "[242/1000]\n",
      "- Train Loss : 0.41016798416773476 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6921197772026062 Score : 0.4920635223388672\n",
      "[243/1000]\n",
      "- Train Loss : 0.4101322770118713 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6920925378799438 Score : 0.4920635223388672\n",
      "[244/1000]\n",
      "- Train Loss : 0.4100973884264628 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6920648217201233 Score : 0.4920635223388672\n",
      "[245/1000]\n",
      "- Train Loss : 0.410062305132548 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6920387148857117 Score : 0.4920635223388672\n",
      "[246/1000]\n",
      "- Train Loss : 0.41002809604008994 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6920127272605896 Score : 0.4920635223388672\n",
      "[247/1000]\n",
      "- Train Loss : 0.40999399026234945 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6919875144958496 Score : 0.4920635223388672\n",
      "[248/1000]\n",
      "- Train Loss : 0.40996066729227704 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6919617652893066 Score : 0.4920635223388672\n",
      "[249/1000]\n",
      "- Train Loss : 0.4099274714787801 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6919364929199219 Score : 0.4920635223388672\n",
      "[250/1000]\n",
      "- Train Loss : 0.4098944385846456 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6919123530387878 Score : 0.4920635223388672\n",
      "[251/1000]\n",
      "- Train Loss : 0.4098620971043905 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6918883323669434 Score : 0.4920635223388672\n",
      "[252/1000]\n",
      "- Train Loss : 0.40983031193415326 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6918635368347168 Score : 0.4920635223388672\n",
      "[253/1000]\n",
      "- Train Loss : 0.409798534711202 Score : 0.32712658643722536\n",
      "- Val Loss : 0.691839337348938 Score : 0.4920635223388672\n",
      "[254/1000]\n",
      "- Train Loss : 0.40976689259211224 Score : 0.32712658643722536\n",
      "- Val Loss : 0.691816508769989 Score : 0.4920635223388672\n",
      "[255/1000]\n",
      "- Train Loss : 0.40973612864812214 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6917932629585266 Score : 0.4920635223388672\n",
      "[256/1000]\n",
      "- Train Loss : 0.4097055196762085 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6917700171470642 Score : 0.4920635223388672\n",
      "[257/1000]\n",
      "- Train Loss : 0.4096749981244405 Score : 0.32712658643722536\n",
      "- Val Loss : 0.691747784614563 Score : 0.4920635223388672\n",
      "[258/1000]\n",
      "- Train Loss : 0.4096452275911967 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6917251944541931 Score : 0.4920635223388672\n",
      "[259/1000]\n",
      "- Train Loss : 0.40961541334788004 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6917032599449158 Score : 0.4920635223388672\n",
      "[260/1000]\n",
      "- Train Loss : 0.409586238861084 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6916810870170593 Score : 0.4920635223388672\n",
      "[261/1000]\n",
      "- Train Loss : 0.4095571756362915 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6916592717170715 Score : 0.4920635223388672\n",
      "[262/1000]\n",
      "- Train Loss : 0.4095282435417175 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6916383504867554 Score : 0.4920635223388672\n",
      "[263/1000]\n",
      "- Train Loss : 0.4095000068346659 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6916170716285706 Score : 0.4920635223388672\n",
      "[264/1000]\n",
      "- Train Loss : 0.40947190125783284 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6915958523750305 Score : 0.4920635223388672\n",
      "[265/1000]\n",
      "- Train Loss : 0.40944384733835854 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6915756464004517 Score : 0.4920635223388672\n",
      "[266/1000]\n",
      "- Train Loss : 0.40941643714904785 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6915550231933594 Score : 0.4920635223388672\n",
      "[267/1000]\n",
      "- Train Loss : 0.4093892176946004 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6915346384048462 Score : 0.4920635223388672\n",
      "[268/1000]\n",
      "- Train Loss : 0.40936214526494347 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6915146112442017 Score : 0.4920635223388672\n",
      "[269/1000]\n",
      "- Train Loss : 0.4093352993329366 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6914954781532288 Score : 0.4920635223388672\n",
      "[270/1000]\n",
      "- Train Loss : 0.40930904547373453 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6914756894111633 Score : 0.4920635223388672\n",
      "[271/1000]\n",
      "- Train Loss : 0.40928287903467814 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6914559602737427 Score : 0.4920635223388672\n",
      "[272/1000]\n",
      "- Train Loss : 0.4092567563056946 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6914373636245728 Score : 0.4920635223388672\n",
      "[273/1000]\n",
      "- Train Loss : 0.4092312296231588 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6914182901382446 Score : 0.4920635223388672\n",
      "[274/1000]\n",
      "- Train Loss : 0.40920584201812743 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6913993954658508 Score : 0.4920635223388672\n",
      "[275/1000]\n",
      "- Train Loss : 0.40918062925338744 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6913807988166809 Score : 0.4920635223388672\n",
      "[276/1000]\n",
      "- Train Loss : 0.40915554761886597 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6913629770278931 Score : 0.4920635223388672\n",
      "[277/1000]\n",
      "- Train Loss : 0.4091310699780782 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6913447380065918 Score : 0.4920635223388672\n",
      "[278/1000]\n",
      "- Train Loss : 0.4091066400210063 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6913266181945801 Score : 0.4920635223388672\n",
      "[279/1000]\n",
      "- Train Loss : 0.4090823610623678 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6913087964057922 Score : 0.4920635223388672\n",
      "[280/1000]\n",
      "- Train Loss : 0.40905823310216266 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6912918090820312 Score : 0.4920635223388672\n",
      "[281/1000]\n",
      "- Train Loss : 0.40903465350468954 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6912742853164673 Score : 0.4920635223388672\n",
      "[282/1000]\n",
      "- Train Loss : 0.4090112010637919 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6912566423416138 Score : 0.4920635223388672\n",
      "[283/1000]\n",
      "- Train Loss : 0.4089878718058268 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6912392973899841 Score : 0.4920635223388672\n",
      "[284/1000]\n",
      "- Train Loss : 0.4089645862579346 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6912228465080261 Score : 0.4920635223388672\n",
      "[285/1000]\n",
      "- Train Loss : 0.40894184112548826 Score : 0.32712658643722536\n",
      "- Val Loss : 0.691206157207489 Score : 0.4920635223388672\n",
      "[286/1000]\n",
      "- Train Loss : 0.40891921520233154 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6911894083023071 Score : 0.4920635223388672\n",
      "[287/1000]\n",
      "- Train Loss : 0.4088967243830363 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6911730766296387 Score : 0.4920635223388672\n",
      "[288/1000]\n",
      "- Train Loss : 0.4088744560877482 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6911562085151672 Score : 0.4920635223388672\n",
      "[289/1000]\n",
      "- Train Loss : 0.40885292291641234 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6911370158195496 Score : 0.4920635223388672\n",
      "[290/1000]\n",
      "- Train Loss : 0.40883058309555054 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6911205053329468 Score : 0.4920635223388672\n",
      "[291/1000]\n",
      "- Train Loss : 0.4088087280591329 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6911054253578186 Score : 0.4920635223388672\n",
      "[292/1000]\n",
      "- Train Loss : 0.4087873617808024 Score : 0.32712658643722536\n",
      "- Val Loss : 0.69109046459198 Score : 0.4920635223388672\n",
      "[293/1000]\n",
      "- Train Loss : 0.4087662617365519 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6910750269889832 Score : 0.4920635223388672\n",
      "[294/1000]\n",
      "- Train Loss : 0.408745265007019 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6910598874092102 Score : 0.4920635223388672\n",
      "[295/1000]\n",
      "- Train Loss : 0.40872432390848795 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6910449862480164 Score : 0.4920635223388672\n",
      "[296/1000]\n",
      "- Train Loss : 0.4087037960688273 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6910300850868225 Score : 0.4920635223388672\n",
      "[297/1000]\n",
      "- Train Loss : 0.4086832324663798 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6910155415534973 Score : 0.4920635223388672\n",
      "[298/1000]\n",
      "- Train Loss : 0.408663018544515 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6910006403923035 Score : 0.4920635223388672\n",
      "[299/1000]\n",
      "- Train Loss : 0.40864288806915283 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6909860968589783 Score : 0.4920635223388672\n",
      "[300/1000]\n",
      "- Train Loss : 0.4086228609085083 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690971851348877 Score : 0.4920635223388672\n",
      "[301/1000]\n",
      "- Train Loss : 0.40860299666722616 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6909581422805786 Score : 0.4920635223388672\n",
      "[302/1000]\n",
      "- Train Loss : 0.4085835178693136 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6909440159797668 Score : 0.4920635223388672\n",
      "[303/1000]\n",
      "- Train Loss : 0.40856407483418783 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6909298300743103 Score : 0.4920635223388672\n",
      "[304/1000]\n",
      "- Train Loss : 0.40854472319285073 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6909160017967224 Score : 0.4920635223388672\n",
      "[305/1000]\n",
      "- Train Loss : 0.40852546294530234 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690902829170227 Score : 0.4920635223388672\n",
      "[306/1000]\n",
      "- Train Loss : 0.4085066239039103 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908891797065735 Score : 0.4920635223388672\n",
      "[307/1000]\n",
      "- Train Loss : 0.40848782857259114 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908755898475647 Score : 0.4920635223388672\n",
      "[308/1000]\n",
      "- Train Loss : 0.4084691087404887 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908623576164246 Score : 0.4920635223388672\n",
      "[309/1000]\n",
      "- Train Loss : 0.4084505597750346 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908493638038635 Score : 0.4920635223388672\n",
      "[310/1000]\n",
      "- Train Loss : 0.4084321141242981 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908367872238159 Score : 0.4920635223388672\n",
      "[311/1000]\n",
      "- Train Loss : 0.40841402212778727 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908237338066101 Score : 0.4920635223388672\n",
      "[312/1000]\n",
      "- Train Loss : 0.4083959539731344 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6908106803894043 Score : 0.4920635223388672\n",
      "[313/1000]\n",
      "- Train Loss : 0.4083779732386271 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907979846000671 Score : 0.4920635223388672\n",
      "[314/1000]\n",
      "- Train Loss : 0.40836017529169716 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907857060432434 Score : 0.4920635223388672\n",
      "[315/1000]\n",
      "- Train Loss : 0.408342448870341 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907736659049988 Score : 0.4920635223388672\n",
      "[316/1000]\n",
      "- Train Loss : 0.4083250403404236 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907610893249512 Score : 0.4920635223388672\n",
      "[317/1000]\n",
      "- Train Loss : 0.40830769141515094 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907487511634827 Score : 0.4920635223388672\n",
      "[318/1000]\n",
      "- Train Loss : 0.40829039812088014 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907366514205933 Score : 0.4920635223388672\n",
      "[319/1000]\n",
      "- Train Loss : 0.40827327966690063 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690724790096283 Score : 0.4920635223388672\n",
      "[320/1000]\n",
      "- Train Loss : 0.40825631221135456 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907129287719727 Score : 0.4920635223388672\n",
      "[321/1000]\n",
      "- Train Loss : 0.408239483833313 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6907011866569519 Score : 0.4920635223388672\n",
      "[322/1000]\n",
      "- Train Loss : 0.40822269519170123 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906898617744446 Score : 0.4920635223388672\n",
      "[323/1000]\n",
      "- Train Loss : 0.4082062085469564 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906781196594238 Score : 0.4920635223388672\n",
      "[324/1000]\n",
      "- Train Loss : 0.4081897258758545 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906664967536926 Score : 0.4920635223388672\n",
      "[325/1000]\n",
      "- Train Loss : 0.4081733504931132 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906551718711853 Score : 0.4920635223388672\n",
      "[326/1000]\n",
      "- Train Loss : 0.40815713405609133 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906439661979675 Score : 0.4920635223388672\n",
      "[327/1000]\n",
      "- Train Loss : 0.4081410487492879 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906329393386841 Score : 0.4920635223388672\n",
      "[328/1000]\n",
      "- Train Loss : 0.4081250588099162 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906219124794006 Score : 0.4920635223388672\n",
      "[329/1000]\n",
      "- Train Loss : 0.4081092119216919 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690610945224762 Score : 0.4920635223388672\n",
      "[330/1000]\n",
      "- Train Loss : 0.4080934524536133 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6906000971794128 Score : 0.4920635223388672\n",
      "[331/1000]\n",
      "- Train Loss : 0.4080778439839681 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905893683433533 Score : 0.4920635223388672\n",
      "[332/1000]\n",
      "- Train Loss : 0.40806233088175453 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905787587165833 Score : 0.4920635223388672\n",
      "[333/1000]\n",
      "- Train Loss : 0.4080470601717631 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905675530433655 Score : 0.4920635223388672\n",
      "[334/1000]\n",
      "- Train Loss : 0.4080317179361979 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905567049980164 Score : 0.4920635223388672\n",
      "[335/1000]\n",
      "- Train Loss : 0.40801648298899335 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690546452999115 Score : 0.4920635223388672\n",
      "[336/1000]\n",
      "- Train Loss : 0.40800135135650634 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690536618232727 Score : 0.4920635223388672\n",
      "[337/1000]\n",
      "- Train Loss : 0.40798650979995726 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905263066291809 Score : 0.4920635223388672\n",
      "[338/1000]\n",
      "- Train Loss : 0.40797168016433716 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905158758163452 Score : 0.4920635223388672\n",
      "[339/1000]\n",
      "- Train Loss : 0.40795689821243286 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6905058026313782 Score : 0.4920635223388672\n",
      "[340/1000]\n",
      "- Train Loss : 0.4079422275225321 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690496027469635 Score : 0.4920635223388672\n",
      "[341/1000]\n",
      "- Train Loss : 0.4079276839892069 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904860734939575 Score : 0.4920635223388672\n",
      "[342/1000]\n",
      "- Train Loss : 0.40791327158610025 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904763579368591 Score : 0.4920635223388672\n",
      "[343/1000]\n",
      "- Train Loss : 0.4078989346822103 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690466582775116 Score : 0.4920635223388672\n",
      "[344/1000]\n",
      "- Train Loss : 0.40788469314575193 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904569864273071 Score : 0.4920635223388672\n",
      "[345/1000]\n",
      "- Train Loss : 0.4078705628712972 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904475092887878 Score : 0.4920635223388672\n",
      "[346/1000]\n",
      "- Train Loss : 0.40785651206970214 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904380917549133 Score : 0.4920635223388672\n",
      "[347/1000]\n",
      "- Train Loss : 0.4078425645828247 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904287338256836 Score : 0.4920635223388672\n",
      "[348/1000]\n",
      "- Train Loss : 0.4078287164370219 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904193758964539 Score : 0.4920635223388672\n",
      "[349/1000]\n",
      "- Train Loss : 0.4078149477640788 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6904101967811584 Score : 0.4920635223388672\n",
      "[350/1000]\n",
      "- Train Loss : 0.40780128637949625 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690401017665863 Score : 0.4920635223388672\n",
      "[351/1000]\n",
      "- Train Loss : 0.4077877124150594 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903919577598572 Score : 0.4920635223388672\n",
      "[352/1000]\n",
      "- Train Loss : 0.4077742258707682 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903829574584961 Score : 0.4920635223388672\n",
      "[353/1000]\n",
      "- Train Loss : 0.4077608386675517 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903741955757141 Score : 0.4920635223388672\n",
      "[354/1000]\n",
      "- Train Loss : 0.4077475150426229 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903652548789978 Score : 0.4920635223388672\n",
      "[355/1000]\n",
      "- Train Loss : 0.4077343106269836 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690356433391571 Score : 0.4920635223388672\n",
      "[356/1000]\n",
      "- Train Loss : 0.40772117376327516 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903477311134338 Score : 0.4920635223388672\n",
      "[357/1000]\n",
      "- Train Loss : 0.4077081441879272 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903391480445862 Score : 0.4920635223388672\n",
      "[358/1000]\n",
      "- Train Loss : 0.4076951781908671 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903305649757385 Score : 0.4920635223388672\n",
      "[359/1000]\n",
      "- Train Loss : 0.40768229564030967 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903221011161804 Score : 0.4920635223388672\n",
      "[360/1000]\n",
      "- Train Loss : 0.40766950050989786 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903136968612671 Score : 0.4920635223388672\n",
      "[361/1000]\n",
      "- Train Loss : 0.4076567808787028 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6903052926063538 Score : 0.4920635223388672\n",
      "[362/1000]\n",
      "- Train Loss : 0.4076441407203674 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902971267700195 Score : 0.4920635223388672\n",
      "[363/1000]\n",
      "- Train Loss : 0.4076316197713216 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902888417243958 Score : 0.4920635223388672\n",
      "[364/1000]\n",
      "- Train Loss : 0.4076191504796346 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902807354927063 Score : 0.4920635223388672\n",
      "[365/1000]\n",
      "- Train Loss : 0.4076067566871643 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902726292610168 Score : 0.4920635223388672\n",
      "[366/1000]\n",
      "- Train Loss : 0.40759443839391074 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902645826339722 Score : 0.4920635223388672\n",
      "[367/1000]\n",
      "- Train Loss : 0.4075821995735168 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902567148208618 Score : 0.4920635223388672\n",
      "[368/1000]\n",
      "- Train Loss : 0.40757004419962567 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902487874031067 Score : 0.4920635223388672\n",
      "[369/1000]\n",
      "- Train Loss : 0.40755797227223717 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902409195899963 Score : 0.4920635223388672\n",
      "[370/1000]\n",
      "- Train Loss : 0.4075459639231364 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902332305908203 Score : 0.4920635223388672\n",
      "[371/1000]\n",
      "- Train Loss : 0.4075340191523234 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902254819869995 Score : 0.4920635223388672\n",
      "[372/1000]\n",
      "- Train Loss : 0.40752215385437013 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902179718017578 Score : 0.4920635223388672\n",
      "[373/1000]\n",
      "- Train Loss : 0.4075103799502055 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902103424072266 Score : 0.4920635223388672\n",
      "[374/1000]\n",
      "- Train Loss : 0.4074986537297567 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6902028322219849 Score : 0.4920635223388672\n",
      "[375/1000]\n",
      "- Train Loss : 0.4074870109558105 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901952624320984 Score : 0.4920635223388672\n",
      "[376/1000]\n",
      "- Train Loss : 0.4074754238128662 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690187931060791 Score : 0.4920635223388672\n",
      "[377/1000]\n",
      "- Train Loss : 0.40746393203735354 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901806592941284 Score : 0.4920635223388672\n",
      "[378/1000]\n",
      "- Train Loss : 0.4074524958928426 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690173327922821 Score : 0.4920635223388672\n",
      "[379/1000]\n",
      "- Train Loss : 0.4074411471684774 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901661157608032 Score : 0.4920635223388672\n",
      "[380/1000]\n",
      "- Train Loss : 0.4074298222859701 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901589632034302 Score : 0.4920635223388672\n",
      "[381/1000]\n",
      "- Train Loss : 0.40741859674453734 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901518702507019 Score : 0.4920635223388672\n",
      "[382/1000]\n",
      "- Train Loss : 0.40740743080774944 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901448369026184 Score : 0.4920635223388672\n",
      "[383/1000]\n",
      "- Train Loss : 0.4073963244756063 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901378035545349 Score : 0.4920635223388672\n",
      "[384/1000]\n",
      "- Train Loss : 0.4073852777481079 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901308298110962 Score : 0.4920635223388672\n",
      "[385/1000]\n",
      "- Train Loss : 0.4073743144671122 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901240944862366 Score : 0.4920635223388672\n",
      "[386/1000]\n",
      "- Train Loss : 0.4073634068171183 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901172399520874 Score : 0.4920635223388672\n",
      "[387/1000]\n",
      "- Train Loss : 0.4073525587717692 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901105046272278 Score : 0.4920635223388672\n",
      "[388/1000]\n",
      "- Train Loss : 0.40734177430470786 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6901038289070129 Score : 0.4920635223388672\n",
      "[389/1000]\n",
      "- Train Loss : 0.40733105341593423 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900970339775085 Score : 0.4920635223388672\n",
      "[390/1000]\n",
      "- Train Loss : 0.40732040802637737 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900904774665833 Score : 0.4920635223388672\n",
      "[391/1000]\n",
      "- Train Loss : 0.4073098023732503 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900839805603027 Score : 0.4920635223388672\n",
      "[392/1000]\n",
      "- Train Loss : 0.40729926427205404 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900774240493774 Score : 0.4920635223388672\n",
      "[393/1000]\n",
      "- Train Loss : 0.40728877385457357 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900709867477417 Score : 0.4920635223388672\n",
      "[394/1000]\n",
      "- Train Loss : 0.4072783629099528 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900646090507507 Score : 0.4920635223388672\n",
      "[395/1000]\n",
      "- Train Loss : 0.4072680075963338 Score : 0.32712658643722536\n",
      "- Val Loss : 0.690058171749115 Score : 0.4920635223388672\n",
      "[396/1000]\n",
      "- Train Loss : 0.4072576999664307 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900519728660583 Score : 0.4920635223388672\n",
      "[397/1000]\n",
      "- Train Loss : 0.4072474439938863 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900457739830017 Score : 0.4920635223388672\n",
      "[398/1000]\n",
      "- Train Loss : 0.4072372396787008 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900395750999451 Score : 0.4920635223388672\n",
      "[399/1000]\n",
      "- Train Loss : 0.4072271148363749 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900333762168884 Score : 0.4920635223388672\n",
      "[400/1000]\n",
      "- Train Loss : 0.40721704562505084 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900273561477661 Score : 0.4920635223388672\n",
      "[401/1000]\n",
      "- Train Loss : 0.4072070042292277 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900213360786438 Score : 0.4920635223388672\n",
      "[402/1000]\n",
      "- Train Loss : 0.4071970502535502 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900152564048767 Score : 0.4920635223388672\n",
      "[403/1000]\n",
      "- Train Loss : 0.40718713998794553 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900092959403992 Score : 0.4920635223388672\n",
      "[404/1000]\n",
      "- Train Loss : 0.4071772654851278 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6900033950805664 Score : 0.4920635223388672\n",
      "[405/1000]\n",
      "- Train Loss : 0.4071674625078837 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899976134300232 Score : 0.4920635223388672\n",
      "[406/1000]\n",
      "- Train Loss : 0.40715771118799843 Score : 0.32712658643722536\n",
      "- Val Loss : 0.68999183177948 Score : 0.4920635223388672\n",
      "[407/1000]\n",
      "- Train Loss : 0.4071479876836141 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899860501289368 Score : 0.4920635223388672\n",
      "[408/1000]\n",
      "- Train Loss : 0.40713833570480346 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899803876876831 Score : 0.4920635223388672\n",
      "[409/1000]\n",
      "- Train Loss : 0.4071287433306376 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899746060371399 Score : 0.4920635223388672\n",
      "[410/1000]\n",
      "- Train Loss : 0.4071191986401876 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899690628051758 Score : 0.4920635223388672\n",
      "[411/1000]\n",
      "- Train Loss : 0.4071096897125244 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899634599685669 Score : 0.4920635223388672\n",
      "[412/1000]\n",
      "- Train Loss : 0.407100244363149 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899579167366028 Score : 0.4920635223388672\n",
      "[413/1000]\n",
      "- Train Loss : 0.40709083477656044 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899524331092834 Score : 0.4920635223388672\n",
      "[414/1000]\n",
      "- Train Loss : 0.40708147287368773 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899469494819641 Score : 0.4920635223388672\n",
      "[415/1000]\n",
      "- Train Loss : 0.40707217454910277 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899415254592896 Score : 0.4920635223388672\n",
      "[416/1000]\n",
      "- Train Loss : 0.4070629080136617 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899362206459045 Score : 0.4920635223388672\n",
      "[417/1000]\n",
      "- Train Loss : 0.4070537845293681 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899304389953613 Score : 0.4920635223388672\n",
      "[418/1000]\n",
      "- Train Loss : 0.40704458157221474 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899247169494629 Score : 0.4920635223388672\n",
      "[419/1000]\n",
      "- Train Loss : 0.40703544616699217 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689919650554657 Score : 0.4920635223388672\n",
      "[420/1000]\n",
      "- Train Loss : 0.40702636241912843 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899146437644958 Score : 0.4920635223388672\n",
      "[421/1000]\n",
      "- Train Loss : 0.40701735417048135 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6899095773696899 Score : 0.4920635223388672\n",
      "[422/1000]\n",
      "- Train Loss : 0.4070083777109782 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689904510974884 Score : 0.4920635223388672\n",
      "[423/1000]\n",
      "- Train Loss : 0.40699944098790486 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898993849754333 Score : 0.4920635223388672\n",
      "[424/1000]\n",
      "- Train Loss : 0.4069905400276184 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898945569992065 Score : 0.4920635223388672\n",
      "[425/1000]\n",
      "- Train Loss : 0.4069817145665487 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898894906044006 Score : 0.4920635223388672\n",
      "[426/1000]\n",
      "- Train Loss : 0.40697290500005084 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689884603023529 Score : 0.4920635223388672\n",
      "[427/1000]\n",
      "- Train Loss : 0.4069642623265584 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898789405822754 Score : 0.4920635223388672\n",
      "[428/1000]\n",
      "- Train Loss : 0.4069555322329203 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898736357688904 Score : 0.4920635223388672\n",
      "[429/1000]\n",
      "- Train Loss : 0.4069468180338542 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898689270019531 Score : 0.4920635223388672\n",
      "[430/1000]\n",
      "- Train Loss : 0.4069381634394328 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689864456653595 Score : 0.4920635223388672\n",
      "[431/1000]\n",
      "- Train Loss : 0.40692958831787107 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898598670959473 Score : 0.4920635223388672\n",
      "[432/1000]\n",
      "- Train Loss : 0.4069210807482401 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898550987243652 Score : 0.4920635223388672\n",
      "[433/1000]\n",
      "- Train Loss : 0.4069125890731812 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689850389957428 Score : 0.4920635223388672\n",
      "[434/1000]\n",
      "- Train Loss : 0.40690412521362307 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689845860004425 Score : 0.4920635223388672\n",
      "[435/1000]\n",
      "- Train Loss : 0.4068957010904948 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898412704467773 Score : 0.4920635223388672\n",
      "[436/1000]\n",
      "- Train Loss : 0.4068873167037964 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898368000984192 Score : 0.4920635223388672\n",
      "[437/1000]\n",
      "- Train Loss : 0.4068789998690287 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898322701454163 Score : 0.4920635223388672\n",
      "[438/1000]\n",
      "- Train Loss : 0.40687069495519 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898279190063477 Score : 0.4920635223388672\n",
      "[439/1000]\n",
      "- Train Loss : 0.40686244567235313 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898234486579895 Score : 0.4920635223388672\n",
      "[440/1000]\n",
      "- Train Loss : 0.40685420831044516 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898191571235657 Score : 0.4920635223388672\n",
      "[441/1000]\n",
      "- Train Loss : 0.4068460385004679 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898147463798523 Score : 0.4920635223388672\n",
      "[442/1000]\n",
      "- Train Loss : 0.40683788458506265 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898104548454285 Score : 0.4920635223388672\n",
      "[443/1000]\n",
      "- Train Loss : 0.40682979027430216 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898061633110046 Score : 0.4920635223388672\n",
      "[444/1000]\n",
      "- Train Loss : 0.4068216919898987 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6898020505905151 Score : 0.4920635223388672\n",
      "[445/1000]\n",
      "- Train Loss : 0.4068136731783549 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897978186607361 Score : 0.4920635223388672\n",
      "[446/1000]\n",
      "- Train Loss : 0.406805682182312 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897937059402466 Score : 0.4920635223388672\n",
      "[447/1000]\n",
      "- Train Loss : 0.40679771900177003 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897895932197571 Score : 0.4920635223388672\n",
      "[448/1000]\n",
      "- Train Loss : 0.40678980350494387 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897855997085571 Score : 0.4920635223388672\n",
      "[449/1000]\n",
      "- Train Loss : 0.4067818999290466 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897816061973572 Score : 0.4920635223388672\n",
      "[450/1000]\n",
      "- Train Loss : 0.4067740480105082 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897774934768677 Score : 0.4920635223388672\n",
      "[451/1000]\n",
      "- Train Loss : 0.40676621596018475 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897735595703125 Score : 0.4920635223388672\n",
      "[452/1000]\n",
      "- Train Loss : 0.406758451461792 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897696256637573 Score : 0.4920635223388672\n",
      "[453/1000]\n",
      "- Train Loss : 0.4067507028579712 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897656321525574 Score : 0.4920635223388672\n",
      "[454/1000]\n",
      "- Train Loss : 0.4067429939905802 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897616982460022 Score : 0.4920635223388672\n",
      "[455/1000]\n",
      "- Train Loss : 0.40673529307047523 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897578835487366 Score : 0.4920635223388672\n",
      "[456/1000]\n",
      "- Train Loss : 0.4067276438077291 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897541284561157 Score : 0.4920635223388672\n",
      "[457/1000]\n",
      "- Train Loss : 0.4067200263341268 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897503137588501 Score : 0.4920635223388672\n",
      "[458/1000]\n",
      "- Train Loss : 0.4067124406496684 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897467374801636 Score : 0.4920635223388672\n",
      "[459/1000]\n",
      "- Train Loss : 0.4067048986752828 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897430419921875 Score : 0.4920635223388672\n",
      "[460/1000]\n",
      "- Train Loss : 0.4066973845163981 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897394061088562 Score : 0.4920635223388672\n",
      "[461/1000]\n",
      "- Train Loss : 0.4066898783047994 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897357702255249 Score : 0.4920635223388672\n",
      "[462/1000]\n",
      "- Train Loss : 0.4066824436187744 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897321939468384 Score : 0.4920635223388672\n",
      "[463/1000]\n",
      "- Train Loss : 0.40667500098546344 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897286176681519 Score : 0.4920635223388672\n",
      "[464/1000]\n",
      "- Train Loss : 0.4066676139831543 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897251009941101 Score : 0.4920635223388672\n",
      "[465/1000]\n",
      "- Train Loss : 0.4066602428754171 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897217631340027 Score : 0.4920635223388672\n",
      "[466/1000]\n",
      "- Train Loss : 0.40665292342503867 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897182464599609 Score : 0.4920635223388672\n",
      "[467/1000]\n",
      "- Train Loss : 0.4066456119219462 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897148489952087 Score : 0.4920635223388672\n",
      "[468/1000]\n",
      "- Train Loss : 0.40663835604985554 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897115111351013 Score : 0.4920635223388672\n",
      "[469/1000]\n",
      "- Train Loss : 0.4066311160723368 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897080540657043 Score : 0.4920635223388672\n",
      "[470/1000]\n",
      "- Train Loss : 0.406623899936676 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897048354148865 Score : 0.4920635223388672\n",
      "[471/1000]\n",
      "- Train Loss : 0.40661672353744505 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6897016167640686 Score : 0.4920635223388672\n",
      "[472/1000]\n",
      "- Train Loss : 0.40660958687464394 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689698338508606 Score : 0.4920635223388672\n",
      "[473/1000]\n",
      "- Train Loss : 0.4066024502118429 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896951794624329 Score : 0.4920635223388672\n",
      "[474/1000]\n",
      "- Train Loss : 0.4065953413645426 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689691960811615 Score : 0.4920635223388672\n",
      "[475/1000]\n",
      "- Train Loss : 0.4065882881482442 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896888017654419 Score : 0.4920635223388672\n",
      "[476/1000]\n",
      "- Train Loss : 0.4065812627474467 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896857619285583 Score : 0.4920635223388672\n",
      "[477/1000]\n",
      "- Train Loss : 0.4065742492675781 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896826028823853 Score : 0.4920635223388672\n",
      "[478/1000]\n",
      "- Train Loss : 0.40656727155049643 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896795630455017 Score : 0.4920635223388672\n",
      "[479/1000]\n",
      "- Train Loss : 0.40656030575434365 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896765828132629 Score : 0.4920635223388672\n",
      "[480/1000]\n",
      "- Train Loss : 0.4065533916155497 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896734833717346 Score : 0.4920635223388672\n",
      "[481/1000]\n",
      "- Train Loss : 0.4065465013186137 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896705627441406 Score : 0.4920635223388672\n",
      "[482/1000]\n",
      "- Train Loss : 0.40653961499532065 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896676421165466 Score : 0.4920635223388672\n",
      "[483/1000]\n",
      "- Train Loss : 0.4065327684084574 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896647214889526 Score : 0.4920635223388672\n",
      "[484/1000]\n",
      "- Train Loss : 0.40652594168980916 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896618604660034 Score : 0.4920635223388672\n",
      "[485/1000]\n",
      "- Train Loss : 0.40651914676030476 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689659059047699 Score : 0.4920635223388672\n",
      "[486/1000]\n",
      "- Train Loss : 0.4065124829610189 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896549463272095 Score : 0.4920635223388672\n",
      "[487/1000]\n",
      "- Train Loss : 0.40650569200515746 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896515488624573 Score : 0.4920635223388672\n",
      "[488/1000]\n",
      "- Train Loss : 0.406498912970225 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896489858627319 Score : 0.4920635223388672\n",
      "[489/1000]\n",
      "- Train Loss : 0.4064922213554382 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896466016769409 Score : 0.4920635223388672\n",
      "[490/1000]\n",
      "- Train Loss : 0.4064855734507243 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896440386772156 Score : 0.4920635223388672\n",
      "[491/1000]\n",
      "- Train Loss : 0.40647895336151124 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896414160728455 Score : 0.4920635223388672\n",
      "[492/1000]\n",
      "- Train Loss : 0.4064723372459412 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689638614654541 Score : 0.4920635223388672\n",
      "[493/1000]\n",
      "- Train Loss : 0.406465740998586 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896360516548157 Score : 0.4920635223388672\n",
      "[494/1000]\n",
      "- Train Loss : 0.40645917256673175 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896334886550903 Score : 0.4920635223388672\n",
      "[495/1000]\n",
      "- Train Loss : 0.4064526677131653 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896309852600098 Score : 0.4920635223388672\n",
      "[496/1000]\n",
      "- Train Loss : 0.4064461390177409 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896281838417053 Score : 0.4920635223388672\n",
      "[497/1000]\n",
      "- Train Loss : 0.4064396421114604 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896256804466248 Score : 0.4920635223388672\n",
      "[498/1000]\n",
      "- Train Loss : 0.40643314917882284 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896232962608337 Score : 0.4920635223388672\n",
      "[499/1000]\n",
      "- Train Loss : 0.4064267039299011 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896209716796875 Score : 0.4920635223388672\n",
      "[500/1000]\n",
      "- Train Loss : 0.40642029444376626 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896186470985413 Score : 0.4920635223388672\n",
      "[501/1000]\n",
      "- Train Loss : 0.40641388098398845 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896162629127502 Score : 0.4920635223388672\n",
      "[502/1000]\n",
      "- Train Loss : 0.4064075191815694 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896141171455383 Score : 0.4920635223388672\n",
      "[503/1000]\n",
      "- Train Loss : 0.4064011653264364 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896117925643921 Score : 0.4920635223388672\n",
      "[504/1000]\n",
      "- Train Loss : 0.40639482736587523 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896094679832458 Score : 0.4920635223388672\n",
      "[505/1000]\n",
      "- Train Loss : 0.40638850529988607 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896072626113892 Score : 0.4920635223388672\n",
      "[506/1000]\n",
      "- Train Loss : 0.40638222694396975 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896051168441772 Score : 0.4920635223388672\n",
      "[507/1000]\n",
      "- Train Loss : 0.4063759446144104 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896029710769653 Score : 0.4920635223388672\n",
      "[508/1000]\n",
      "- Train Loss : 0.4063697059949239 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6896008849143982 Score : 0.4920635223388672\n",
      "[509/1000]\n",
      "- Train Loss : 0.4063634634017944 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895986199378967 Score : 0.4920635223388672\n",
      "[510/1000]\n",
      "- Train Loss : 0.4063572605450948 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895966529846191 Score : 0.4920635223388672\n",
      "[511/1000]\n",
      "- Train Loss : 0.4063510775566101 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895946264266968 Score : 0.4920635223388672\n",
      "[512/1000]\n",
      "- Train Loss : 0.40634491443634035 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895926594734192 Score : 0.4920635223388672\n",
      "[513/1000]\n",
      "- Train Loss : 0.40633877913157146 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895906329154968 Score : 0.4920635223388672\n",
      "[514/1000]\n",
      "- Train Loss : 0.4063326358795166 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895888447761536 Score : 0.4920635223388672\n",
      "[515/1000]\n",
      "- Train Loss : 0.40632654825846354 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895867586135864 Score : 0.4920635223388672\n",
      "[516/1000]\n",
      "- Train Loss : 0.4063204566637675 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895849108695984 Score : 0.4920635223388672\n",
      "[517/1000]\n",
      "- Train Loss : 0.40631439288457233 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895831227302551 Score : 0.4920635223388672\n",
      "[518/1000]\n",
      "- Train Loss : 0.4063083449999491 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895812749862671 Score : 0.4920635223388672\n",
      "[519/1000]\n",
      "- Train Loss : 0.40630231698354086 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895794868469238 Score : 0.4920635223388672\n",
      "[520/1000]\n",
      "- Train Loss : 0.4062963048617045 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895776987075806 Score : 0.4920635223388672\n",
      "[521/1000]\n",
      "- Train Loss : 0.40629030466079713 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895759701728821 Score : 0.4920635223388672\n",
      "[522/1000]\n",
      "- Train Loss : 0.40628434816996256 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895742416381836 Score : 0.4920635223388672\n",
      "[523/1000]\n",
      "- Train Loss : 0.40627840360005696 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895725131034851 Score : 0.4920635223388672\n",
      "[524/1000]\n",
      "- Train Loss : 0.4062724709510803 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895709037780762 Score : 0.4920635223388672\n",
      "[525/1000]\n",
      "- Train Loss : 0.4062665541966756 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895692944526672 Score : 0.4920635223388672\n",
      "[526/1000]\n",
      "- Train Loss : 0.4062606612841288 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895676851272583 Score : 0.4920635223388672\n",
      "[527/1000]\n",
      "- Train Loss : 0.40625478823979694 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895660161972046 Score : 0.4920635223388672\n",
      "[528/1000]\n",
      "- Train Loss : 0.40624889930089314 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895645260810852 Score : 0.4920635223388672\n",
      "[529/1000]\n",
      "- Train Loss : 0.4062430739402771 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689562976360321 Score : 0.4920635223388672\n",
      "[530/1000]\n",
      "- Train Loss : 0.40623725255330406 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895614862442017 Score : 0.4920635223388672\n",
      "[531/1000]\n",
      "- Train Loss : 0.406231431166331 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689560055732727 Score : 0.4920635223388672\n",
      "[532/1000]\n",
      "- Train Loss : 0.4062256574630737 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895585656166077 Score : 0.4920635223388672\n",
      "[533/1000]\n",
      "- Train Loss : 0.4062198559443156 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895571351051331 Score : 0.4920635223388672\n",
      "[534/1000]\n",
      "- Train Loss : 0.4062141100565592 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689555823802948 Score : 0.4920635223388672\n",
      "[535/1000]\n",
      "- Train Loss : 0.4062083641688029 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895545125007629 Score : 0.4920635223388672\n",
      "[536/1000]\n",
      "- Train Loss : 0.4062026619911194 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895532011985779 Score : 0.4920635223388672\n",
      "[537/1000]\n",
      "- Train Loss : 0.406196931997935 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895517706871033 Score : 0.4920635223388672\n",
      "[538/1000]\n",
      "- Train Loss : 0.40619124174118043 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895505785942078 Score : 0.4920635223388672\n",
      "[539/1000]\n",
      "- Train Loss : 0.4061855673789978 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895493268966675 Score : 0.4920635223388672\n",
      "[540/1000]\n",
      "- Train Loss : 0.4061799089113871 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895481944084167 Score : 0.4920635223388672\n",
      "[541/1000]\n",
      "- Train Loss : 0.40617427428563435 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895470023155212 Score : 0.4920635223388672\n",
      "[542/1000]\n",
      "- Train Loss : 0.4061686356862386 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895458102226257 Score : 0.4920635223388672\n",
      "[543/1000]\n",
      "- Train Loss : 0.4061630288759867 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689544677734375 Score : 0.4920635223388672\n",
      "[544/1000]\n",
      "- Train Loss : 0.4061574260393778 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689543604850769 Score : 0.4920635223388672\n",
      "[545/1000]\n",
      "- Train Loss : 0.40615184704462687 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895425915718079 Score : 0.4920635223388672\n",
      "[546/1000]\n",
      "- Train Loss : 0.4061462720235189 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895414590835571 Score : 0.4920635223388672\n",
      "[547/1000]\n",
      "- Train Loss : 0.4061407208442688 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689540445804596 Score : 0.4920635223388672\n",
      "[548/1000]\n",
      "- Train Loss : 0.4061351974805196 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895394921302795 Score : 0.4920635223388672\n",
      "[549/1000]\n",
      "- Train Loss : 0.40612966219584146 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895384788513184 Score : 0.4920635223388672\n",
      "[550/1000]\n",
      "- Train Loss : 0.4061241706212362 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895376443862915 Score : 0.4920635223388672\n",
      "[551/1000]\n",
      "- Train Loss : 0.40611865917841594 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895367503166199 Score : 0.4920635223388672\n",
      "[552/1000]\n",
      "- Train Loss : 0.40611318747202557 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895357966423035 Score : 0.4920635223388672\n",
      "[553/1000]\n",
      "- Train Loss : 0.4061077197392782 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895349621772766 Score : 0.4920635223388672\n",
      "[554/1000]\n",
      "- Train Loss : 0.4061022679011027 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895341277122498 Score : 0.4920635223388672\n",
      "[555/1000]\n",
      "- Train Loss : 0.4060968478520711 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895334124565125 Score : 0.4920635223388672\n",
      "[556/1000]\n",
      "- Train Loss : 0.4060914079348246 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895326972007751 Score : 0.4920635223388672\n",
      "[557/1000]\n",
      "- Train Loss : 0.406085999806722 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895319223403931 Score : 0.4920635223388672\n",
      "[558/1000]\n",
      "- Train Loss : 0.40608059962590537 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895312070846558 Score : 0.4920635223388672\n",
      "[559/1000]\n",
      "- Train Loss : 0.4060752232869466 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895304918289185 Score : 0.4920635223388672\n",
      "[560/1000]\n",
      "- Train Loss : 0.40606984694798787 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895298361778259 Score : 0.4920635223388672\n",
      "[561/1000]\n",
      "- Train Loss : 0.4060644904772441 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689529299736023 Score : 0.4920635223388672\n",
      "[562/1000]\n",
      "- Train Loss : 0.4060591538747152 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895286440849304 Score : 0.4920635223388672\n",
      "[563/1000]\n",
      "- Train Loss : 0.4060538212458293 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895281076431274 Score : 0.4920635223388672\n",
      "[564/1000]\n",
      "- Train Loss : 0.4060485084851583 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895275712013245 Score : 0.4920635223388672\n",
      "[565/1000]\n",
      "- Train Loss : 0.4060431957244873 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895270943641663 Score : 0.4920635223388672\n",
      "[566/1000]\n",
      "- Train Loss : 0.4060379107793172 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895265579223633 Score : 0.4920635223388672\n",
      "[567/1000]\n",
      "- Train Loss : 0.4060326337814331 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895261406898499 Score : 0.4920635223388672\n",
      "[568/1000]\n",
      "- Train Loss : 0.4060273687044779 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895259022712708 Score : 0.4920635223388672\n",
      "[569/1000]\n",
      "- Train Loss : 0.40602210760116575 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895254254341125 Score : 0.4920635223388672\n",
      "[570/1000]\n",
      "- Train Loss : 0.40601686636606854 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895250082015991 Score : 0.4920635223388672\n",
      "[571/1000]\n",
      "- Train Loss : 0.4060116608937581 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895246505737305 Score : 0.4920635223388672\n",
      "[572/1000]\n",
      "- Train Loss : 0.4060064236323039 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895243525505066 Score : 0.4920635223388672\n",
      "[573/1000]\n",
      "- Train Loss : 0.4060012022654215 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895240545272827 Score : 0.4920635223388672\n",
      "[574/1000]\n",
      "- Train Loss : 0.4059960126876831 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895238161087036 Score : 0.4920635223388672\n",
      "[575/1000]\n",
      "- Train Loss : 0.4059908310572306 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895236968994141 Score : 0.4920635223388672\n",
      "[576/1000]\n",
      "- Train Loss : 0.4059856653213501 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895235180854797 Score : 0.4920635223388672\n",
      "[577/1000]\n",
      "- Train Loss : 0.4059805035591125 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895232796669006 Score : 0.4920635223388672\n",
      "[578/1000]\n",
      "- Train Loss : 0.40597533782323203 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895231604576111 Score : 0.4920635223388672\n",
      "[579/1000]\n",
      "- Train Loss : 0.4059702078501383 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895230412483215 Score : 0.4920635223388672\n",
      "[580/1000]\n",
      "- Train Loss : 0.40596508582433066 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895230412483215 Score : 0.4920635223388672\n",
      "[581/1000]\n",
      "- Train Loss : 0.4059599677721659 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689522922039032 Score : 0.4920635223388672\n",
      "[582/1000]\n",
      "- Train Loss : 0.4059548616409302 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895230412483215 Score : 0.4920635223388672\n",
      "[583/1000]\n",
      "- Train Loss : 0.40594977140426636 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895231008529663 Score : 0.4920635223388672\n",
      "[584/1000]\n",
      "- Train Loss : 0.40594468514124554 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895231008529663 Score : 0.4920635223388672\n",
      "[585/1000]\n",
      "- Train Loss : 0.40593961874643963 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895231604576111 Score : 0.4920635223388672\n",
      "[586/1000]\n",
      "- Train Loss : 0.4059345523516337 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895233988761902 Score : 0.4920635223388672\n",
      "[587/1000]\n",
      "- Train Loss : 0.4059295177459717 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895235180854797 Score : 0.4920635223388672\n",
      "[588/1000]\n",
      "- Train Loss : 0.4059244592984517 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895236372947693 Score : 0.4920635223388672\n",
      "[589/1000]\n",
      "- Train Loss : 0.4059194326400757 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895238757133484 Score : 0.4920635223388672\n",
      "[590/1000]\n",
      "- Train Loss : 0.4059144139289856 Score : 0.32712658643722536\n",
      "- Val Loss : 0.6895240545272827 Score : 0.4920635223388672\n",
      "[591/1000]\n",
      "- Train Loss : 0.40590938727060955 Score : 0.32712658643722536\n",
      "- Val Loss : 0.689524233341217 Score : 0.4920635223388672\n",
      "성능 및 손실 개선이 없어서 591 EPOCH에 학습 중단\n"
     ]
    }
   ],
   "source": [
    "## 학습의 효과 확인 손실값과 성능평가값 저장 필요 \n",
    "LOSS_HISTORY, SCORE_HISTORY = [[],[]],[[],[]]\n",
    "BATCH_CNT = iris_ds.n_rows/BATCH_SIZE\n",
    "print(f'CNT : {BATCH_CNT}')\n",
    "\n",
    "# =================================================================================================================\n",
    "# 학습 모니터링/스케줄링 설정 \n",
    "# => LOSS_HISTORY, SCORE_HISTORY 활용 \n",
    "# => 임계 기준 : 10번 \n",
    "BREAK_CNT = 0\n",
    "LIMIT_VALUE = 10 # 멈추는 기준 \n",
    "# =================================================================================================================\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    # 학습 모드로 모델 설정 \n",
    "    model.train()\n",
    "\n",
    "    # 배치 크기 만큼 데이터 로딩해서 학습 진행 \n",
    "    loss_total, score_total = 0,0\n",
    "    for feature_ts, target_ts in train_dl: # iris_dl -> train_dl \n",
    "\n",
    "        # 학습 진행 \n",
    "        pre_y = model(feature_ts)\n",
    "\n",
    "        # 손실 계산 : nn.CrossEntropyLoss 요구사항 : 정답(=타겟)은 0차원 or 1차원, type = long\n",
    "        loss = req_loss(pre_y, target_ts.reshape(-1).long())\n",
    "        loss_total += loss.item() # tensor 라서 item으로 값 넣어야 함 \n",
    "\n",
    "        # 성능 평가 계산 \n",
    "        score = MulticlassF1Score(num_classes=3)(pre_y, target_ts.reshape(-1).long()) \n",
    "        score_total += score.item() # tensor 라서 item으로 값 넣어야 함 \n",
    "\n",
    "        # 최적화 진행 \n",
    "        optimizer.zero_grad()       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 에포크 당 검증기능 \n",
    "    # 모델 검증 모드 설정 \n",
    "    model.eval()\n",
    "    # 검증한 결과를 저장해야 함 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋 \n",
    "        val_feature_ts = torch.FloatTensor(val_ds.feature_df.values) # values쓰면 array 됨 -> torch사용 -> torch됨 \n",
    "        val_target_ts = torch.FloatTensor(val_ds.target_df.values)\n",
    "\n",
    "        # 평가 \n",
    "        pre_val =model(val_feature_ts)\n",
    "        \n",
    "        # 손실 \n",
    "        loss_val = req_loss(pre_val, val_target_ts.reshape(-1).long())\n",
    "\n",
    "        # 성능 평가 \n",
    "        score_val = MulticlassF1Score(num_classes=3)(pre_val, val_target_ts.reshape(-1).long())\n",
    "\n",
    "\n",
    "    # for문 다 돌면 1 epoch 종료 \n",
    "    # 손실값과 성능평가값 저장 \n",
    "    LOSS_HISTORY[0].append(loss_total/BATCH_CNT)\n",
    "    SCORE_HISTORY[0].append(score_total/BATCH_CNT)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {LOSS_HISTORY[0][-1]} Score : {SCORE_HISTORY[0][-1]}')\n",
    "    print(f'- Val Loss : {LOSS_HISTORY[1][-1]} Score : {SCORE_HISTORY[1][-1]}')\n",
    "\n",
    "    # =================================================================================================================\n",
    "\n",
    "    # 학습 진행 모니터링/스케줄링 - 검증 DS 기준 \n",
    "    # 아래 중 둘 중 하나만 하면 됨 - 두개 다 하면 cnt만 올라감 \n",
    "\n",
    "    # validation loss \n",
    "    if len(LOSS_HISTORY[1]) >= 2:\n",
    "        if LOSS_HISTORY[1][-1] >= LOSS_HISTORY[1][-2] : BREAK_CNT +=1 # LOSS_HISTORY[1][-1]  < LOSS_HISTORY[1][-2]이 정상 \n",
    "\n",
    "    # # validation score   \n",
    "    # if len(SCORE_HISTORY[1]) >= 2: \n",
    "    #     if SCORE_HISTORY[1][-1] <= SCORE_HISTORY[1][-2] : BREAK_CNT +=1 \n",
    "    \n",
    "\n",
    "    # 학습 중단 여부 설정 \n",
    "    # if BREAK_CNT >= 9:  # BREAK_CNT가 0부터 시작하면 >= 9, 1부터 시작하면 >10\n",
    "    #     print('성능 및 손실 개선이 없어서 학습 중단')\n",
    "    #     break\n",
    "\n",
    "    if BREAK_CNT > LIMIT_VALUE:  # LIMIT_VALUE 변수 사용 \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 결과 체크 => 학습과 검증의 Loss 변화, 성능 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
