{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "p324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 모델 \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_vocab, \n",
    "        hidden_dim,\n",
    "        embedding_dim, \n",
    "        n_layers,\n",
    "        dropout = 0.5,\n",
    "        bidirectional = True,\n",
    "        model_type = \"lstm\"\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding( # 인스턴스 생성 \n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2,1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, inputs): # 순방향 메서드\n",
    "        embeddings = self.embedding(inputs) # 입력받은 정수 인코딩을 임베딩 계층에 통과시켜 임베딩 값을 얻음\n",
    "        output, _ = self.model(embeddings) # 얻음 임베딩 값을 모델에 입력하여 출력값 얻음 \n",
    "        last_output = output[:,-1,:] \n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output) # 마지막 값만 추출하여 분류기 계층에 전달 \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-35\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-35\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 세트 불러오기 \n",
    "\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state=42) # 90%만 뽑아서 train \n",
    "test = corpus_df.drop(train.index) # drop한 나머지 10%를 test\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "# 데이터 토큰화 및 단어 사전 구축 \n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus = train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"]) \n",
    "# 문장의 길이를 맞추기 위해 <pad> 토큰 추가, 단어 사전 최대 길이 5000\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[  1  33  76 308   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[   1 1404    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 및 패딩 - 토큰을 정수로 변환 \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "## 너무 긴 문장은 최대 길이로 줄이고 너무 작은 길이는 최대 길이와 동일한 크기로 변환 \n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list() # []\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence =  sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [[token_to_id.get(token, unk_id) for token in review] for review in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "## 패딩 \n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])\n",
    "\n",
    "print(train_ids[2])\n",
    "print(test_ids[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> OOV의 경우 1(<\"unk\">)로 인코딩된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 적용 \n",
    "\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype = torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "## TensorDataset : pytorch tensor 형태를 입력값으로 받음 \n",
    "## -> 정수 인코딩과 라벨값을 pytorch tensor 형태로 변환\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "## 모델 학습 과정 시 각 step 마다 데이터를 batch size 크기로 분할하여 넣어 효과적이고 효율적인 학습 진행하도록 \n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수와 최적화 함수 정의 \n",
    "\n",
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64             # 은닉 상태 크기 64\n",
    "embedding_dim = 128         # embedding 벡터 크기 128\n",
    "n_layers = 2                # 신경망을 2개의 층으로 구성 \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "\n",
    "## 문장의 긍정, 부정을 분류하므로 손실함수는 이진 교차 엔트로피 함수 적용 \n",
    "## BCEWithLogitsLoss : BCELoss + Sigmoid\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)   \n",
    "\n",
    "## RMSprop : 모든 기울기를 누적하지 않고, 지수 가중 이동 평균(EWMA)을 사용해 학습률 조절 \n",
    "## - 기울기 제곱 값의 평균값이 작아지면 학습률 증가, 반대일 경우 학습률을 감소시켜 불필요한 지역 최솟값에 빠지는 것 방지 \n",
    "## - 기울기의 크기가 큰 경우에는 빠른 수렴을 보이며 작은 경우에는 더 작은 학습률을 유지시켜 더 안정적으로 최적화 수행 \n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6771101355552673\n",
      "Train Loss 500 : 0.6929116890340032\n",
      "Train Loss 1000 : 0.675214636218655\n",
      "Train Loss 1500 : 0.6573497134991124\n",
      "Train Loss 2000 : 0.6340165737344288\n",
      "Train Loss 2500 : 0.611079863039125\n",
      "Val Loss : 0.48223146953331397, Val Accuracy : 0.7716\n",
      "Train Loss 0 : 0.319711297750473\n",
      "Train Loss 500 : 0.46402559532019905\n",
      "Train Loss 1000 : 0.4583107573645456\n",
      "Train Loss 1500 : 0.45414992874062593\n",
      "Train Loss 2000 : 0.44682633556213336\n",
      "Train Loss 2500 : 0.44069109470700796\n",
      "Val Loss : 0.417280415709788, Val Accuracy : 0.8038\n",
      "Train Loss 0 : 0.3209379315376282\n",
      "Train Loss 500 : 0.37337728475263254\n",
      "Train Loss 1000 : 0.3746809307095054\n",
      "Train Loss 1500 : 0.3772474047166359\n",
      "Train Loss 2000 : 0.37759283012610445\n",
      "Train Loss 2500 : 0.3751473431823636\n",
      "Val Loss : 0.3963756580560352, Val Accuracy : 0.8144\n",
      "Train Loss 0 : 0.38737574219703674\n",
      "Train Loss 500 : 0.330378498071563\n",
      "Train Loss 1000 : 0.3296247088014484\n",
      "Train Loss 1500 : 0.33187238574474753\n",
      "Train Loss 2000 : 0.33404232089271374\n",
      "Train Loss 2500 : 0.3345217908613804\n",
      "Val Loss : 0.41416004054938643, Val Accuracy : 0.825\n",
      "Train Loss 0 : 0.3581652045249939\n",
      "Train Loss 500 : 0.30363151297984486\n",
      "Train Loss 1000 : 0.29883929349474614\n",
      "Train Loss 1500 : 0.30146766685540083\n",
      "Train Loss 2000 : 0.30146998159591104\n",
      "Train Loss 2500 : 0.303252239212221\n",
      "Val Loss : 0.38548729104546314, Val Accuracy : 0.827\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 테스트 \n",
    "# - 모델 학습 중간에 학습이 잘 이뤄지고 있는지 확인하기 위해 일정 배치 학습 후 테스트 데이터세트로 손실값 확인 \n",
    "\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train() # train 모드 설정 \n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1) # unsqueeze : 지정한 자리에 size가 1인 빈 공간을 채워주면서 차원을 확장\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval() # 검증 모드 설정 \n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels= labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss= criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits) > .5 \n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "## 위의 코드에서 정의한 criterion, optimizer 사용 \n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device) # 검증은 test 데이터셋으로 진행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 테스트 데이터세트에 대해서 손실이 감소하며 정확도 상승 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습 과정에서 임베딩 계층을 비롯한 순환 신경망 내의 여러 가중치 최적화\n",
    "- 학습된 임베딩 계층의 가중치를 동일한 단어 사전을 사용하는 토큰의 임베딩 값으로 사용 ㅇ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-1.1848216   1.3872174   0.5732668   1.1607504  -0.662444    0.99736863\n",
      "  0.7150054  -0.96393883 -0.9589178   1.6164086  -0.59270656  0.0131274\n",
      "  0.6605736   0.9091631   0.30927375  0.714278    0.36193472 -1.8440436\n",
      "  0.01952901  0.43299648 -1.9145285  -1.130032    0.29609177  1.8629019\n",
      " -0.12541625 -1.3493133   0.68224716 -0.36267665 -0.4312131  -1.742426\n",
      " -0.7756632   0.05902952 -1.1977922   0.92043453 -0.502909   -0.34839186\n",
      "  0.08734956  1.2330111   0.181254   -0.69788235 -0.7337799   0.5403862\n",
      " -0.15272637 -0.7318662   0.2503486  -0.23352377  0.20349824  0.67190075\n",
      "  0.671132   -1.7439362   0.23648489  0.6276126  -2.5532656  -0.32033324\n",
      " -3.0388176  -0.4134272  -1.1825596   1.2390556   0.13895416  0.25890353\n",
      " -1.0866429   1.8996519   0.8110003   0.57520425  0.3537721   1.4119903\n",
      "  1.092689   -0.43305323 -0.67739654  0.16830924 -1.0965645   1.1179385\n",
      " -0.01284698 -1.0276898   0.43917525 -0.19183785 -0.5664117  -0.38659865\n",
      "  1.3136035  -0.37380627 -0.78816944  0.08326602 -0.1963941  -0.7573682\n",
      "  0.13748737 -0.2064661  -0.25852954 -0.69659626 -1.0156176  -0.47343707\n",
      " -1.1200001   0.37818602 -0.6910484   1.0049165   0.51864094  0.786109\n",
      " -0.17761298 -0.826258    0.10889468  0.14857815  0.19370899 -0.5325706\n",
      "  0.7628888   1.5769839  -0.19543499 -2.5610137   0.8275788   0.14202209\n",
      " -2.606548    0.6169995   0.2975404  -0.0915008   0.41097352  0.69883657\n",
      "  0.53588605  0.6914721   1.1267561   0.16678447  1.441134    0.02158967\n",
      " -0.20810795 -0.8410666   2.7675803   0.9770607  -0.49757352 -0.61175555\n",
      "  1.0780704  -0.12491271]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출 \n",
    "\n",
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "# weight (Tensor) \n",
    "# – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\n",
    "# .detach().cpu().numpy() : tensor를 numpy로 변환하기 위해서는 cpu 메모리로 옮겨야 한다, .numpy() 이전에 .cpu()가 실행되야 한다.\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-35\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-35\\Korpora\\nsmc\\ratings_test.txt\n",
      "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 임베딩 값을 초기값으로 적용해 모델 학습시키는 방법 \n",
    "\n",
    "## 1) p288 영화 리뷰 데이터세트 전처리\n",
    "\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus = pd.DataFrame(corpus.test)\n",
    "\n",
    "tokenizer = Okt() \n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text] # 형태소 추출 \n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) p297 Word2Vec 모델 학습 \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "## Gensim은 텍스트 관련 문제를 해결하기 위한 강력한 도구로, 주로 주제 모델링과 문서 유사성 분석에 사용 \n",
    "## 대규모 데이터셋을 처리하는 능력과 다양한 자연어 처리 기능으로 인해, 데이터 과학자, 연구원, 개발자 사이에서 널리 사용\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences = tokens,\n",
    "    vector_size = 128,\n",
    "    window = 5,\n",
    "    min_count = 1,\n",
    "    sg = 1,\n",
    "    epochs= 3,\n",
    "    max_final_vocab = 10000\n",
    ")\n",
    "\n",
    "word2vec.save('../10_08/models/word2vec.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 모델로 임베딩 계층 초기화 \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "word2vec = Word2Vec.load(\"../10_08/models/word2vec.model\")\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in [\"<pad>\", \"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "## 임베딩 계층은 from_pretrained 메서드로 초기화 ㅇ \n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 SentenceClassifier 클래스 수정 \n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_vocab, \n",
    "        hidden_dim,\n",
    "        embedding_dim, \n",
    "        n_layers,\n",
    "        dropout = 0.5,\n",
    "        bidirectional = True,\n",
    "        model_type = \"lstm\",\n",
    "        pretrained_embedding = None\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding( # 인스턴스 생성 \n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2,1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 사전 학습된 임베딩(pretrained_embedding)이 None이 아니라면 전달된 값을 임베딩 계층으로 초기화화\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=n_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx = 0\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, inputs): # 순방향 메서드\n",
    "        embeddings = self.embedding(inputs) # 입력받은 정수 인코딩을 임베딩 계층에 통과시켜 임베딩 값을 얻음\n",
    "        output, _ = self.model(embeddings) # 얻은임베딩 값을 모델에 입력하여 출력값 얻음 \n",
    "        last_output = output[:,-1,:] \n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output) # 마지막 값만 추출하여 분류기 계층에 전달 \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6916605830192566\n",
      "Train Loss 500 : 0.6834308970236255\n",
      "Train Loss 1000 : 0.6704473164412644\n",
      "Train Loss 1500 : 0.6398692919682217\n",
      "Train Loss 2000 : 0.6208030407962533\n",
      "Train Loss 2500 : 0.6073142584802055\n",
      "Val Loss : 0.5821949984318913, Val Accuracy : 0.7008\n",
      "Train Loss 0 : 0.6439685821533203\n",
      "Train Loss 500 : 0.5270143159611258\n",
      "Train Loss 1000 : 0.5239736239065776\n",
      "Train Loss 1500 : 0.5144156443921826\n",
      "Train Loss 2000 : 0.5082371082828976\n",
      "Train Loss 2500 : 0.5015851934937562\n",
      "Val Loss : 0.47169405250503615, Val Accuracy : 0.7834\n",
      "Train Loss 0 : 0.6342604756355286\n",
      "Train Loss 500 : 0.4666483946546109\n",
      "Train Loss 1000 : 0.466072138655674\n",
      "Train Loss 1500 : 0.46236131353667387\n",
      "Train Loss 2000 : 0.46436137084601103\n",
      "Train Loss 2500 : 0.4624076225504023\n",
      "Val Loss : 0.4343377980180442, Val Accuracy : 0.8026\n",
      "Train Loss 0 : 0.48298218846321106\n",
      "Train Loss 500 : 0.45271911369469353\n",
      "Train Loss 1000 : 0.447593104559463\n",
      "Train Loss 1500 : 0.44408658016808106\n",
      "Train Loss 2000 : 0.44010486157386675\n",
      "Train Loss 2500 : 0.439988708383486\n",
      "Val Loss : 0.4271667438288466, Val Accuracy : 0.8078\n",
      "Train Loss 0 : 0.6431658267974854\n",
      "Train Loss 500 : 0.4286121439315126\n",
      "Train Loss 1000 : 0.4299480411541331\n",
      "Train Loss 1500 : 0.4289156698499577\n",
      "Train Loss 2000 : 0.4289920424957802\n",
      "Train Loss 2500 : 0.4275778820071636\n",
      "Val Loss : 0.418913329704501, Val Accuracy : 0.8066\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 임베딩을 사용한 모델 학습 \n",
    "\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab = n_vocab, hidden_dim=hidden_dim, embedding_dim= embedding_dim,\n",
    "    n_layers= n_layers, pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 학습된 임베딩을 사용하는 것은 모델 성능을 개선할 수 있는 방법 중 하나 \n",
    "- but 학습 데이터의 양이 많다면 모델의 목적에 맞게 새로운 임베딩 층을 학습하는 것이 더 좋은 결과일 수 ㅇ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
